\\usepackage{grffile}
                      help="latex file containing header and footer")
data_file = open(path, 'rb')
irows = filter_rows(csv.reader(data_file))
def get_citations_from_directories(dir_name=".", key_prefix="", output_filename="output.txt"):
    files = glob.glob(dir_name)
    out_file = open(output_filename, "w")
    for filename in files:
        print filename
        page = open(filename, "r").read()
        key_suffix = get_name_of_inner_subdirectory(filename)
        file_prefix = os.path.basename(filename)
        keyname = key_prefix + file_prefix + key_suffix
            out_file.write(cite + "\n")
        out_file.flush()
    out_file.close()
def get_name_of_inner_subdirectory(filename):
    the_dirname = os.path.dirname(filename)
def write_unique_strings(strings, filename):
    fh = open(filename, "w")
    infofile = os.path.join(FOLDER,subfolder,'info.txt')
    if os.path.exists(infofile):
                    for line in open(infofile).readlines()
    filename = os.path.join(FOLDER,subfolder,'chapters.txt')
                for line in open(filename).readlines()
    filename = os.path.join(FOLDER,subfolder,'%.2i.markmin' % chapter_id)
    if (not os.path.isfile(dest)) or FORCE_RENDER:
        content = open(filename).read()
        filename = os.path.join(FOLDER,subfolder,'%.2i.markmin' % chapter_id)
        data = open(filename).read().replace('\r','')
    filename = os.path.join(FOLDER,subfolder,'images',key)
    if not os.path.isfile(filename):
    return response.stream(filename)
    filename = os.path.join(FOLDER,subfolder,'references',key)
    if not os.path.isfile(filename):
                for line in open(filename).readlines()
fileHandle = open ( '/home/bani/Dropbox/tasks.txt', 'a' )
fileHandle.write (gitem2)
fileHandle.write (output)
fileHandle.close()
# and I use this to open the file:
fileHandle = open("/home/bani/Dropbox/IDE/snip/snip_js.txt", "r")
buffer.append(fileHandle.read())
# fileHandle = open("/home/bani/Dropbox/IDE/snip/snip_js.txt", "r", "utf-8")
# buffer.append(fileHandle.read())
# fileHandle = open("/home/bani/Dropbox/IDE/snip/snip_js.txt", "wr")
# buffer.append(fileHandle.read())
fileHandle = codecs.open('/home/bani/Dropbox/IDE/snip/snip_js.txt', 'w', encoding='utf-8')
fileHandle.write(gitem)
fileHandle.write (output)
fileHandle.close()
# and I use this to open the file:
fileHandle = open ( "/root/.kde/share/apps/ktexteditor_snippets/data/wwww.xml", 'r+' )
data = fileHandle.read()
fileHandle.write(xmlData)
fileHandle.write(allgitems)
fileHandle.close()
# and I use this to open the file:
fileHandle = open ( '/home/bani/Dropbox/IDE/snip/snip_py.txt', 'a' )
fileHandle = open ( '/home/bani/Dropbox/tasks.txt', 'a' )
fileHandle.write (gitem2)
fileHandle.write (output)
fileHandle.close()
# and I use this to open the file:
matchstr = """<snippets namespace="" license="BSD" filetypes="yacas" authors="bani" name="English">
#  myfile.flush()  # Flush the I/O buffer
# stdout is treated as a file.  If you ever need to flush it, do so:
fileHandle =""
# with open('/home/bani/.kde/share/apps/ktexteditor_snippets/data/test.xml', 'w') as fileHandle
# fileHandle = open ( "/home/bani/.kde/share/apps/ktexteditor_snippets/data/Inbox.xml", 'a' )
#   fileHandle = open(/home/bani/.kde/share/apps/ktexteditor_snippets/data/Inbox.xml, "r+")
fileHandle = open ( "/home/bani/.kde/share/apps/ktexteditor_snippets/data/Inbox.xml", "r+")
rs = fileHandle.read()
fileHandle.seek(0, 0)
fileHandle.write(xp)
fileHandle.truncate(fileHandle.tell())
fileHandle.close()
fileHandle.flush() 
fileName = "/home/tazjel/Dropbox/py_test.py"
system.create_file(fileName, contents= ww)
fileName = "/home/tazjel/Dropbox/ak_files.txt"
system.create_file(fileName, contents= ww)
fileHandle = open ( '/home/bani/Dropbox/IDE/snip/snip_py.txt', 'a' )
web2file.py - Grab a webpage and save it to a file.
out_filename = url.split('/')[-1]
out_fh = open(out_filename, 'w')
print "Saved contents of %s locally as %s." % (url, out_filename)
find_duplicate_files.py - Identify identical files, even if they have different
    ''' Read a file, and creates a cumulative checksum, line by line. The final
def build_file_paths(path):
    '''Recursively traverse the filesystem, starting at path, and return a full list
       of files.'''
    for dirpath, dirnames, filenames in os.walk(path):
        for file in filenames:
            fullpath = os.path.join(dirpath, file)
    '''Determine duplicate files based on filesize, and checksum. We use a
    file_list = build_file_paths(path)
    print "Traversed %d files." % len(file_list)
    for file in file_list:
        compound_key = (os.path.getsize(file), create_checksum(file))
            duplicates.append(file)
            seen[compound_key] = file
# with open('/home/bani/.kde/share/apps/ktexteditor_snippets/data/test.xml', 'w') as fileHandle
fileHandle = open ( "/home/bani/.kde/share/apps/ktexteditor_snippets/data/Inbox.xml", 'a' )
fileHandle = open ( "/home/bani/.kde/share/apps/ktexteditor_snippets/data/Inbox.xml", 'r' )
s = fileHandle.read()
fileHandle.close()
fileHandle = open ( "/home/bani/.kde/share/apps/ktexteditor_snippets/data/Inbox.xml", 'w' )
fileHandle.write(xp)
fileHandle.close()
# and I use this to open the file:
fileHandle = open ( "/root/.kde/share/apps/ktexteditor_snippets/data/wwww.xml", 'r+' )
data = fileHandle.read()
fileHandle.write(xmlData)
fileHandle.write(allgitems)
fileHandle.close()
# and I use this to open the file:
fileHandle = open("/home/bani/Dropbox/IDE/snip/snip_js.txt", "r")
buffer.append(fileHandle.read())
# fileHandle = open("/home/bani/Dropbox/IDE/snip/snip_js.txt", "r", "utf-8")
# buffer.append(fileHandle.read())
# fileHandle = open("/home/bani/Dropbox/IDE/snip/snip_js.txt", "wr")
# buffer.append(fileHandle.read())
fileHandle = codecs.open('/home/bani/Dropbox/IDE/snip/snip_js.txt', 'w', encoding='utf-8')
fileHandle.write(gitem)
fileHandle.write (output)
fileHandle.close()
# and I use this to open the file:
#  myfile.flush()  # Flush the I/O buffer
# stdout is treated as a file.  If you ever need to flush it, do so:
fileHandle =""
# with open('/home/bani/.kde/share/apps/ktexteditor_snippets/data/test.xml', 'w') as fileHandle
# fileHandle = open ( "/home/bani/.kde/share/apps/ktexteditor_snippets/data/Inbox.xml", 'a' )
#   fileHandle = open(/home/bani/.kde/share/apps/ktexteditor_snippets/data/Inbox.xml, "r+")
fileHandle = open ( "/home/bani/.kde/share/apps/ktexteditor_snippets/data/Inbox.xml", "r+")
rs = fileHandle.read()
fileHandle.seek(0, 0)
fileHandle.write(xp)
fileHandle.truncate(fileHandle.tell())
fileHandle.close()
fileHandle.flush() 
#  myfile.flush()  # Flush the I/O buffer
# stdout is treated as a file.  If you ever need to flush it, do so:
fileHandle =""
fileHandle = open("/home/bani/Dropbox/@mm/Inbox.mm", "r+")
rs = fileHandle.read()
fileHandle.seek(0, 0)
fileHandle.write(xp)
fileHandle.truncate(fileHandle.tell())
# fileHandle.close()
fileHandle.flush()
fileHandle = open ( "/home/tazjel/.kde/share/apps/ktexteditor_snippets/data/vi.xml", 'a' )
fileHandle = open ( "/home/tazjel/.kde/share/apps/ktexteditor_snippets/data/vi.xml", 'r' )
s = fileHandle.read()
fileHandle.close()
fileHandle = open ( "/home/tazjel/.kde/share/apps/ktexteditor_snippets/data/vi.xml", 'w' )
fileHandle.write(xp)
fileHandle.close()
fileHandle = open ( "/home/tazjel/.kde/share/apps/ktexteditor_snippets/data/vi.xml", 'a' )
fileHandle = open ( "/home/tazjel/.kde/share/apps/ktexteditor_snippets/data/vi.xml", 'r' )
s = fileHandle.read()
fileHandle.close()
for zf in list_files:
    fileHandle_loop = open (zf, 'a')
fileHandle = open ( "/home/tazjel/.kde/share/apps/ktexteditor_snippets/data/vi.xml", 'w' )
fileHandle.write(xp)
fileHandle.close()
# keyboard.send_keys("""{{response.files.append(URL(r=request,c=static,f='%s'))}}"""%text)
y = """{{response.files.append(URL(r=request,c=static,f='%s'))}}""" % text
abbrfile = "/home/tazjel/Dropbox/myfile.txt" 
f = open(abbrfile, 'r') 
abbrfile = "/home/tazjel/Dropbox/myfile.txt" 
f = open(abbrfile, 'r') 
def gen_find(filepat,top):
    for path, dirlist, filelist in os.walk(top):
        for name in fnmatch.filter(filelist,filepat):
text = "<foo> This is a <b> foo file </b> you know. </foo>"
for dirpath, dirnames, files in os.walk(topdir):
    for name in files:
with open(logpath, 'a') as logfile:
    logfile.write('%s\n' % os.path.join(dirname, name))
logname = 'findfiletype.log'
            # Instead of printing, open up the log file for appending
            with open(logpath, 'a') as logfile:
                logfile.write('%s\n' % os.path.join(dirname, name))
# Change the arg to a tuple containing the file
# extension and the log file name. Start the walk.
logname = 'findfiletype.log'
for dirpath, dirnames, files in os.walk(topdir):
    for name in files:
# Write results to logfile
with open(logname, 'w') as logfile:
    logfile.write(results)
from gluon.fileutils import listdir, cleanpath, tar, tar_compiled, untar
from gluon.fileutils import check_credentials
                     ('head', 'profile'),
       'baseProfile', 'bbox', 'begin', 'by', 'calcMode', 'cap-height',
       'baseProfile', 'bbox', 'begin', 'by', 'calcMode', 'cap-height',
                 'dcoref.logFile', '/dev/null', '-')
        """ walks a directory, and executes a callback on each file """
        for file in [file for file in os.listdir(dir) if not file in [".",".."]]:
            nfile = os.path.join(dir,file)
            meth(nfile)
            if os.path.isdir(nfile):
                self.walk(nfile,meth)
#A super nice way to do! I used it to look for specific file type. Here is the one I modified.
    def walk(self,dir,meth, fileType=”.xml”):
        “”" walks a directory, and executes a callback on each file “”"
        for file in [file for file in os.listdir(dir) if not file in [".",".."]]:
            nfile = os.path.join(dir,file)
            if(os.path.isfile(nfile)):
                basename, extention = os.path.splitext(nfile)
            if(extention == fileType):
                meth(nfile)
            if os.path.isdir(nfile):
                self.walk(nfile,meth)
            elif(os.path.isdir(nfile)):
                self.walk(nfile,meth)
file_list = []
for root, dirs, files in os.walk('..'):
    print 'files =', files
    for name in files:
            file_list.append(os.path.join(root,name))
print file_list
def wdouble(filename):
    with open(filename) as fin:
#	OpenShot Video Editor is a program that creates, modifies, and edits video files.
#	This file is part of OpenShot Video Editor (http://launchpad.net/openshot/).
base_path = os.path.dirname(os.path.abspath(__file__))
    # Specify which files and folders to ignore in the project.
    # VCSs.  Also they are not returned in `Project.get_files()`.
    # Specifies which files should be considered python files.  It is
    # useful when you have scripts inside your project.  Only files
    # ending with ``.py`` are considered to be python files by
    #prefs['python_files'] = ['*.py']
# code below only runs when this file is run as a script
        def delete_file(path):
            delete_file(path)
        def open_file(data,data_str,path):
            file = open(path,"wb")
            if file:
                    pickle.dump(data,file)
                file.close()
        def load_file(data,data_str,path):
                file1 = open(path,'rb')
                if file1:
                        data = pickle.load(file1)
                    open_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    open_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    open_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    index = load_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    graph = load_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    ranks = load_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
#  myfile.flush()  # Flush the I/O buffer
# stdout is treated as a file.  If you ever need to flush it, do so:
fileHandle =""
fileHandle = open("/home/bani/Dropbox/@mm/Inbox.mm", "r+")
rs = fileHandle.read()
fileHandle.seek(0, 0)
fileHandle.write(xp)
fileHandle.truncate(fileHandle.tell())
# fileHandle.close()
fileHandle.flush()
        wiki = re.sub(r'(?i)\[\[File:[^\[\]]*?\]\]', '', wiki)
        wiki = re.sub(r'(?i)File:[^\[\]]*?', '', wiki)
    wiki = """[[File:LocationUruguay.png|right|]]
file.write(" ".join([str1, str2, str3, "\n"]))
if not os.path.isfile(pdfpath):
if os.path.isfile(pdfpath):
find_duplicate_files.py - Identify identical files, even if they have different
    ''' Read a file, and creates a cumulative checksum, line by line. The final
def build_file_paths(path):
    '''Recursively traverse the filesystem, starting at path, and return a full list
       of files.'''
    for dirpath, dirnames, filenames in os.walk(path):
        for file in filenames:
            fullpath = os.path.join(dirpath, file)
    '''Determine duplicate files based on filesize, and checksum. We use a
    file_list = build_file_paths(path)
    print "Traversed %d files." % len(file_list)
    for file in file_list:
        compound_key = (os.path.getsize(file), create_checksum(file))
            duplicates.append(file)
            seen[compound_key] = file
    filenames = path.path(data_path).files('*.json')
    for filename in sorted(filenames):
        with open(filename) as f:
        debate['filename'] = str(filename)
	(dirName, fileName) = os.path.split(name)
	(fileBaseName, fileExtension)=os.path.splitext(fileName)
	return dirName, fileName, fileBaseName, fileExtension
                file = nameSplit(sys.argv[1])
                name = file[2]
        print 'filename required as argument'
    #print("Usage: %s [option] <textfile>" % os.path.basename(sys.argv[0]))
    File = open("/home/ahmed/Dropbox/11_CAR.snip", "rb")
    FileContent = File.read()
    File.close()
    print("Could not open file!")
#print("%s: Could not open file!" % sys.argv[1])
            for Match in RegExp.finditer(FileContent):
                Line = FileContent.count("\n", 0, Match.start(0)) + 1
'''This file is adapted from the pattern library.
    MODULE = os.path.dirname(os.path.abspath(__file__))
    """ Returns an iterator over the lines in the file at the given path,
            # From file path.
        self._path       = path   # XML file path.
        """ Loads the XML-file (with sentiment annotations) from the given path.
def read_file(name):
  print 'printing contents of file ' + name
  #We use try...finally to ensure that the file is closed even if there
    f = file(name, 'r')
  #We use try...finally to ensure that the file is closed even if there
    f = file(name, 'w')
    f.write('# Filename : ' + name + '\n')
  read_file(name)
  #Remove the file so we do not pollute the disk
#for root, subFolders, files in os.walk(rootdir):
#subFolders: Files in root of type directory
#files: Files in root (not in subFolders) of type other than directory
#And please use os.path.join instead of concatenating with a slash! Your problem is filePath = rootdir + '/' + file - you must concatenate the currently "walked" folder instead of the topmost folder. So that must be filePath = os.path.join(root, file). BTW "file" is a builtin, so you don't normally use it as variable name.
for root, subFolders, files in os.walk(rootdir):
    outfileName = os.path.join(root, "py-outfile.txt")
    print "outfileName is " + outfileName
    with open( outfileName, 'w' ) as folderOut:
        for filename in files:
            filePath = os.path.join(root, filename)
            with open( filePath, 'r' ) as f:
                folderOut.write("The file %s contains %s" % (filePath, toWrite))
#If you didn't know, the "with" statement for files is a shorthand:
with open("filename", "r") as f:
f = open("filename", "r")
    for root, subFolders, files in os.walk(rootdir):
        for file in files:
            if (file == 'data.txt'):
                #print file
                with open(os.path.join(root, file), 'r') as fin:
for root, dirnames, filenames in os.walk('src'):
  for filename in fnmatch.filter(filenames, '*.c'):
      matches.append(os.path.join(root, filename))
#Similar to other solutions, but using fnmatch.fnmatch instead of glob, since os.walk already listed the filenames:
def find_files(directory, pattern):
    for root, dirs, files in os.walk(directory):
        for basename in files:
                filename = os.path.join(root, basename)
                yield filename
for filename in find_files('src', '*.c'):
    print 'Found C source:', filename
# audit-tool.py - A simple one way file comparision utility.
def compareFiles(filename1, filename2, ignorecase):
  Given two filenames and an ignorecase booelean, compares filename1
  against filename2 and returns list of the differences and a count of
  how many were found. If ignorecase is 1, the contents of both files
    f1 = open(filename1, 'rU')
    print 'Could not find the specified file:', filename1
    f2 = open(filename2, 'rU')
    print 'Could not find the specified file:', filename2
    print 'usage: ./audit.py filename1 filename2 [--ignorecase]'
  filename1 = sys.argv[1]
  filename2 = sys.argv[2]
  (results, diffcount) = compareFiles(filename1, filename2, ignorecase)
    print '\nNo differences -- files are identical.'
def read_file(name):
  print 'printing contents of file ' + name
  #We use try...finally to ensure that the file is closed even if there
    f = file(name, 'r')
  #We use try...finally to ensure that the file is closed even if there
    f = file(name, 'w')
    f.write('# Filename : ' + name + '\n')
  read_file(name)
  #Remove the file so we do not pollute the disk
NWORDS = train(words(file('big.txt').read()))
# Filename : adaptive_backup.py
import tarfile
#List of directories and files to backup
tar_file = tarfile.open(bk_fn, 'w:gz')
for file in bk_src:
  tar_file.add(file)
tar_file.close()
HERE = os.path.dirname(os.path.abspath(__file__))
#This file uses code from http://norvig.com/spell-correct.html
  #and bigrams contained in the file features
    self.NWORDS, self.NWORDS2 = train(words(file('../AdditionalFiles/big.txt').read())) #It calculates counts of words and bigrams
                                                                     #from file 'big.txt'
    reader2 = file('../AdditionalFiles/ae2.txt').read()                      # It creates lexicon from
    self.lexicon = re.findall( r'(\w+)\n', str(reader2))  # file 'ae2.txt'
        self.ask_file_name()
    def ask_file_name(self):
        self.view.window().show_input_panel('File Name', self.trigger + '.sublime-snippet', self.make_snippet, None, None)
    def make_snippet(self, file_name):
        if re.match('^\w+\.sublime\-snippet$', file_name):
            file_path = os.path.join(sublime.packages_path(), 'User', file_name)
            if os.path.exists(file_path):
                if sublime.ok_cancel_dialog('Override %s?' % file_name) is False:
                    self.ask_file_name()
            file = open(file_path, "wb")
                file.write(bytes(snippet_xml, 'UTF-8'))
                file.write(bytes(snippet_xml))
            file.close()
            self.view.window().open_file(file_path)
            sublime.error_message('Please specify a valid snippet file name!! i.e. `awesome.sublime-snippet`')
            self.ask_file_name()
            [os.path.basename(filepath), filepath]
                for filepath
                self.window.open_file(snippets[index][1])
                self.window.open_file(snippets[index][1], sublime.TRANSIENT)
>2 /home/ahmed/zdotfiles/2/
/home/ahmed/zdotfiles/3/
  python-read-files.py
    for path, dirs, files in os.walk(os.path.abspath(directory)):
        for filename in fnmatch.filter(files, glob_pattern):
            filepath = os.path.join(path, filename)
            #backup orig file
                backup_path = filepath + '.bak'
                shutil.copyfile(filepath, backup_path)
            with open(filepath) as f:
            with open(filepath, "w") as f:
                print 'DBG: replacing in file', filepath
def read_file(name):
  print 'printing contents of file ' + name
  #We use try...finally to ensure that the file is closed even if there
    f = file(name, 'r')
    '''Return a list of file names in directory 'dir_name'
    Additional arguments, if any, are file extensions to add to the list.
    Example usage: fileList = dir_list(r'H:\TEMP', False, 'txt', 'py', 'dat', 'log', 'jpg')
    fileList = []
    for file in os.listdir(dir_name):
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
                fileList.append(dirfile)
                if os.path.splitext(dirfile)[1][1:] in args:
                    fileList.append(dirfile)
        # recursively access file names in subdirectories
        elif os.path.isdir(dirfile) and subdir:
            # print "Accessing directory:", dirfile
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
def combine_files(fileList, fn):
    for file in fileList:
        print 'Writing file %s' % file
        f.write(open(file).read())
    fileList = dir_list(r'/home/ahmed/alltxt/', True, 'txt' )
    combine_files(fileList, fn)
	"""Move through all files, directories, and subdirectories of a path"""
	"""Move through all files, directories, and subdirectories of a path"""
This file contains all the functions concerning analysing the
function pre_process_struc in file se_procedure.py.
# Called by find_and_label_section_sents in this file after the indices for the section being called have been calculated by find_section_paras
# Called by find_no_concl_heading_indices and other functions in this file.
# Called by several functions in this file.
# Called by several functions in this file.
# Called by find_and_label_section_sents in this file.
# Called by find_and_label_headings in this file.
# Called by find_and_label_headings in this file.
# Called by get_more_headings_using_contents in this file.
# Called by find_and_label_headings in this file.
import tarfile
def read_file(name):
  print 'printing contents of file ' + name
  #We use try...finally to ensure that the file is closed even if there
    f = file(name, 'r')
    '''Return a list of file names in directory 'dir_name'
    Additional arguments, if any, are file extensions to add to the list.
    Example usage: fileList = dir_list(r'H:\TEMP', False, 'txt', 'py', 'dat', 'log', 'jpg')
    fileList = []
    for file in os.listdir(dir_name):
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
                fileList.append(dirfile)
                if os.path.splitext(dirfile)[1][1:] in args:
                    fileList.append(dirfile)
        # recursively access file names in subdirectories
        elif os.path.isdir(dirfile) and subdir:
            # print "Accessing directory:", dirfile
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
def combine_files(fileList, fn):
    for file in fileList:
        print 'Writing file %s' % file
        f.write(open(file).read())
#List of directories and files to backup
tar_file = tarfile.open(bk_fn, 'w:gz')
for file in bk_src:
    tar_file.close()
    fn = "/home/ahmed/Dropbox/output_file.txt"
    dialog.open_file(title="Open File", initialDir="~", fileTypes="*|All Files", rememberAs=None, **kwargs) Show an Open File dialog
    dialog.save_file(title="Save As", initialDir="~", fileTypes="*|All Files", rememberAs=None, **kwargs) Show a Save As dialog
    system.create_file(fileName, contents="") Create a file with contents
  Refer to the file COPYING (which should be included in this distribution)
        """Load from a todotxt file:
        self.todotxt = todotxt  # ok, save file path
            # 'file not found' exception?
            print err  # and continue (new file created)
        """Save to a todotxt file:
        parser.error("missing todo file argument")
File: Conflict Analysis & Resolution
# code below only runs when this file is run as a script
        def delete_file(path):
            delete_file(path)
        def open_file(data,data_str,path):
            file = open(path,"wb")
            if file:
                    pickle.dump(data,file)
                file.close()
        def load_file(data,data_str,path):
                file1 = open(path,'rb')
                if file1:
                        data = pickle.load(file1)
                    open_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    open_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    open_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    index = load_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    graph = load_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    ranks = load_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
        #Write lists to file
    url_image = 'http://%s.wikipedia.org/w/index.php?title=Special:FilePath&file=%s'
    if os.path.isfile(refpath):
    if os.path.isfile(citpath):
    if os.path.isfile(topWordsPicklePath):
    imfiles = [f for f in os.listdir(pdir) if re.match(r'thumb.*\.png', f)]
    if len(imfiles)>0:
      thumbfiles = [("thumb-%d.png" % (i, )) for i in range(len(imfiles))]
      thumbs = [os.path.join('resources', pid, x) for x in thumbfiles]
outfile = os.path.join('client', 'db.json')
f = open(outfile, 'w')
def read_file(name):
  print 'printing contents of file ' + name
  #We use try...finally to ensure that the file is closed even if there
    f = file(name, 'r')
    '''Return a list of file names in directory 'dir_name'
    Additional arguments, if any, are file extensions to add to the list.
    Example usage: fileList = dir_list(r'H:\TEMP', False, 'txt', 'py', 'dat', 'log', 'jpg')
    fileList = []
    for file in os.listdir(dir_name):
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
                fileList.append(dirfile)
                if os.path.splitext(dirfile)[1][1:] in args:
                    fileList.append(dirfile)
        # recursively access file names in subdirectories
        elif os.path.isdir(dirfile) and subdir:
            # print "Accessing directory:", dirfile
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
def combine_files(fileList, fn):
    for file in fileList:
        print 'Writing file %s' % file
        f.write(open(file).read())
fileList = dir_list(r'/home/ahmed/Dropbox', False, 'txt', 'py')
for wxp in fileList:
#print fileList
fn = "/home/ahmed/Dropbox/output_file.txt"
#combine_files(fileList, fn)
def ParseLog(filename, search_string):
    f = open(filename, 'rU')
    print '\n*** I/O Error: Can\'t read file', filename, '***\n'
    print 'usage: ./checklog filename1 filename2 filename3 ...'
dialog.open_file(title="Open File", initialDir="~", fileTypes="*|All Files", rememberAs=None, **kwargs) Show an Open File dialog
dialog.save_file(title="Save As", initialDir="~", fileTypes="*|All Files", rememberAs=None, **kwargs) Show a Save As dialog
system.create_file(fileName, contents="") Create a file with contents
base_path = os.path.dirname(os.path.abspath(__file__))
# textblob documentation build configuration file, created by
# This file is execfile()d with the current directory set to its containing dir.
# autogenerated file.
# The suffix of source filenames.
# The encoding of source files.
# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# The name of an image file (relative to this directory) to place at the top
# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
# If true, an OpenSearch description file will be output, and all pages will
# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None
# Output file base name for HTML help builder.
# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
# The name of an image file (relative to this directory) to place at the top of
# (source start file, name, description, authors, manual section).
# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
HERE = os.path.abspath(os.path.dirname(__file__))
CSV_FILE = os.path.join(HERE, 'data.csv')
JSON_FILE = os.path.join(HERE, "data.json")
TSV_FILE = os.path.join(HERE, "data.tsv")
    def test_init_with_csv_file(self):
        cl = NaiveBayesClassifier(CSV_FILE, format="csv")
    def test_init_with_csv_file_without_format_specifier(self):
        cl = NaiveBayesClassifier(CSV_FILE)
    def test_init_with_json_file(self):
        cl = NaiveBayesClassifier(JSON_FILE, format="json")
    def test_init_with_json_file_without_format_specifier(self):
        cl = NaiveBayesClassifier(JSON_FILE)
    def test_accuracy_on_a_csv_file(self):
        a = self.classifier.accuracy(CSV_FILE)
    def test_accuracy_on_json_file(self):
        a = self.classifier.accuracy(JSON_FILE)
    def test_init_with_tsv_file(self):
        cl = NaiveBayesClassifier(TSV_FILE)
            NaiveBayesClassifier(CSV_FILE, format='unknown')
HERE = os.path.abspath(os.path.dirname(__file__))
HERE = os.path.abspath(os.path.dirname(__file__))
CSV_FILE = os.path.join(HERE, 'data.csv')
JSON_FILE = os.path.join(HERE, "data.json")
TSV_FILE = os.path.join(HERE, "data.tsv")
        format = formats.detect(CSV_FILE)
        format = formats.detect(JSON_FILE)
        with open(CSV_FILE, 'r') as fp:
        with open(JSON_FILE, 'r') as fp:
    def test_read_from_filename(self):
        data = formats.CSV(CSV_FILE)
        with open(CSV_FILE, 'r') as fp:
        with open(JSON_FILE, 'r') as fp:
    def test_read_from_filename(self):
        data = formats.TSV(TSV_FILE)
        with open(TSV_FILE, 'r') as fp:
        with open(CSV_FILE, 'r') as fp:
    def test_read_from_filename(self):
        formats.JSON(JSON_FILE)
        with open(JSON_FILE, 'r') as fp:
        with open(CSV_FILE, 'r') as fp:
        d = formats.JSON(JSON_FILE)
                        os.path.isfile(os.path.join(fn, '__init__.py'))):
    '''Attempts to find the version number in the file names fname.
PACKAGE_DIR = os.path.dirname(os.path.abspath(__file__))
    item = treebank._fileids[0]
    def read_rule (self, filename):
        rules = load('nltk:stemmers/rslp/' + filename, format='raw').decode("utf8")
#            infile = open(f, 'r')
#                w = infile.readline()
    for item in treebank.files()[:3]:
# in the file VERSION.
    # If a VERSION file exists, use it!
    version_file = os.path.join(os.path.dirname(__file__), 'VERSION')
    with open(version_file) as fh:
NLTK_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    # XXX: imports can't be moved to the top of the file
        #'--xunit-file=$WORKSPACE/nosetests.xml',
project's ``setup-eggs.py`` file.  Here, we create a test suite that
    #print "-----", glob(os.path.join(os.path.dirname(__file__), '*.doctest'))
    dir = os.path.dirname(__file__)
    files = [ os.path.basename(path) for path in paths ]
        [ doctest.DocFileSuite(file) for file in files ]
        'corpora/inaugural/README', # A very short file (160 chars)
        'corpora/inaugural/1793-Washington.txt', # A relatively short file (791 chars)
        'corpora/inaugural/1909-Taft.txt', # A longer file (32k chars)
                file_data = fp.read().decode('utf8')
            yield f, file_data
        for f, file_data in self.data():
            self.assertEqual(list(v), file_data.split())
            self.assertEqual(list(v), self.linetok.tokenize(file_data))
        for f, file_data in self.data():
            self.assertEqual(len(v), len(file_data.split()))
            self.assertEqual(len(v), len(self.linetok.tokenize(file_data)))
        for name in udhr.fileids():
        for name in udhr.fileids():
@skipIf(not ptb.fileids(), "A full installation of the Penn Treebank is not available")
    def test_fileids(self):
            ptb.fileids()[:4],
    def test_news_fileids(self):
            ptb.fileids('news')[:3],
    This is a test file.
    "This file can be encoded with latin1. \x83",
    This is a test file.
    This is a test file.
This is a larger file.  It has some lines that are longer \
from nose.plugins.doctests import Doctest, log, DocFileCase
        changes the encoding of doctest files
    def loadTestsFromFileUnicode(self, filename):
        if self.extension and anyp(filename.endswith, self.extension):
            name = os.path.basename(filename)
            dh = codecs.open(filename, 'r', self.options.get('doctestencoding'))
            globs = {'__file__': filename}
                dirname = os.path.dirname(filename)
                filename=filename, lineno=0)
                case = DocFileCase(
    def loadTestsFromFile(self, filename):
        cases = self.loadTestsFromFileUnicode(filename)
def hole_readings(sentence, grammar_filename=None, verbose=False):
    if not grammar_filename:
        grammar_filename = 'grammars/sample_grammars/hole.fcfg'
    if verbose: print('Reading grammar file', grammar_filename)
    parser = load_parser(grammar_filename)
import tempfile
#        if 'ERROR: input file contains no ccg/2 terms.' in boxer_out:
        :param filename: str A filename for the output file
            fd, temp_filename = tempfile.mkstemp(prefix='boxer-', suffix='.in', text=True)
                '--input', temp_filename]
        os.remove(temp_filename)
files.
relation files ('the world database'), and convert then into a format
input files are available in the NLTK corpora directory.
The Chat-80 World Database consists of the following files::
a set of Prolog rules have been omitted. The modified file is named
``world1.pl``. Currently, the file ``rivers.pl`` is not read in, since
Reading Chat-80 Files
the files ``borders.pl`` and ``contains.pl``. These contain facts of the
          'filename': 'cities.pl'}
According to this, the file ``city['filename']`` contains a list of
The set of rules is written to the file ``chat_pnames.cfg`` in the
           'filename': 'borders.pl'}
            'filename': 'contain.pl'}
        'filename': 'cities.pl'}
           'filename': 'countries.pl'}
                 'filename': 'world1.pl'}
                 'filename': 'world1.pl'}
             'filename': 'world1.pl'}
          'filename': 'world1.pl'}
         'filename': 'world1.pl'}
       'filename': 'world1.pl'}
def clause2concepts(filename, rel_name, schema, closures=[]):
    Convert a file of Prolog clauses into a list of ``Concept`` objects.
    :param filename: filename containing the relations
    :type filename: str
    # convert a file into a list of lists
    records = _str2records(filename, rel_name)
    if not filename in not_unary:
def cities2table(filename, rel_name, dbname, verbose=False, setup=False):
    Convert a file of Prolog clauses into a database table.
    :param filename: filename containing the relations
    :type filename: str
    :param dbname: filename of persistent store
    records = _str2records(filename, rel_name)
    :param dbname: filename of persistent store
        warnings.warn("Make sure the database file %s is installed and uncompressed." % dbname)
def _str2records(filename, rel):
    Read a file into memory and convert each relation clause into a list.
    contents = nltk.data.load("corpora/chat80/%s" % filename, format="text")
        filename = rel['filename']
        concept_list = clause2concepts(filename, rel_name, schema, closures)
    :param db: name of file to which data is written.
    :param db: name of file from which data is read.
        sys.exit("Cannot read file: %s" % dbname)
Extract data from the Chat-80 Prolog files and convert them into a
                    help="write a file of lexical entries for country names, then exit")
                sys.exit("Cannot read file: %s" % dbname)
    Parse a line in a valuation file.
    Convert a valuation file into a valuation.
    :param s: the contents of a valuation file
def read_sents(filename, encoding='utf8'):
    with codecs.open(filename, 'r', encoding) as fp:
                        help="read in a file of test sentences S", metavar="S")
    gramfile = 'grammars/sample_grammars/sem2.fcfg'
        sentsfile = options.sentences
        gramfile = options.grammar
        sents = read_sents(sentsfile)
            batch_evaluate(sents, gramfile, model, g, trace=options.semtrace)
            batch_interpret(sents, gramfile, trace=options.syntrace)
    Convert a file of First Order Formulas into a list of {Expression}s.
    :param s: the contents of the file
    for file in ieer.fileids():
        for doc in ieer.parsed_docs(file):
    for file in ieer.fileids():
        for doc in ieer.parsed_docs(file):
    trees = [doc.headline for file in ieer.fileids() for doc in ieer.parsed_docs(file)]
    def __init__(self, filename, encoding=None):
        self.filename = filename
        self.file_encoding = encoding
        self.read_file()
    def read_file(self, empty_first=True):
            contents = nltk.data.load(self.filename, format='text', encoding=self.file_encoding)
            # TODO: the above can't handle zip files, but this should anyway be fixed in nltk.data.load()
                contents = nltk.data.load('file:' + self.filename, format='text', encoding=self.file_encoding)
    def __init__(self, semtype_file=None, remove_duplicates=False,
        if semtype_file:
            self.semtype_file = semtype_file
            self.semtype_file = 'glue.semtype'
            self.depparser.train_from_file(nltk.data.find(
        return GlueDict(self.semtype_file)
    def __init__(self, semtype_file=None, remove_duplicates=False,
        if not semtype_file:
            semtype_file = 'drt_glue.semtype'
        Glue.__init__(self, semtype_file, remove_duplicates, depparser, verbose)
        return DrtGlueDict(self.semtype_file)
        filemenu = Menu(menubar, tearoff=0)
        filemenu.add_command(label='Exit', underline=1,
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
        self._cframe.print_to_file()
    >>> for fileid in gutenberg.fileids(): # doctest: +SKIP
    ...     doc = gutenberg.words(fileid) # doctest: +SKIP
    ...     print fileid, classifier.classify(document_features(doc)) # doctest: +SKIP
import tempfile
from nltk.classify.megam import call_megam, write_megam_file, parse_megam_weights
from nltk.classify.tadm import call_tadm, write_tadm_file, parse_tadm_weights
# [xx] possible extension: add support for using implicit file format;
    # Write a training file for megam.
        fd, trainfile_name = tempfile.mkstemp(prefix='nltk-')
        trainfile = open(trainfile_name, 'w')
        write_megam_file(train_toks, encoding, trainfile, \
        trainfile.close()
        raise ValueError('Error while creating megam training file: %s' % e)
    # Run megam on the training file.
    options += ['multiclass', trainfile_name]
    # Delete the training file
    try: os.remove(trainfile_name)
        print('Warning: unable to delete %s: %s' % (trainfile_name, e))
        trainfile_fd, trainfile_name = \
            tempfile.mkstemp(prefix='nltk-tadm-events-', suffix='.gz')
        weightfile_fd, weightfile_name = \
            tempfile.mkstemp(prefix='nltk-tadm-weights-')
        trainfile = gzip_open_unicode(trainfile_name, 'w')
        write_tadm_file(train_toks, encoding, trainfile)
        trainfile.close()
        options.extend(['-events_in', trainfile_name])
        options.extend(['-params_out', weightfile_name])
        with open(weightfile_name, 'r') as weightfile:
            weights = parse_tadm_weights(weightfile)
        os.remove(trainfile_name)
        os.remove(weightfile_name)
def write_megam_file(train_toks, encoding, stream,
    Generate an input file for ``megam`` based on the given corpus of
    :param stream: The stream to which the megam input file should be
    # Write the file, which contains one line per instance.
        # For implicit file formats, just list the features that fire
def write_tadm_file(train_toks, encoding, stream):
    Generate an input file for ``tadm`` based on the given corpus of
    :param stream: The stream to which the ``tadm`` input file should be
    # See the following for a file format description:
def parse_tadm_weights(paramfile):
    for line in paramfile:
    write_tadm_file(tokens, encoding, sys.stdout)
import tempfile
import zipfile
        zf = zipfile.ZipFile(jar)
    def __init__(self, formatter, model_filename):
        self._model = model_filename
        temp_dir = tempfile.mkdtemp()
            # Write the test data file.
            test_filename = os.path.join(temp_dir, 'test.arff')
            self._formatter.write(test_filename, featuresets)
                   '-l', self._model, '-T', test_filename] + options
    def train(cls, model_filename, featuresets,
        temp_dir = tempfile.mkdtemp()
            # Write the training data file.
            train_filename = os.path.join(temp_dir, 'train.arff')
            formatter.write(train_filename, featuresets)
            cmd = [javaclass, '-d', model_filename, '-t', train_filename]
            return WekaClassifier(formatter, model_filename)
    def write(self, outfile, tokens):
        """Writes ARFF data to a file for the given data."""
        if not hasattr(outfile, 'write'):
            outfile = open(outfile, 'w')
        outfile.write(self.format(tokens))
        outfile.close()
        s = ('% Weka ARFF file\n' +
    _mallet_classpath = os.path.pathsep.join(os.path.join(lib_dir, filename)
                                  for filename in sorted(os.listdir(lib_dir))
                                  if filename.endswith('.jar'))
    modified to include both ``nltk.jar`` and all the ``.jar`` files defined by
import tkinter.font, tkinter.messagebox, tkinter.filedialog
            self._parent.bind('<Control-p>', lambda e: self.print_to_file())
        filemenu = Menu(menubar, tearoff=0)
        filemenu.add_command(label='Print to Postscript', underline=0,
                             command=self.print_to_file, accelerator='Ctrl-p')
        filemenu.add_command(label='Exit', underline=1,
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
    def print_to_file(self, filename=None):
        file.  If no filename is given, then prompt the user for one.
        :param filename: The name of the file to print the tree to.
        :type filename: str
        if filename is None:
            from tkinter.filedialog import asksaveasfilename
            ftypes = [('Postscript files', '.ps'),
                      ('All files', '*')]
            filename = asksaveasfilename(filetypes=ftypes,
            if not filename: return
        self._canvas.postscript(file=filename, x=x0, y=y0,
        self._top.bind('<Control-p>', self._cframe.print_to_file)
        filemenu = Menu(menubar, tearoff=0)
        filemenu.add_command(label='Print to Postscript', underline=0,
                             command=self._cframe.print_to_file,
        filemenu.add_command(label='Exit', underline=1,
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
        if not path.isfile(self.executable):
    if not path.isfile(tagger.executable):
from tempfile import mkstemp
import zipfile
    Ecah MalletCRF object is backed by a crf model file.  This
    model file is actually a zip file, and it contains one file for
    the serialized model ``crf-model.ser`` and one file for
    :param filename: The filename of the model file that backs this CRF.
        only needs to be given if the model file does not contain
    def __init__(self, filename, feature_detector=None):
        # Read the CRFInfo from the model file.
        zf = zipfile.ZipFile(filename)
    def filename(self):
        The filename of the crf model file that backs this
        MalletCRF.  The crf model file is actually a zip file, and
        it contains one file for the serialized model
        ``crf-model.ser`` and one file for information about the
        return self.crf_info.model_filename
        # Write the test corpus to a temporary file
        (fd, test_file) = mkstemp('.txt', 'test')
            # Run mallet on the test file.
                '--model-file', os.path.abspath(self.crf_info.model_filename),
                '--test-file', test_file], stdout='pipe')
            os.remove(test_file)
    def train(cls, feature_detector, corpus, filename=None,
        model file, containing both a serialized Mallet model and
        information about the CRF's structure.  This crf model file
        it, you must delete it manually.  The filename of the model
        file for a MalletCRF crf is available as ``crf.filename()``.
        :type filename: str
        :param filename: The filename that should be used for the crf
            model file that backs the new MalletCRF.  If no
            filename is given, then a new filename will be chosen
        # If they did not supply a model filename, then choose one.
        if filename is None:
            (fd, filename) = mkstemp('.crf', 'model')
        # Ensure that the filename ends with '.zip'
        if not filename.endswith('.crf'):
            filename += '.crf'
            print('[MalletCRF] Training a new CRF: %s' % filename)
            add_end_state, filename, feature_detector)
        # Create a zipfile, and write crf-info to it.
            print('[MalletCRF] Adding crf-info.xml to %s' % filename)
        zf = zipfile.ZipFile(filename, mode='w')
        crf = MalletCRF(filename, feature_detector)
        # Write the Training corpus to a temporary file.
        (fd, train_file) = mkstemp('.txt', 'train')
                   '--model-file', os.path.abspath(filename),
                   '--train-file', train_file]
            # Delete the temp file containing the training corpus.
            os.remove(train_file)
            print('[MalletCRF]   Model stored in: %s' % filename)
                        model_filename, feature_detector):
                       model_filename, feature_detector)
        return 'MalletCRF(%r)' % self.crf_info.model_filename
    serialized to an XML file, which can then be read by NLTK's custom
                 add_start_state, add_end_state, model_filename,
        self.model_filename = model_filename
        '  <modelFile>%(model_filename)s</modelFile>\n'
                       etree.find('modelFile').text,
    def write(self, filename, encoding='utf8'):
        with codecs.open(filename, 'w', encoding) as out:
    print('Clean-up: deleting', crf.filename)
    os.remove(crf.filename)
def _load_universal_map(fileid):
    contents = load(join(_UNIVERSAL_DATA, fileid+'.map'), format="text")
        assert fine not in _MAPPINGS[fileid]['universal'], 'Multiple entries for original tag: {}'.format(fine)
        _MAPPINGS[fileid]['universal'][fine] = coarse
import random        # for shuffling WSJ files
import yaml          # to save and load taggers in files
    :param error_output: the file where errors will be saved
    :param rule_output: the file where rules will be saved
    print_rules = file(rule_output, 'w')
    error_file = file(error_output, 'w')
    error_file.write('Errors for Brill Tagger %r\n\n' % rule_output)
        error_file.write(e+'\n')
    error_file.close()
import tempfile
from nltk.internals import find_file, find_jar, config_java, java, _java_options
    - ``_JAR`` file: Class constant that represents the jar file name.
        self._stanford_model = find_file(path_to_model,
        # Create a temporary input file
        _input_fh, self._input_file_path = tempfile.mkstemp(text=True)
        # Write the actual sentences to the temporary input file
        # Delete the temporary file
        os.unlink(self._input_file_path)
     - (optionally) the path to the stanford tagger jar file. If not specified here,
       then this jar file must be specified in the CLASSPATH envinroment variable.
                '-model', self._stanford_model, '-textFile', \
                self._input_file_path, '-tokenize', 'false']
    - (optionally) the path to the stanford tagger jar file. If not specified here,
      then this jar file must be specified in the CLASSPATH envinroment variable.
                '-loadClassifier', self._stanford_model, '-textFile', \
                self._input_file_path, '-outputFormat', self._FORMAT]
from nltk.internals import find_binary, find_file
        :param path_to_model: The model file.
        self._hunpos_model = find_file(path_to_model,
that can be used to read corpus files in a variety of formats.  These
functions can be used to read both the corpus files that are
distributed in the NLTK corpus package, and corpus files that are part
- If ``item`` is a filename, then that file will be read.
    cat_file='cats.txt', tagset='brown', encoding="ascii")
    cat_file='cats.txt', textid_file='textids.txt', encoding='utf8')
    cat_file='allcats.txt', tagset='wsj')
    cat_file='cats.txt', encoding='ISO-8859-2')
    'semcor', SemcorCorpusReader, r'brown./tagfiles/br-.*\.xml')
    lambda filename: re.sub(r'^wsj/\d\d/', '', filename),
    lambda filename: re.sub(r'^wsj/\d\d/', '', filename),
    lambda filename: filename.upper(),
    lambda filename: filename.upper(),
	def __init__(self, corpus_file,
		StreamBackedCorpusView.__init__(self, corpus_file, startpos=headLen)
		if 'textid_file' in kwargs: self._textids = kwargs['textid_file']
				file_id, text_ids = line.split(' ', 1)
				if file_id not in self.fileids():
					raise ValueError('In text_id mapping file %s: %s '
									 'not found' % (catfile, file_id))
					self._add_textids(file_id, text_id)
	def _add_textids(self, file_id, text_id):
		self._f2t[file_id].append(text_id)
		self._t2f[text_id].append(file_id)
	def _resolve(self, fileids, categories, textids=None):
		if fileids is not None:
				tmp = fileids, None
				raise ValueError('Specify only fileids, categories or textids')
				tmp = self.fileids(categories), None
				raise ValueError('Specify only fileids, categories or textids')
				files = sum((self._t2f[t] for t in textids), [])
				for f in files:
				tmp = files, tdict
				raise ValueError('Specify only fileids, categories or textids')
	def textids(self, fileids=None, categories=None):
		file and thus both methods provide identical functionality. In order
		fileids, _ = self._resolve(fileids, categories)
		if fileids is None: return sorted(self._t2f)
		if isinstance(fileids, compat.string_types):
			fileids = [fileids]
		return sorted(sum((self._f2t[d] for d in fileids), []))
	def words(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
										textids=textids[fileid])
								for fileid in fileids])
			return concat([TEICorpusView(self.abspath(fileid),
								for fileid in fileids])
	def sents(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
										textids=textids[fileid])
								for fileid in fileids])
			return concat([TEICorpusView(self.abspath(fileid),
								for fileid in fileids])
	def paras(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
										textids=textids[fileid])
								for fileid in fileids])
			return concat([TEICorpusView(self.abspath(fileid),
								for fileid in fileids])
	def tagged_words(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
										textids=textids[fileid])
								for fileid in fileids])
			return concat([TEICorpusView(self.abspath(fileid),
								for fileid in fileids])
	def tagged_sents(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
										textids=textids[fileid])
								for fileid in fileids])
			return concat([TEICorpusView(self.abspath(fileid),
								for fileid in fileids])
	def tagged_paras(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
										textids=textids[fileid])
								for fileid in fileids])
			return concat([TEICorpusView(self.abspath(fileid),
								for fileid in fileids])
	def xml(self, fileids=None, categories=None):
		fileids, _ = self._resolve(fileids, categories)
		if len(fileids) == 1: return XMLCorpusReader.xml(self, fileids[0])
		else: raise TypeError('Expected a single file')
	def raw(self, fileids=None, categories=None):
		fileids, _ = self._resolve(fileids, categories)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
		return concat([self.open(f).read() for f in fileids])
NIST 1999 IE-ER Evaluation.  The files were taken from the
and filenames were shortened.
The corpus contains the following files: APW_19980314, APW_19980424,
    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def docs(self, fileids=None):
        return concat([StreamBackedCorpusView(fileid, self._read_block,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def parsed_docs(self, fileids=None):
        return concat([StreamBackedCorpusView(fileid,
                       for (fileid, enc) in self.abspaths(fileids, True)])
        :param root: root directory containing thesaurus LISP files
        for path, encoding, fileid in self.abspaths(include_encoding=True, include_fileid=True):
            with open(path) as lin_file:
                for line in lin_file:
                            self._thesaurus[fileid][key][ngram.strip('"')] = float(score)
    def similarity(self, ngram1, ngram2, fileid=None):
        :param fileid: thesaurus fileid to search in. If None, search all fileids.
        :type fileid: C{string}
        :return: If fileid is specified, just the score for the two ngrams; otherwise,
                 list of tuples of fileids and scores.
            if fileid:
                return [(fid, 1.0) for fid in self._fileids]
            if fileid:
                return self._thesaurus[fileid][ngram1][ngram2] if ngram2 in self._thesaurus[fileid][ngram1] else self._badscore
                                  else self._badscore)) for fid in self._fileids]
    def scored_synonyms(self, ngram, fileid=None):
        :param fileid: thesaurus fileid to search in. If None, search all fileids.
        :type fileid: C{string}
        :return: If fileid is specified, list of tuples of scores and synonyms; otherwise,
                 list of tuples of fileids and lists, where inner lists consist of tuples of
        if fileid:
            return self._thesaurus[fileid][ngram].items()
            return [(fileid, self._thesaurus[fileid][ngram].items()) for fileid in self._fileids]
    def synonyms(self, ngram, fileid=None):
        :param fileid: thesaurus fileid to search in. If None, search all fileids.
        :type fileid: C{string}
        :return: If fileid is specified, list of synonyms; otherwise, list of tuples of fileids and
        if fileid:
            return self._thesaurus[fileid][ngram].keys()
            return [(fileid, self._thesaurus[fileid][ngram].keys()) for fileid in self._fileids]
        return reduce(lambda accum, fileid: accum or (ngram in self._thesaurus[fileid]), self._fileids, False)
    print(thes.synonyms(word1, fileid="simN.lsp"))
    print(thes.synonyms(word1, fileid="simN.lsp"))
    def __init__(self, root, fileids, wrap_etree=False, tagset=None):
        XMLCorpusReader.__init__(self, root, fileids, wrap_etree)
    def xml_posts(self, fileids=None):
            return concat([XMLCorpusView(fileid, 'Session/Posts/Post',
                           for fileid in self.abspaths(fileids)])
            return concat([XMLCorpusView(fileid, 'Session/Posts/Post')
                           for fileid in self.abspaths(fileids)])
    def posts(self, fileids=None):
        return concat([XMLCorpusView(fileid, 'Session/Posts/Post/terminals',
                       for fileid in self.abspaths(fileids)])
    def tagged_posts(self, fileids=None, tagset=None):
        return concat([XMLCorpusView(fileid, 'Session/Posts/Post/terminals',
                       for fileid in self.abspaths(fileids)])
    def words(self, fileids=None):
        return LazyConcatenation(self.posts(fileids))
    def tagged_words(self, fileids=None, tagset=None):
        return LazyConcatenation(self.tagged_posts(fileids, tagset))
    def __init__(self, root, fileids,
        :param fileids: A list or regexp specifying the fileids in this corpus.
        CorpusReader.__init__(self, root, fileids, encoding)
    def raw(self, fileids=None):
        :return: the given file(s) as a single string.
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def words(self, fileids=None):
        :return: the given file(s) as a list of words
                       for (path, enc, fileid)
                       in self.abspaths(fileids, True, True)])
    def sents(self, fileids=None):
        :return: the given file(s) as a list of
                       for (path, enc, fileid)
                       in self.abspaths(fileids, True, True)])
    def paras(self, fileids=None):
        :return: the given file(s) as a list of
                       for (path, enc, fileid)
                       in self.abspaths(fileids, True, True)])
    categories based on their file identifiers.
        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to
    def _resolve(self, fileids, categories):
        if fileids is not None and categories is not None:
            raise ValueError('Specify fileids or categories, not both')
            return self.fileids(categories)
            return fileids
    def raw(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def words(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def sents(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def paras(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def chapters(self, fileids=None):
        :return: the given file(s) as a list of
        return concat([self.CorpusView(fileid, self._read_para_block,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def paras(self, fileids=None):
that can be used to read corpus fileids in a variety of formats.  These
functions can be used to read both the corpus fileids that are
distributed in the NLTK corpus package, and corpus fileids that are part
- If ``item`` is a fileid, then that file will be read.
    'PlaintextCorpusReader', 'find_corpus_fileids',
    def __init__(self, root, fileids, extension='',
        :param fileids: A list or regexp specifying the fileids in this corpus.
        CorpusReader.__init__(self, root, fileids, encoding)
    def raw(self, fileids=None):
        :return: the given file(s) as a single string.
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def words(self, fileids=None):
        :return: the given file(s) as a list of words
                       for (f, enc) in self.abspaths(fileids, True)])
    def sents(self, fileids=None):
        :return: the given file(s) as a list of
                       for (f, enc) in self.abspaths(fileids, True)])
    def paras(self, fileids=None):
        :return: the given file(s) as a list of
                       for (f, enc) in self.abspaths(fileids, True)])
    def tagged_words(self, fileids=None):
        :return: the given file(s) as a list of tagged
                       for (f, enc) in self.abspaths(fileids, True)])
    def tagged_sents(self, fileids=None):
        :return: the given file(s) as a list of
                       for (f, enc) in self.abspaths(fileids, True)])
    def tagged_paras(self, fileids=None):
        :return: the given file(s) as a list of
                       for (f, enc) in self.abspaths(fileids, True)])
    def chunked_words(self, fileids=None):
        :return: the given file(s) as a list of tagged
                       for (f, enc) in self.abspaths(fileids, True)])
    def chunked_sents(self, fileids=None):
        :return: the given file(s) as a list of
                       for (f, enc) in self.abspaths(fileids, True)])
    def chunked_paras(self, fileids=None):
        :return: the given file(s) as a list of
                       for (f, enc) in self.abspaths(fileids, True)])
    def __init__(self, fileid, encoding, tagged, group_by_sent,
        StreamBackedCorpusView.__init__(self, fileid, encoding=encoding)
The YCOE corpus is divided into 100 files, each representing
        documents = set(f[:-4] for f in self._psd_reader.fileids())
        if set(f[:-4] for f in self._pos_reader.fileids()) != documents:
        fileids = sorted(['%s.psd' % doc for doc in documents] +
        CorpusReader.__init__(self, root, fileids, encoding)
    def documents(self, fileids=None):
        this corpus, or for the documents with the given file(s) if
        if fileids is None:
        if isinstance(fileids, compat.string_types):
            fileids = [fileids]
        for f in fileids:
            if f not in self._fileids:
                raise KeyError('File id %s not found' % fileids)
        return sorted(set(f[:-4] for f in fileids))
    def fileids(self, documents=None):
        Return a list of file identifiers for the files that make up
            return self._fileids
    def _getfileids(self, documents, subcorpus):
        Helper that selects the appropriate fileids for a given set of
                            'Expected a document identifier, not a file '
        return self._pos_reader.words(self._getfileids(documents, 'pos'))
        return self._pos_reader.sents(self._getfileids(documents, 'pos'))
        return self._pos_reader.paras(self._getfileids(documents, 'pos'))
        return self._pos_reader.tagged_words(self._getfileids(documents, 'pos'))
        return self._pos_reader.tagged_sents(self._getfileids(documents, 'pos'))
        return self._pos_reader.tagged_paras(self._getfileids(documents, 'pos'))
        return self._psd_reader.parsed_sents(self._getfileids(documents, 'psd'))
    def __init__(self, root, fileids, lazy=True):
        XMLCorpusReader.__init__(self, root, fileids)
    def words(self, fileids=None, strip_space=True, stem=False):
        :return: the given file(s) as a list of words
            return concat([BNCWordView(fileid, False, None,
                           for fileid in self.abspaths(fileids)])
            return concat([self._words(fileid, False, None,
                           for fileid in self.abspaths(fileids)])
    def tagged_words(self, fileids=None, c5=False, strip_space=True, stem=False):
        :return: the given file(s) as a list of tagged
            return concat([BNCWordView(fileid, False, tag, strip_space, stem)
                           for fileid in self.abspaths(fileids)])
            return concat([self._words(fileid, False, tag, strip_space, stem)
                           for fileid in self.abspaths(fileids)])
    def sents(self, fileids=None, strip_space=True, stem=False):
        :return: the given file(s) as a list of
            return concat([BNCWordView(fileid, True, None, strip_space, stem)
                           for fileid in self.abspaths(fileids)])
            return concat([self._words(fileid, True, None, strip_space, stem)
                           for fileid in self.abspaths(fileids)])
    def tagged_sents(self, fileids=None, c5=False, strip_space=True,
        :return: the given file(s) as a list of
            return concat([BNCWordView(fileid, True, tag, strip_space, stem)
                           for fileid in self.abspaths(fileids)])
            return concat([self._words(fileid, True, tag, strip_space, stem)
                           for fileid in self.abspaths(fileids)])
    def _words(self, fileid, bracket_sent, tag, strip_space, stem):
        :param fileid: The name of the underlying file.
        xmldoc = ElementTree.parse(fileid).getroot()
    def __init__(self, fileid, sent, tag, strip_space, stem):
        :param fileid: The name of the underlying file.
        XMLCorpusView.__init__(self, fileid, tagspec)
    def __init__(self, root, fileids, comment_char=None,
        :param fileids: A list or regexp specifying the fileids in this corpus.
        CorpusReader.__init__(self, root, fileids, encoding)
    divided into categories based on their file identifiers.
        (C{cat_pattern}, C{cat_map}, and C{cat_file}) are passed to
    def _resolve(self, fileids, categories):
        if fileids is not None and categories is not None:
            raise ValueError('Specify fileids or categories, not both')
            return self.fileids(categories)
            return fileids
    def raw(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def words(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def sents(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def paras(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def tagged_words(self, fileids=None, categories=None, tagset=None):
            self, self._resolve(fileids, categories), tagset)
    def tagged_sents(self, fileids=None, categories=None, tagset=None):
            self, self._resolve(fileids, categories), tagset)
    def tagged_paras(self, fileids=None, categories=None, tagset=None):
            self, self._resolve(fileids, categories), tagset)
    def parsed_words(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def parsed_sents(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def parsed_paras(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    files" which define the argument labels used by the annotations,
    on a per-noun basis.  Each "frameset file" contains one or more
    each "roleset", the frameset file provides descriptions of the
    def __init__(self, root, nomfile, framefiles='',
                 nounsfile=None, parse_fileid_xform=None,
        :param nomfile: The name of the file containing the predicate-
        :param framefiles: A list or regexp specifying the frameset
            fileids for this corpus.
        :param parse_fileid_xform: A transform that should be applied
            to the fileids in this corpus.  This should be a function
            of one argument (a fileid) that returns a string (the new
            fileid).
        # If framefiles is specified as a regexp, expand it.
        if isinstance(framefiles, string_types):
            framefiles = find_corpus_fileids(root, framefiles)
        framefiles = list(framefiles)
        CorpusReader.__init__(self, root, [nomfile, nounsfile] + framefiles,
        # Record our frame fileids & nom file.
        self._nomfile = nomfile
        self._framefiles = framefiles
        self._nounsfile = nounsfile
        self._parse_fileid_xform = parse_fileid_xform
    def raw(self, fileids=None):
        :return: the text contents of the given fileids, as a single string.
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
        return StreamBackedCorpusView(self.abspath(self._nomfile),
                                      encoding=self.encoding(self._nomfile))
        each line in the predicate-argument annotation file.
        return StreamBackedCorpusView(self.abspath(self._nomfile),
                                      encoding=self.encoding(self._nomfile))
        framefile = 'frames/%s.xml' % baseform
        if framefile not in self._framefiles:
            raise ValueError('Frameset file for %s not found' %
        # n.b.: The encoding for XML fileids is specified by the file
        etree = ElementTree.parse(self.abspath(framefile).open()).getroot()
                             (roleset_id, framefile))
            framefile = 'frames/%s.xml' % baseform
            if framefile not in self._framefiles:
                raise ValueError('Frameset file for %s not found' %
            framefiles = [framefile]
            framefiles = self._framefiles
        for framefile in framefiles:
            # n.b.: The encoding for XML fileids is specified by the file
            etree = ElementTree.parse(self.abspath(framefile).open()).getroot()
        in this corpus (from the nombank.1.0.words file).
        return StreamBackedCorpusView(self.abspath(self._nounsfile),
                                      encoding=self.encoding(self._nounsfile))
                    line, self._parse_fileid_xform,
    def __init__(self, fileid, sentnum, wordnum, baseform, sensenumber,
        self.fileid = fileid
        """The name of the file containing the parse tree for this
        """The sentence number of this sentence within ``fileid``.
                (self.fileid, self.sentnum, self.wordnum))
        s = '%s %s %s %s %s' % (self.fileid, self.sentnum, self.wordnum,
        if self.fileid not in self.parse_corpus.fileids(): return None
        return self.parse_corpus.parsed_sents(self.fileid)[self.sentnum]
    def parse(s, parse_fileid_xform=None, parse_corpus=None):
        (fileid, sentnum, wordnum,
        # Apply the fileid selector, if any.
        if parse_fileid_xform is not None:
            fileid = parse_fileid_xform(fileid)
        return NombankInstance(fileid, sentnum, wordnum, baseform, sensenumber,
    def words(self, fileids=None):
        return line_tokenize(self.raw(fileids))
    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def entries(self, fileids=None):
        :return: a tuple of words for the specified fileids.
        if not fileids:
            fileids = self.fileids()
        wordlists = [self.words(f) for f in fileids]
Toolbox databases and settings fileids.
    def xml(self, fileids, key=None):
                       for (path, enc) in self.abspaths(fileids, True)])
    def fields(self, fileids, strip=True, unwrap=True, encoding='utf8',
        return concat([list(ToolboxData(fileid,enc).fields(
                       for (fileid, enc)
                       in self.abspaths(fileids, include_encoding=True)])
    def entries(self, fileids, **kwargs):
        for marker, contents in self.fields(fileids, **kwargs):
    def words(self, fileids, key='lx'):
        return [contents for marker, contents in self.fields(fileids) if marker == key]
    def raw(self, fileids):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
        and a method for detecting the sentence blocks in corpus files.
    def __init__(self, root, fileids, encoding='utf8', morphs2str=_morphs2str_default):
        CorpusReader.__init__(self, root, fileids, encoding)
    fileids = [f for f in find_corpus_fileids(FileSystemPathPointer(root), ".*")
    def _knbc_fileids_sort(x):
                            sorted(fileids, key=_knbc_fileids_sort), encoding='euc-jp')
    print(knbc.fileids()[:10])
The NLTK version of the Senseval 2 files uses well-formed XML.
    def instances(self, fileids=None):
        return concat([SensevalCorpusView(fileid, enc)
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def raw(self, fileids=None):
        :return: the text contents of the given fileids, as a single string.
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def __init__(self, fileid, encoding):
        StreamBackedCorpusView.__init__(self, fileid, encoding=encoding)
import tempfile
from nltk.data import PathPointer, FileSystemPathPointer, ZipFilePathPointer
    A 'view' of a corpus file, which acts like a sequence of tokens:
    a corpus fileid (specified as a string or as a ``PathPointer``);
    index to file position, with one entry per block.  When a token
      1. First, it searches the toknum/filepos mapping for the token
      2. Then, starting at the file position corresponding to that
    The toknum/filepos mapping is created lazily: it is initially
    initial token is added to the mapping.  (Thus, the toknum/filepos
    :note: Each ``CorpusView`` object internally maintains an open file
        object for its underlying corpus file.  This file should be
        closed, the file object will be automatically re-opened.
    :warning: If the contents of the file are modified during the
        a single block from the underlying file stream.
        with ``_filepos``, this forms a partial mapping between token
        indices and file positions.
    :ivar _filepos: A list containing the file position of each block
        file position of the first character in block ``i``.  Together
        indices and file positions.
    :ivar _stream: The stream used to access the underlying corpus file.
        file.  This is calculated when the corpus view is initialized,
        and is used to decide when the end of file has been reached.
    def __init__(self, fileid, block_reader=None, startpos=0,
        Create a new corpus view, based on the file ``fileid``, and
        :param fileid: The path to the file that is read by this
            corpus view.  ``fileid`` can either be a string or a
        :param startpos: The file position at which the view will
            read the file's contents.  If no encoding is specified,
            then the file's contents will be read as a non-unicode
        # Initialize our toknum/filepos mapping.
        self._filepos = [startpos]
        self._fileid = fileid
        # Find the length of the file.
            if isinstance(self._fileid, PathPointer):
                self._eofpos = self._fileid.file_size()
                self._eofpos = os.stat(self._fileid).st_size
                             (fileid, exc))
    fileid = property(lambda self: self._fileid, doc="""
        The fileid of the file that is accessed by this view.
        Open the file stream associated with this corpus view.  This
        while its file stream is closed.
        if isinstance(self._fileid, PathPointer):
            self._stream = self._fileid.open(self._encoding)
                open(self._fileid, 'rb'), self._encoding)
            self._stream = open(self._fileid, 'rb')
        Close the file stream associated with this corpus view.  This
        can be useful if you are worried about running out of file
            # of the file:
        # Decide where in the file we should start.  If `start` is in
            filepos = self._filepos[block_index]
            filepos = self._filepos[-1]
        while filepos < self._eofpos:
            self._stream.seek(filepos)
            new_filepos = self._stream.tell()
            assert new_filepos > filepos, (
                'block reader %s() should consume at least 1 byte (filepos=%d)' %
                (self.read_block.__name__, filepos))
                    assert new_filepos > self._filepos[-1] # monotonic!
                    self._filepos.append(new_filepos)
                    assert new_filepos == self._filepos[block_index], (
            # If we reached the end of the file, then update self._len
            if new_filepos == self._eofpos:
            # If we're at the end of the file, then we're done.
            assert new_filepos <= self._eofpos
            if new_filepos == self._eofpos:
            filepos = new_filepos
    A 'view' of a corpus file that joins together one or more
    one file handle is left open at any time.
    A stream backed corpus view for corpus files that consist of
        >>> PickleCorpusView.write(feature_corpus, some_fileid)  # doctest: +SKIP
        >>> pcv = PickleCorpusView(some_fileid) # doctest: +SKIP
    def __init__(self, fileid, delete_on_gc=False):
        ``fileid``.
        :param delete_on_gc: If true, then ``fileid`` will be deleted
        StreamBackedCorpusView.__init__(self, fileid)
        fileid.  (This method is called whenever a
            if os.path.exists(self._fileid):
                try: os.remove(self._fileid)
    def write(cls, sequence, output_file):
        if isinstance(output_file, compat.string_types):
            output_file = open(output_file, 'wb')
            pickle.dump(item, output_file, cls.PROTOCOL)
    def cache_to_tempfile(cls, sequence, delete_on_gc=True):
        Write the given sequence to a temporary file as a pickle
        temporary corpus file.
        :param delete_on_gc: If true, then the temporary file will be
            fd, output_file_name = tempfile.mkstemp('.pcv', 'nltk-')
            output_file = os.fdopen(fd, 'wb')
            cls.write(sequence, output_file)
            output_file.close()
            return PickleCorpusView(output_file_name, delete_on_gc)
            raise ValueError('Error while creating temp file: %s' % e)
        # End of file:
        # End of file:
        if not line: return [] # end of file.
        # End of file:
    stream's file position at the end the last complete s-expression
    unless there are no more s-expressions in the file.
    If the file ends in in the middle of an s-expression, then that
    incomplete s-expression is returned when the end of the file is
                    # The file ended mid-sexpr -- return what we got.
def find_corpus_fileids(root, regexp):
        raise TypeError('find_corpus_fileids: expected a PathPointer')
    # Find fileids in a zipfile: scan the zipfile's namelist.  Filter
    if isinstance(root, ZipFilePathPointer):
        fileids = [name[len(root.entry):] for name in root.zipfile.namelist()
        items = [name for name in fileids if re.match(regexp, name)]
    # Find fileids in a directory: use os.walk to search all (proper
    elif isinstance(root, FileSystemPathPointer):
        for dirname, subdirs, fileids in os.walk(root.path, **kwargs):
            items += [prefix+fileid for fileid in fileids
                      if re.match(regexp, prefix+fileid)]
#{ Paragraph structure in Treebank files
        # End of file:
Corpus reader for corpora whose documents are xml files.
    Corpus reader for corpora whose documents are xml files.
    the XML files themselves.  See the XML specs for more info.
    def __init__(self, root, fileids, wrap_etree=False):
        CorpusReader.__init__(self, root, fileids)
    def xml(self, fileid=None):
        # Make sure we have exactly one file -- no concatenating XML.
        if fileid is None and len(self._fileids) == 1:
            fileid = self._fileids[0]
        if not isinstance(fileid, compat.string_types):
            raise TypeError('Expected a single file identifier string')
        elt = ElementTree.parse(self.abspath(fileid).open()).getroot()
    def words(self, fileid=None):
        Returns all of the words and punctuation symbols in the specified file
        fileid can only specify one file.
        :return: the given file's text nodes as a list of words and punctuation symbols
        elt = self.xml(fileid)
        encoding = self.encoding(fileid)
    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    file, and provides a flat list-like interface for accessing them.
    def __init__(self, fileid, tagspec, elt_handler=None):
        Create a new corpus view based on a specified XML file.
        specified by the XML files themselves.
        """A dictionary mapping from file positions (as returned by
        encoding = self._detect_encoding(fileid)
        StreamBackedCorpusView.__init__(self, fileid, encoding=encoding)
    def _detect_encoding(self, fileid):
        if isinstance(fileid, PathPointer):
            s = fileid.open().readline()
            with open(fileid, 'rb') as fp:
    #: emtpy-elt tags in an XML file.  This regexp is more lenient than
            # End of file?
                raise ValueError('Unexpected end of file: tag not closed')
            # End of file.
                else: raise ValueError('Unexpected end of file')
from nltk.corpus.reader.util import find_corpus_fileids
        # The following files are not fully decodable because they
        # the following files are not supported by Python:
        # The following files are encoded for specific fonts:
        # The following files are unintended:
        fileids = find_corpus_fileids(root, r'(?!README|\.).*')
            [fileid for fileid in fileids if fileid not in self.SKIP],
    def decorator(self, fileids=None, **kwargs):
        if not fileids:
            fileids = self.fileids()
        return fun(self, fileids, **kwargs)
    ``categories()``. You can use also this metadata to filter files, e.g.:
    ``fileids(channel='prasa')``, ``fileids(categories='publicystyczny')``.
    def __init__(self, root, fileids):
        CorpusReader.__init__(self, root, fileids, None, None)
    def raw(self, fileids=None):
        if not fileids:
            fileids = self.fileids()
        return ''.join([open(fileid, 'r').read()
            for fileid in self._list_morph_files(fileids)])
    def channels(self, fileids=None):
        if not fileids:
            fileids = self.fileids()
        return self._parse_header(fileids, 'channel')
    def domains(self, fileids=None):
        if not fileids:
            fileids = self.fileids()
        return self._parse_header(fileids, 'domain')
    def categories(self, fileids=None):
        if not fileids:
            fileids = self.fileids()
                for cat in self._parse_header(fileids, 'keyTerm')]
    def fileids(self, channels=None, domains=None, categories=None):
            return CorpusReader.fileids(self)
            return self._list_morph_files_by('channel', channels)
            return self._list_morph_files_by('domain', domains)
            return self._list_morph_files_by('keyTerm', categories,
    def sents(self, fileids=None, **kwargs):
        return concat([self._view(fileid,
            for fileid in self._list_morph_files(fileids)])
    def paras(self, fileids=None, **kwargs):
        return concat([self._view(fileid,
            for fileid in self._list_morph_files(fileids)])
    def words(self, fileids=None, **kwargs):
        return concat([self._view(fileid, tags=False, **kwargs)
            for fileid in self._list_morph_files(fileids)])
    def tagged_sents(self, fileids=None, **kwargs):
        return concat([self._view(fileid, mode=IPIPANCorpusView.SENTS_MODE,
            for fileid in self._list_morph_files(fileids)])
    def tagged_paras(self, fileids=None, **kwargs):
        return concat([self._view(fileid, mode=IPIPANCorpusView.PARAS_MODE,
            for fileid in self._list_morph_files(fileids)])
    def tagged_words(self, fileids=None, **kwargs):
        return concat([self._view(fileid, **kwargs)
            for fileid in self._list_morph_files(fileids)])
    def _list_morph_files(self, fileids):
        return [f for f in self.abspaths(fileids)]
    def _list_header_files(self, fileids):
                for f in self._list_morph_files(fileids)]
    def _parse_header(self, fileids, tag):
        for f in self._list_header_files(fileids):
    def _list_morph_files_by(self, tag, values, map=None):
        fileids = self.fileids()
        ret_fileids = set()
        for f in fileids:
                    ret_fileids.add(f)
        return list(ret_fileids)
    def _view(self, filename, **kwargs):
        return IPIPANCorpusView(filename,
    def __init__(self, filename, startpos=0, **kwargs):
        StreamBackedCorpusView.__init__(self, filename, None, startpos, None)
import tempfile
    file layout and use of file formats).  The corpus root directory
    should contain the following files:
    for each speaker, containing three files for each utterance:
      - <utterance-id>.wav: utterance sound file
    _FILE_RE = (r'(\w+-\w+/\w+\.(phn|txt|wav|wrd))|' +
    """A regexp matching fileids that are used by this corpus reader."""
        # Ensure that wave files don't get treated as unicode data:
                              find_corpus_fileids(root, self._FILE_RE),
                            find_corpus_fileids(root, self._UTTERANCE_RE)]
    def fileids(self, filetype=None):
        Return a list of file identifiers for the files that make up
        :param filetype: If specified, then ``filetype`` indicates that
            only the files that have the given type should be
        if filetype is None:
            return CorpusReader.fileids(self)
        elif filetype in ('txt', 'wrd', 'phn', 'wav'):
            return ['%s.%s' % (u, filetype) for u in self._utterances]
        elif filetype == 'metadata':
            raise ValueError('Bad value for filetype: %r' % filetype)
                for fileid in self._utterance_fileids(utterances, '.phn')
                for line in self.open(fileid) if line.strip()]
                for fileid in self._utterance_fileids(utterances, '.phn')
                for line in self.open(fileid) if line.strip()]
                for fileid in self._utterance_fileids(utterances, '.wrd')
                for line in self.open(fileid) if line.strip()]
                for fileid in self._utterance_fileids(utterances, '.wrd')
                for line in self.open(fileid) if line.strip()]
                 for line in self.open(fileid) if line.strip()]
                for fileid in self._utterance_fileids(utterances, '.wrd')]
                for fileid in self._utterance_fileids(utterances, '.txt')
                for line in self.open(fileid) if line.strip()]
    # fileids are WAV fileids (aka RIFF), but they're actually NIST SPHERE
    # fileids.
        # Open a new temporary file -- the wave module requires
        # an actual file, and won't work w/ stringio. :(
        tf = tempfile.TemporaryFile()
        # Write the parameters & data to the new file.
        # Read the data back from the file, and return it.  The
        # file will automatically be deleted when we return.
    def _utterance_fileids(self, utterances, extension):
                                     "activate your audio device."), file=sys.stderr)
                print("system error message:", str(e), file=sys.stderr)
                             "for audio playback."), file=sys.stderr)
    _FILES = ['tagged']
    # Use the "tagged" file even for non-tagged data methods, since
        CorpusReader.__init__(self, root, self._FILES)
    def __init__(self, root, fileids, encoding='utf8', sent_splitter=None):
        CorpusReader.__init__(self, root, fileids, encoding)
    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def words(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
            for (fileid, enc) in self.abspaths(fileids, True)])
    def tagged_words(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
            for (fileid, enc) in self.abspaths(fileids, True)])
    def sents(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
            for (fileid, enc) in self.abspaths(fileids, True)])
    def tagged_sents(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
            for (fileid, enc) in self.abspaths(fileids, True)])
    def paras(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
            for (fileid, enc) in self.abspaths(fileids, True)])
    def tagged_paras(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
            for (fileid, enc) in self.abspaths(fileids, True)])
    def __init__(self, corpus_file, encoding,
        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)
from nltk.util import binary_search_file as _binary_search_file
    - offset: The offset in the WordNet dict file of this synset.
    - #lexname: The name of the lexicographer file containing this synset.
    #{ Filename constants
    _FILEMAP = {ADJ: 'adj', ADV: 'adv', NOUN: 'noun', VERB: 'verb'}
    #: A list of file identifiers for all the fileids used by this
    _FILES = ('cntlist.rev', 'lexnames', 'index.sense',
        super(WordNetCorpusReader, self).__init__(root, self._FILES,
        """A index that provides the file offset
        self._data_file_map = {}
        self._key_count_file = None
        self._key_synset_file = None
        # load the exception file data into memory
        for suffix in self._FILEMAP.values():
            # parse each line of the file (ignoring comment lines)
                # raise more informative error with file name and line number
                    raise WordNetError('file %s, line %i: %s' % tup)
        # load the exception file data into memory
        for pos, suffix in self._FILEMAP.items():
        fh = self._data_file(ADJ)
        # open the key -> synset file if necessary
        if self._key_synset_file is None:
            self._key_synset_file = self.open('index.sense')
        synset_line = _binary_search_file(self._key_synset_file, key)
        # load synset information from the appropriate file
    def _data_file(self, pos):
        Return an open file pointer for the data file for the given
        if self._data_file_map.get(pos) is None:
            fileid = 'data.%s' % self._FILEMAP[pos]
            self._data_file_map[pos] = self.open(fileid)
        return self._data_file_map[pos]
        data_file = self._data_file(pos)
        data_file.seek(offset)
        data_file_line = data_file.readline()
        synset = self._synset_from_pos_and_line(pos, data_file_line)
    def _synset_from_pos_and_line(self, pos, data_file_line):
            columns_str, gloss = data_file_line.split('|')
            # determine the lexicographer file name
            raise WordNetError('line %r: %s' % (data_file_line, e))
            pos_tags = self._FILEMAP.keys()
            # Open the file for reading.  Note that we can not re-use
            # the file poitners from self._data_file_map here, because
            # we're defining an iterator, and those file pointers might
            fileid = 'data.%s' % self._FILEMAP[pos_tag]
            data_file = self.open(fileid)
                # generate synsets for each line in the POS file
                offset = data_file.tell()
                line = data_file.readline()
                        # adjective satellites are in the same file as
                    offset = data_file.tell()
                    line = data_file.readline()
            # close the extra file handle we opened
                data_file.close()
                data_file.close()
        # open the count file if we haven't already
        if self._key_count_file is None:
            self._key_count_file = self.open('cntlist.rev')
        # find the key in the counts file and return the count
        line = _binary_search_file(self._key_count_file, lemma.key)
    def __init__(self, root, fileids):
        CorpusReader.__init__(self, root, fileids, encoding='utf8')
    def ic(self, icfile):
        Load an information content file from the wordnet_ic corpus
        :type icfile: str
        :param icfile: The name of the wordnet_ic file (e.g. "ic-brown.dat")
        for num, line in enumerate(self.open(icfile)):
        msg = 'Information content file has no entries for part-of-speech: %s'
        msg = "Unidentified part of speech in WordNet Information Content file for field %s" % field
File Format: Each line consists of an uppercased word, a counter
        return concat([StreamBackedCorpusView(fileid, read_cmudict_block,
                       for fileid, enc in self.abspaths(None, True)])
        fileids = self._fileids
        if isinstance(fileids, compat.string_types):
            fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
        if line == '': return entries # end of file.
Read CoNLL-style chunk fileids.
    A corpus reader for CoNLL-style files.  These files consist of a
    annotation type.  The set of columns used by CoNLL-style files can
        parallel files contain different columns.
    def __init__(self, root, fileids, columntypes,
        CorpusReader.__init__(self, root, fileids, encoding)
    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def words(self, fileids=None):
        return LazyConcatenation(LazyMap(self._get_words, self._grids(fileids)))
    def sents(self, fileids=None):
        return LazyMap(self._get_words, self._grids(fileids))
    def tagged_words(self, fileids=None, tagset=None):
                                         self._grids(fileids)))
    def tagged_sents(self, fileids=None, tagset=None):
        return LazyMap(get_tagged_words, self._grids(fileids))
    def chunked_words(self, fileids=None, chunk_types=None,
                                         self._grids(fileids)))
    def chunked_sents(self, fileids=None, chunk_types=None,
        return LazyMap(get_chunked_words, self._grids(fileids))
    def parsed_sents(self, fileids=None, pos_in_tree=None, tagset=None):
        return LazyMap(get_parsed_sent, self._grids(fileids))
    def srl_spans(self, fileids=None):
        return LazyMap(self._get_srl_spans, self._grids(fileids))
    def srl_instances(self, fileids=None, pos_in_tree=None, flatten=True):
        result = LazyMap(get_srl_instances, self._grids(fileids))
    def iob_words(self, fileids=None, tagset=None):
        :param fileids: the list of fileids that make up this corpus
        :type fileids: None or str or list
        return LazyConcatenation(LazyMap(get_iob_words, self._grids(fileids)))
    def iob_sents(self, fileids=None, tagset=None):
        :param fileids: the list of fileids that make up this corpus
        :type fileids: None or str or list
        return LazyMap(get_iob_words, self._grids(fileids))
    def _grids(self, fileids=None):
        # fileids), which would let us reuse the same corpus view for
        return concat([StreamBackedCorpusView(fileid, self._read_grid_block,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    A ConllCorpusReader whose data file contains three columns: words,
    def __init__(self, root, fileids, chunk_types, encoding='utf8',
            self, root, fileids, ('words', 'pos', 'chunk'),
    def words(self, fileids=None):
        return concat([IndianCorpusView(fileid, enc,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def tagged_words(self, fileids=None, tagset=None):
        return concat([IndianCorpusView(fileid, enc,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def sents(self, fileids=None):
        return concat([IndianCorpusView(fileid, enc,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def tagged_sents(self, fileids=None, tagset=None):
        return concat([IndianCorpusView(fileid, enc,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def __init__(self, corpus_file, encoding, tagged,
        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)
The files were taken from the RTE1, RTE2 and RTE3 datasets and the files
Filenames are of the form rte*_dev.xml and rte*_test.xml. The latter are the
gold standard annotated files.
file, taking values 1, 2 or 3. The GID is formatted 'm-n', where 'm' is the
    def pairs(self, fileids):
        :param fileids: a list of RTE corpus fileids
        if isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self._read_etree(self.xml(fileid)) for fileid in fileids])
    def __init__(self, root, fileids, delimiter=' ', encoding='utf8'):
        :param fileids: A list or regexp specifying the fileids in this corpus.
        CorpusReader.__init__(self, root, fileids, encoding)
    def tuples(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([StreamBackedCorpusView(fileid, self._read_tuple_block,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def raw(self, fileids=None):
        :return: the text contents of the given fileids, as a single string.
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    For access to the file text use the usual nltk functions,
    def __init__(self, root, fileids, lazy=True):
        XMLCorpusReader.__init__(self, root, fileids)
    def words(self, fileids=None, speaker='ALL', stem=False,
        :return: the given file(s) as a list of words
        return concat([self._get_words(fileid, speaker, sent, stem, relation,
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])
    def tagged_words(self, fileids=None, speaker='ALL', stem=False,
        :return: the given file(s) as a list of tagged
        return concat([self._get_words(fileid, speaker, sent, stem, relation,
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])
    def sents(self, fileids=None, speaker='ALL', stem=False,
        :return: the given file(s) as a list of sentences or utterances, each
        return concat([self._get_words(fileid, speaker, sent, stem, relation,
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])
    def tagged_sents(self, fileids=None, speaker='ALL', stem=False,
        :return: the given file(s) as a list of
        return concat([self._get_words(fileid, speaker, sent, stem, relation,
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])
    def corpus(self, fileids=None):
        :return: the given file(s) as a dict of ``(corpus_property_key, value)``
        return [self._get_corpus(fileid) for fileid in self.abspaths(fileids)]
    def _get_corpus(self, fileid):
        xmldoc = ElementTree.parse(fileid).getroot()
    def participants(self, fileids=None):
        :return: the given file(s) as a dict of
        return [self._get_participants(fileid)
                            for fileid in self.abspaths(fileids)]
    def _get_participants(self, fileid):
        xmldoc = ElementTree.parse(fileid).getroot()
    def age(self, fileids=None, speaker='CHI', month=False):
        :return: the given file(s) as string or int
        return [self._get_age(fileid, speaker, month)
                for fileid in self.abspaths(fileids)]
    def _get_age(self, fileid, speaker, month):
        xmldoc = ElementTree.parse(fileid).getroot()
            # some files don't have age data
    def MLU(self, fileids=None, speaker='CHI'):
        :return: the given file(s) as a floating number
        return [self._getMLU(fileid, speaker=speaker)
                for fileid in self.abspaths(fileids)]
    def _getMLU(self, fileid, speaker):
        sents = self._get_words(fileid, speaker=speaker, sent=True, stem=True,
    def _get_words(self, fileid, speaker, sent, stem, relation, pos,
        xmldoc = ElementTree.parse(fileid).getroot()
    The base URL for viewing files on the childes website. This
    def webview_file(self, fileid, urlbase=None):
        """Map a corpus file to its web version on the CHILDES website,
            childes.childes_url_base + urlbase + fileid.replace('.xml', '.cha')
        on the path consisting of <corpus root>+fileid; then if
        one is found, we use the unmodified fileid and hope for the best.
            path = urlbase+"/"+fileid
            full = self.root + "/" + fileid
                path = fileid
        for file in childes.fileids()[:5]:
            for (key,value) in childes.corpus(file)[0].items():
            print("words:", childes.words(file)[:7],"...")
            print("words with replaced words:", childes.words(file, replace=True)[:7]," ...")
            print("words with pos tags:", childes.tagged_words(file)[:7]," ...")
            print("words (only MOT):", childes.words(file, speaker='MOT')[:7], "...")
            print("words (only CHI):", childes.words(file, speaker='CHI')[:7], "...")
            print("stemmed words:", childes.words(file, stem=True)[:7]," ...")
            print("words with relations and pos-tag:", childes.words(file, relation=True)[:5]," ...")
            print("sentence:", childes.sents(file)[:2]," ...")
            for (participant, values) in childes.participants(file)[0].items():
            print("num of sent:", len(childes.sents(file)))
            print("num of morphemes:", len(childes.words(file, stem=True)))
            print("age:", childes.age(file))
            print("age in month:", childes.age(file, month=True))
            print("MLU:", childes.MLU(file))
        #corpus_root_http_bates = zipfile.ZipFile(cStringIO.StringIO(corpus_root_http.read()))
    # No unicode encoding param, since the data files are all XML.
    def __init__(self, root, fileids, wrap_etree=False):
        XMLCorpusReader.__init__(self, root, fileids, wrap_etree)
        self._class_to_fileid = {}
        corresponding file identifiers.  The keys of this dictionary
    def classids(self, lemma=None, wordnetid=None, fileid=None, classid=None):
        Return a list of the verbnet class identifiers.  If a file
        identifiers for classes (and subclasses) defined by that file.
        if len([x for x in [lemma, wordnetid, fileid, classid]
            raise ValueError('Specify at most one of: fileid, wordnetid, '
                             'fileid, classid')
        if fileid is not None:
            return [c for (c,f) in self._class_to_fileid.items()
                    if f == fileid]
            return sorted(self._class_to_fileid.keys())
    def vnclass(self, fileid_or_classid):
        :param fileid_or_classid: An identifier specifying which class
            should be returned.  Can be a file identifier (such as
        # File identifier: just return the xml.
        if fileid_or_classid in self._fileids:
            return self.xml(fileid_or_classid)
        classid = self.longid(fileid_or_classid)
        if classid in self._class_to_fileid:
            fileid = self._class_to_fileid[self.longid(classid)]
            tree = self.xml(fileid)
            raise ValueError('Unknown identifier %s' % fileid_or_classid)
    def fileids(self, vnclass_ids=None):
        Return a list of fileids that make up this corpus.  If
        ``vnclass_ids`` is specified, then return the fileids that make
            return self._fileids
            return [self._class_to_fileid[self.longid(vnclass_ids)]]
            return [self._class_to_fileid[self.longid(vnclass_id)]
        ``_wordnet_to_class``, and ``_class_to_fileid`` by scanning
        through the corpus fileids.  This is fast with cElementTree
        for fileid in self._fileids:
            self._index_helper(self.xml(fileid), fileid)
    def _index_helper(self, xmltree, fileid):
        self._class_to_fileid[vnclass] = fileid
            self._index_helper(subclass, fileid)
        ``_wordnet_to_class``, and ``_class_to_fileid`` by scanning
        through the corpus fileids.  This doesn't do proper xml parsing,
        for fileid in self._fileids:
            vnclass = fileid[:-4] # strip the '.xml'
            self._class_to_fileid[vnclass] = fileid
            for m in self._INDEX_RE.finditer(self.open(fileid).read()):
                    self._class_to_fileid[groups[2]] = fileid
The PP Attachment Corpus contains several files having the format:
The corpus contains the following files:
    def attachments(self, fileids):
        return concat([StreamBackedCorpusView(fileid, self._read_obj_block,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def tuples(self, fileids):
        return concat([StreamBackedCorpusView(fileid, self._read_tuple_block,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def __init__(self, root, fileids, lazy=True):
        XMLCorpusReader.__init__(self, root, fileids)
    def words(self, fileids=None):
        :return: the given file(s) as a list of words and punctuation symbols.
        return self._items(fileids, 'word', False, False, False)
    def chunks(self, fileids=None):
        :return: the given file(s) as a list of chunks,
        return self._items(fileids, 'chunk', False, False, False)
    def tagged_chunks(self, fileids=None, tag=('pos' or 'sem' or 'both')):
        :return: the given file(s) as a list of tagged chunks, represented
        return self._items(fileids, 'chunk', False, tag!='sem', tag!='pos')
    def sents(self, fileids=None):
        :return: the given file(s) as a list of sentences, each encoded
        return self._items(fileids, 'word', True, False, False)
    def chunk_sents(self, fileids=None):
        :return: the given file(s) as a list of sentences, each encoded
        return self._items(fileids, 'chunk', True, False, False)
    def tagged_sents(self, fileids=None, tag=('pos' or 'sem' or 'both')):
        :return: the given file(s) as a list of sentences. Each sentence
        return self._items(fileids, 'chunk', True, tag!='sem', tag!='pos')
    def _items(self, fileids, unit, bracket_sent, pos_tag, sem_tag):
        return concat([_(fileid, unit, bracket_sent, pos_tag, sem_tag)
                       for fileid in self.abspaths(fileids)])
    def _words(self, fileid, unit, bracket_sent, pos_tag, sem_tag):
        :param fileid: The name of the underlying file.
        xmldoc = ElementTree.parse(fileid).getroot()
    def __init__(self, fileid, unit, bracket_sent, pos_tag, sem_tag):
        :param fileid: The name of the underlying file.
        XMLCorpusView.__init__(self, fileid, tagspec)
    def __init__(self, root, fileids, encoding='utf8',
        CorpusReader.__init__(self, root, fileids, encoding)
    def raw(self, fileids=None):
        :return: the given file(s) as a single string.
        for fileid, encoding in self.abspaths(fileids, include_encoding=True):
            if isinstance(fileid, PathPointer):
                result.append(fileid.open(encoding=encoding).read())
                with codecs.open(fileid, "r", encoding) as fp:
    def words(self, fileids=None):
        return concat([DependencyCorpusView(fileid, False, False, False, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])
    def tagged_words(self, fileids=None):
        return concat([DependencyCorpusView(fileid, True, False, False, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])
    def sents(self, fileids=None):
        return concat([DependencyCorpusView(fileid, False, True, False, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])
    def tagged_sents(self, fileids=None):
            return concat([DependencyCorpusView(fileid, True, True, False, encoding=enc)
                           for fileid, enc in self.abspaths(fileids, include_encoding=True)])
    def parsed_sents(self, fileids=None):
        sents=concat([DependencyCorpusView(fileid, False, True, True, encoding=enc)
                      for fileid, enc in self.abspaths(fileids, include_encoding=True)])
    def __init__(self, corpus_file, tagged, group_by_sent, dependencies,
        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)
                raise ValueError('Unexpected number of fields in dependency tree file')
    files" which define the argument labels used by the annotations,
    on a per-verb basis.  Each "frameset file" contains one or more
    each "roleset", the frameset file provides descriptions of the
    def __init__(self, root, propfile, framefiles='',
                 verbsfile=None, parse_fileid_xform=None,
        :param propfile: The name of the file containing the predicate-
        :param framefiles: A list or regexp specifying the frameset
            fileids for this corpus.
        :param parse_fileid_xform: A transform that should be applied
            to the fileids in this corpus.  This should be a function
            of one argument (a fileid) that returns a string (the new
            fileid).
        # If framefiles is specified as a regexp, expand it.
        if isinstance(framefiles, compat.string_types):
            framefiles = find_corpus_fileids(root, framefiles)
        framefiles = list(framefiles)
        CorpusReader.__init__(self, root, [propfile, verbsfile] + framefiles,
        # Record our frame fileids & prop file.
        self._propfile = propfile
        self._framefiles = framefiles
        self._verbsfile = verbsfile
        self._parse_fileid_xform = parse_fileid_xform
    def raw(self, fileids=None):
        :return: the text contents of the given fileids, as a single string.
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
        return StreamBackedCorpusView(self.abspath(self._propfile),
                                      encoding=self.encoding(self._propfile))
        each line in the predicate-argument annotation file.
        return StreamBackedCorpusView(self.abspath(self._propfile),
                                      encoding=self.encoding(self._propfile))
        framefile = 'frames/%s.xml' % baseform
        if framefile not in self._framefiles:
            raise ValueError('Frameset file for %s not found' %
        # n.b.: The encoding for XML fileids is specified by the file
        etree = ElementTree.parse(self.abspath(framefile).open()).getroot()
                             (roleset_id, framefile))
            framefile = 'frames/%s.xml' % baseform
            if framefile not in self._framefiles:
                raise ValueError('Frameset file for %s not found' %
            framefiles = [framefile]
            framefiles = self._framefiles
        for framefile in framefiles:
            # n.b.: The encoding for XML fileids is specified by the file
            etree = ElementTree.parse(self.abspath(framefile).open()).getroot()
        in this corpus (from the verbs.txt file).
        return StreamBackedCorpusView(self.abspath(self._verbsfile),
                                      encoding=self.encoding(self._verbsfile))
                    line, self._parse_fileid_xform,
    def __init__(self, fileid, sentnum, wordnum, tagger, roleset,
        self.fileid = fileid
        """The name of the file containing the parse tree for this
        """The sentence number of this sentence within ``fileid``.
                (self.fileid, self.sentnum, self.wordnum))
        s = '%s %s %s %s %s %s' % (self.fileid, self.sentnum, self.wordnum,
        if self.fileid not in self.parse_corpus.fileids(): return None
        return self.parse_corpus.parsed_sents(self.fileid)[self.sentnum]
    def parse(s, parse_fileid_xform=None, parse_corpus=None):
        (fileid, sentnum, wordnum,
        # Apply the fileid selector, if any.
        if parse_fileid_xform is not None:
            fileid = parse_fileid_xform(fileid)
        return PropbankInstance(fileid, sentnum, wordnum, tagger,
    def __init__(self, root, fileids):
        XMLCorpusReader.__init__(self, root, fileids)
        # sub dir containing the xml files for frames
        # sub dir containing the xml files for lexical units
        # sub dir containing the xml files for fulltext annotation files
        #print('building relation index...', file=sys.stderr)
        #print('...done building relation index', file=sys.stderr)
        Return the contents of the corpus README.txt (or README) file.
            xmlfname = self._fulltext_idx[fn_docid].filename
            xmlfname = self._fulltext_idx[fn_docid].filename
        # construct the path name for the xml file containing the document info
        # construct the path name for the xml file containing the Frame info
        #print(locpath, file=sys.stderr)
        search through ALL of the frame XML files in the db.
        file on disk each time it is called. You may want to cache this
        in the *frame* definition file. That file does not contain 
        corpus annotations, so the LU files will be accessed on demand if those are 
    def _lu_file(self, lu, ignorekeys=[]):
        Augment the LU information that was loaded from the frame file 
        with additional information from the LU file.
        #print(locpath, file=sys.stderr)
        (Not done by default because it requires loading all frame files, 
                    #print(ex, file=sys.stderr)
            #print(i, nPropagations, file=sys.stderr)
            file name of each annotated document. The document's
            file name contains the name of the corpus that the
            file name "LUCorpus-v0.3__20000410_nyt-NEW.xml" is
                - 'filename'
            return PrettyList(x for x in ftlist if re.search(name, x['filename']) is not None)
        Extracts corpus/document info from the fulltextIndex.xml file.
        new attribute called "filename" that is the base file name of
        the xml file for the document in the "fulltext" subdir of the
                doc.filename = "{0}__{1}.xml".format(corpname, docname)
        """Load the info for a Frame from an frame xml file"""
                luentry['subCorpus'] = Future((lambda lu: lambda: self._lu_file(lu))(luentry))
        file. The main element (fullTextAnnotation) contains a 'header'
        """Load the lexical unit info from an xml element in a frame's xml file."""
        Load full info for a lexical unit from its xml file. 
        (which are not included in frame files).
    pprint([x.filename for x in firstcorp_docs])
    #       called, it has to search through ALL of the frame XML files
    def __init__(self, root, fileids,
        :param fileids: A list or regexp specifying the fileids in this corpus.
        CorpusReader.__init__(self, root, fileids, encoding)
    def raw(self, fileids=None):
        :return: the given file(s) as a single string.
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def words(self, fileids=None):
        :return: the given file(s) as a list of words
        return concat([TaggedCorpusView(fileid, enc,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def sents(self, fileids=None):
        :return: the given file(s) as a list of
        return concat([TaggedCorpusView(fileid, enc,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def paras(self, fileids=None):
        :return: the given file(s) as a list of
        return concat([TaggedCorpusView(fileid, enc,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def tagged_words(self, fileids=None, tagset=None):
        :return: the given file(s) as a list of tagged
        return concat([TaggedCorpusView(fileid, enc,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def tagged_sents(self, fileids=None, tagset=None):
        :return: the given file(s) as a list of
        return concat([TaggedCorpusView(fileid, enc,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def tagged_paras(self, fileids=None, tagset=None):
        :return: the given file(s) as a list of
        return concat([TaggedCorpusView(fileid, enc,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    divided into categories based on their file identifiers.
        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to
    def _resolve(self, fileids, categories):
        if fileids is not None and categories is not None:
            raise ValueError('Specify fileids or categories, not both')
            return self.fileids(categories)
            return fileids
    def raw(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def words(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def sents(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def paras(self, fileids=None, categories=None):
            self, self._resolve(fileids, categories))
    def tagged_words(self, fileids=None, categories=None, tagset=None):
            self, self._resolve(fileids, categories), tagset)
    def tagged_sents(self, fileids=None, categories=None, tagset=None):
            self, self._resolve(fileids, categories), tagset)
    def tagged_paras(self, fileids=None, categories=None, tagset=None):
            self, self._resolve(fileids, categories), tagset)
    def __init__(self, corpus_file, encoding, tagged, group_by_sent,
        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)
    def __init__(self, root, fileids, encoding='utf8', tagset=None):
            self, root, fileids, sep='_',
from nltk.data import PathPointer, FileSystemPathPointer, ZipFilePathPointer
    one or more files under a common root directory.  Each file is
    identified by its ``file identifier``, which is the relative path
    to the file from the root directory.
    selection arguments, such as ``fileids`` or ``categories``, which can
    def __init__(self, root, fileids, encoding='utf8', tagset=None):
        :param fileids: A list of the files that make up this corpus.
            strings; or implicitly, as a regular expression over file
            paths.  The absolute path for each file will be constructed
            by joining the reader's root to each file name.
        :param encoding: The default unicode encoding for the files
            - A string: ``encoding`` is the encoding name for all files.
            - A dictionary: ``encoding[file_id]`` is the encoding
              name for the file whose identifier is ``file_id``.  If
              ``file_id`` is not in ``encoding``, then the file
              tuples.  The encoding for a file whose identifier is ``file_id``
              ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``
              matches the ``file_id``, the file contents will be processed
            - None: the file contents of all files will be
            zipfile, zipentry = m.groups()
            if zipfile:
                root = ZipFilePathPointer(zipfile, zipentry)
                root = FileSystemPathPointer(root)
        # If `fileids` is a regexp, then expand it.
        if isinstance(fileids, compat.string_types):
            fileids = find_corpus_fileids(root, fileids)
        self._fileids = fileids
        """A list of the relative paths for the fileids that make up
            for fileid in self._fileids:
                    if re.match(regexp, fileid):
                        encoding_dict[fileid] = enc
        """The default unicode encoding for the fileids that make up
           this corpus.  If ``encoding`` is None, then the file
        if isinstance(self._root, ZipFilePathPointer):
            path = '%s/%s' % (self._root.zipfile.filename, self._root.entry)
        Return the contents of the corpus README file, if it exists.
    def fileids(self):
        Return a list of file identifiers for the fileids that make up
        return self._fileids
    def abspath(self, fileid):
        Return the absolute path for the given file.
        :type file: str
        :param file: The file identifier for the file whose path
        return self._root.join(fileid)
    def abspaths(self, fileids=None, include_encoding=False,
                 include_fileid=False):
        Return a list of the absolute paths for all fileids in this corpus;
        or for the given list of fileids, if specified.
        :type fileids: None or str or list
        :param fileids: Specifies the set of fileids for which paths should
            be returned.  Can be None, for all fileids; a list of
            file identifiers, for a specified set of fileids; or a single
            file identifier, for a single file.  Note that the return
            value is always a list of paths, even if ``fileids`` is a
            single file identifier.
        if fileids is None:
            fileids = self._fileids
        elif isinstance(fileids, compat.string_types):
            fileids = [fileids]
        paths = [self._root.join(f) for f in fileids]
        if include_encoding and include_fileid:
            return zip(paths, [self.encoding(f) for f in fileids], fileids)
        elif include_fileid:
            return zip(paths, fileids)
            return zip(paths, [self.encoding(f) for f in fileids])
    def open(self, file):
        Return an open stream that can be used to read the given file.
        If the file's encoding is not None, then the stream will
        automatically decode the file's contents into unicode.
        :param file: The file identifier of the file to read.
        encoding = self.encoding(file)
        stream = self._root.join(file).open(encoding)
    def encoding(self, file):
        Return the unicode encoding for the given corpus file, if known.
        If the encoding is unknown, or if the given file should be
            return self._encoding.get(file)
    corpus or for a specified set of fileids; and overrides ``fileids()``
    to take a ``categories`` argument, restricting the set of fileids to
        which can be used *instead* of the ``fileids`` parameter, to
        select which fileids should be included in the returned view.
            category for each file identifier.  The pattern will be
            applied to each file identifier, and the first matching
            group will be used as the category label for that file.
          - cat_map: A dictionary, mapping from file identifiers to
          - cat_file: The name of a file that contains the mapping
            from file identifiers to categories.  The argument
        self._f2c = None #: file-to-category mapping
        self._c2f = None #: category-to-file mapping
        self._file = None #: fileid of file containing the mapping
        self._delimiter = None #: delimiter for ``self._file``
        elif 'cat_file' in kwargs:
            self._file = kwargs['cat_file']
            del kwargs['cat_file']
                             'cat_map or cat_file.')
            'cat_file' in kwargs):
                             'cat_map, cat_file.')
            for file_id in self._fileids:
                category = re.match(self._pattern, file_id).group(1)
                self._add(file_id, category)
            for (file_id, categories) in self._map.items():
                    self._add(file_id, category)
        elif self._file is not None:
            for line in self.open(self._file).readlines():
                file_id, categories = line.split(self._delimiter, 1)
                if file_id not in self.fileids():
                    raise ValueError('In category mapping file %s: %s '
                                     'not found' % (self._file, file_id))
                    self._add(file_id, category)
    def _add(self, file_id, category):
        self._f2c[file_id].add(category)
        self._c2f[category].add(file_id)
    def categories(self, fileids=None):
        or for the file(s) if it is given.
        if fileids is None:
        if isinstance(fileids, compat.string_types):
            fileids = [fileids]
        return sorted(set.union(*[self._f2c[d] for d in fileids]))
    def fileids(self, categories=None):
        Return a list of file identifiers for the files that make up
            return super(CategorizedCorpusReader, self).fileids()
        and a method for detecting the sentence blocks in corpus files.
    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def parsed_sents(self, fileids=None):
        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, True)])
    def tagged_sents(self, fileids=None, tagset=None):
        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, True)])
    def sents(self, fileids=None):
        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, True)])
    def tagged_words(self, fileids=None, tagset=None):
        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, True)])
    def words(self, fileids=None):
        return concat([StreamBackedCorpusView(fileid,
                       for fileid, enc in self.abspaths(fileids, True)])
    def __init__(self, root, fileids,
        :param fileids: A list or regexp specifying the fileids in this corpus.
        CorpusReader.__init__(self, root, fileids, encoding)
    def raw(self, fileids=None):
        :return: the given file(s) as a single string.
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
    def words(self, fileids=None):
        :return: the given file(s) as a list of words
        return concat([AlignedSentCorpusView(fileid, enc, False, False,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def sents(self, fileids=None):
        :return: the given file(s) as a list of
        return concat([AlignedSentCorpusView(fileid, enc, False, True,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def aligned_sents(self, fileids=None):
        :return: the given file(s) as a list of AlignedSent objects.
        return concat([AlignedSentCorpusView(fileid, enc, True, True,
                       for (fileid, enc) in self.abspaths(fileids, True)])
    def __init__(self, corpus_file, encoding, aligned, group_by_sent,
        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)
TRY_ZIPFILE_FIRST = False
        if TRY_ZIPFILE_FIRST:
            source = [source.words(f) for f in source.fileids()]
Toolbox databases and settings files.
from nltk.data import PathPointer, ZipFilePathPointer, find
    Class for reading and processing standard format marker files and strings.
    def __init__(self, filename=None, encoding=None):
        if filename is not None:
            self.open(filename)
    def open(self, sfm_file):
        Open a standard format marker file for sequential reading.
        :param sfm_file: name of the standard format marker input file
        :type sfm_file: str
        if isinstance(sfm_file, PathPointer):
            self._file = sfm_file.open(self._encoding)
            self._file = codecs.open(sfm_file, 'rU', self._encoding)
        :param s: string to parse as a standard format marker input file
        self._file = StringIO(s)
        file_iter = iter(self._file)
        line = next(file_iter)
        for line in file_iter:
            Ignored if encoding is None. If the whole file is UTF-8 encoded set
        """Close a previously opened standard format marker file or string."""
        self._file.close()
        Returns an element tree structure corresponding to a toolbox data file with
        Returns an element tree structure corresponding to a toolbox data file
    """This class is the base class for settings files."""
        Return the contents of toolbox settings file with a nested structure.
        :param encoding: encoding used by settings file
    # write XML to file
    # write XML to file
#    lexicon = ToolboxData(ZipFilePathPointer(zip_path, 'toolbox/rotokas.dic')).parse()
    file_path = find('corpora/toolbox/rotokas.dic')
    lexicon = ToolboxData(file_path).parse()
    file_path = find('corpora/toolbox/MDF/MDF_AltH.typ')
    settings.open(file_path)
#    settings.open(ZipFilePathPointer(zip_path, entry='toolbox/MDF/MDF_AltH.typ'))
    """Finds bigram collocations in the files of the WebText corpus."""
    for file in webtext.fileids():
                 for word in webtext.words(file)]
        print(file)
import tempfile
        Transform the output file into an NLTK-style Valuation.
        Transform the output file into any Mace4 ``interpformat`` format.
    and generating prover9-style input files from them.
    def __init__(self, gramfile=None):
        :param gramfile: name of file where grammar can be loaded
        :type gramfile: str
        self._gramfile = (gramfile if gramfile else 'grammars/book_grammars/discourse.fcfg')
        self._parser = load_parser(self._gramfile)
    def __init__(self, semtype_file=None, remove_duplicates=False,
        :param semtype_file: name of file where grammar can be loaded
        if semtype_file is None:
            semtype_file = 'drt_glue.semtype'
        self._glue = DrtGlue(semtype_file=semtype_file,
        show_cfg(self._reading_command._gramfile)
    Convert a  file of first order formulas into a list of ``Expression`` objects.
    :param s: the contents of the file
import tempfile
from nltk.data import ZipFilePathPointer
            will not be required, and MaltParser will use the model file in
        self.working_dir = tempfile.gettempdir() if working_dir is None\
        input_file = tempfile.NamedTemporaryFile(prefix='malt_input.conll',
        output_file = tempfile.NamedTemporaryFile(prefix='malt_output.conll',
                    input_file.write('%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n' %
                input_file.write('\n\n')
            input_file.close()
                   '-c', self.mco, '-i', input_file.name,
                   '-o', output_file.name, '-m', 'parse']
            return DependencyGraph.load(output_file.name)
            input_file.close()
            os.remove(input_file.name)
            output_file.close()
            os.remove(output_file.name)
        input_file = tempfile.NamedTemporaryFile(prefix='malt_train.conll',
            input_file.write('\n'.join(dg.to_conll(10) for dg in depgraphs))
            input_file.close()
            self.train_from_file(input_file.name, verbose=verbose)
            input_file.close()
            os.remove(input_file.name)
    def train_from_file(self, conll_file, verbose=False):
        Train MaltParser from a file
        :param conll_file: str for the filename of the training input data
        # If conll_file is a ZipFilePathPointer, then we need to do some extra
        if isinstance(conll_file, ZipFilePathPointer):
            input_file = tempfile.NamedTemporaryFile(prefix='malt_train.conll',
                conll_str = conll_file.open().read()
                conll_file.close()
                input_file.write(conll_str)
                input_file.close()
                return self.train_from_file(input_file.name, verbose=verbose)
                input_file.close()
                os.remove(input_file.name)
               '-c', self.mco, '-i', conll_file, '-m', 'learn']
    def load(file):
        :param file: a file in Malt-TAB format
        with open(file) as f:
    conll_file_demo()
def conll_file_demo():
def run_profile():
    import profile
    profile.run('for i in range(1): demo()', '/tmp/profile.out')
    p = pstats.Stats('/tmp/profile.out')
    Load a grammar from a file, and build a parser based on that grammar.
        The default protocol is ``"nltk:"``, which searches for the file
contacts the NLTK download server, to retrieve an index file
describing the available packages.  By default, this index file is
specifying a different URL for the package index file.
Handling data files..  Some questions:
* Should the data files be kept zipped or unzipped?  I say zipped.
* Should the data files be kept in svn at all?  Advantages: history;
* Compromise: keep the data files in trunk/data rather than in
  of packages.  The packages should be kept as zip files, because we
  more, but they tend to be binary-ish files anyway, where diffs
  files like abc.zip and treebank.zip and propbank.zip.  For each
  etc).  Collections would also have .xml files.  Finally, we would
  pull all these together to form a single index.xml file.  Some
      index.xml ........................ main index file
        corpora/ ....................... zip & xml files for corpora
        grammars/ ...................... zip & xml files for grammars
        taggers/ ....................... zip & xml files for taggers
        tokenizers/ .................... zip & xml files for tokenizers
      collections/ ..................... xml files for collections
  Where the root (/trunk/data) would contain a makefile; and src/
  would contain a script to update the info.xml file.  It could also
  contain scripts to rebuild some of the various model files.  The
  file expands entirely into a single subdir, whose name matches the
  - in index: change "size" to "filesize" or "compressed-size"
    (we shouldn't need to re-download the file if the zip file is ok
xml file should have:
import time, os, zipfile, sys, textwrap, threading, itertools
# Directory entry objects (from the data server's index file)
    extracted from the XML index file that is downloaded by
    ``Downloader``.  Each package consists of a single file; but if
    that file is a zip file, then it can be automatically decompressed
        """A URL that can be used to download this package's file."""
        """The filesize (in bytes) of the package file."""
        """The total filesize of the files contained in the package's
           zipfile."""
        """The MD-5 checksum of the package file."""
        self.filename = os.path.join(subdir, id+ext)
        """The filename that should be used for this package's file.  It
        # Include any other attributes provided by the XML file.
    These entries are extracted from the XML index file that is
        # Include any other attributes provided by the XML file.
    """The package download file is already up-to-date"""
    """The package download file is out-of-date or corrupt"""
        """The URL for the data server's index file."""
        """The XML index file downloaded from the data server"""
        filepath = os.path.join(download_dir, info.filename)
        if os.path.exists(filepath):
            os.remove(filepath)
        # Download the file.  This will raise an IOError if the url
            infile = compat.urlopen(info.url)
            with open(filepath, 'wb') as outfile:
                    s = infile.read(1024*16) # 16k blocks.
                    outfile.write(s)
            infile.close()
        # If it's a zipfile, uncompress it.
        if info.filename.endswith('.zip'):
                for msg in _unzip_iter(filepath, zipdir, verbose=False):
                        show('Unzipping %s.' % msg.package.filename, '  ')
            filepath = os.path.join(download_dir, info.filename)
                status = self._pkg_status(info, filepath)
                                                                   filepath)
    def _pkg_status(self, info, filepath):
        if not os.path.exists(filepath):
        # Check if the file has the correct size.
        try: filestat = os.stat(filepath)
        if filestat.st_size != int(info.size):
        # Check if the file's checksum matches
        if md5_hexdigest(filepath) != info.checksum:
        # If it's a zipfile, and it's been at least partially
        if filepath.endswith('.zip'):
            unzipdir = filepath[:-4]
                                for d, _, files in os.walk(unzipdir)
                                for f in files)
        # Download the index file.
        """The URL for the data server's index file."""
        filemenu = Menu(menubar, tearoff=0)
        filemenu.add_command(label='Download', underline=0,
        filemenu.add_separator()
        filemenu.add_command(label='Change Server Index', underline=7,
        filemenu.add_command(label='Change Download Directory', underline=0,
        filemenu.add_separator()
        filemenu.add_command(label='Show Log', underline=5,
        filemenu.add_separator()
        filemenu.add_command(label='Exit', underline=1,
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
            show('Unzipping %s' % msg.package.filename)
    in a single zip file, known as a \"package file.\"  You can
                show('Unzipping %s' % msg.package.filename)
def md5_hexdigest(file):
    Calculate and return the MD5 checksum for a given file.
    ``file`` may either be a filename or an open stream.
    if isinstance(file, compat.string_types):
        with open(file, 'rb') as fp:
    return _md5_hexdigest(file)
def unzip(filename, root, verbose=True):
    Extract the contents of the zip file ``filename`` into the
    for message in _unzip_iter(filename, root, verbose):
def _unzip_iter(filename, root, verbose=True):
        sys.stdout.write('Unzipping %s' % os.path.split(filename)[1])
    try: zf = zipfile.ZipFile(filename)
    except zipfile.error as e:
        yield ErrorMessage(filename, 'Error with downloaded zip file')
        yield ErrorMessage(filename, e)
    # Get lists of directories & files
    filelist = [x for x in namelist if not x.endswith('/')]
    # Extract files.
    for i, filename in enumerate(filelist):
        filepath = os.path.join(root, *filename.split('/'))
        with open(filepath, 'wb') as out:
                contents = zf.read(filename)
                yield ErrorMessage(filename, e)
        if verbose and (i*10/len(filelist) > (i-1)*10/len(filelist)):
# This may move to a different file sometime.
import subprocess, zipfile
    Create a new data.xml index file, by combining the xml description
    files for various packages and collections.  ``root`` should be the
    path to a directory containing the package xml and zip files; and
    the collection xml files.  The ``root`` directory is expected to
          corpora/ ................. zip & xml files for corpora
          grammars/ ................ zip & xml files for grammars
          taggers/ ................. zip & xml files for taggers
          tokenizers/ .............. zip & xml files for tokenizers
        collections/ ............... xml files for collections
    For each package, there should be two files: ``package.zip``
    which contains the package itself as a compressed zip file; and
    zipfile ``package.zip`` should expand to a single subdirectory
    named ``package/``.  The base filename ``package`` must match
    the identifier given in the package's xml file.
    For each collection, there should be a single file ``collection.zip``
        zipstat = os.stat(zf.filename)
        url = '%s/%s/%s' % (base_url, subdir, os.path.split(zf.filename)[1])
        unzipped_size = sum(zf_info.file_size for zf_info in zf.infolist())
        pkg_xml.set('checksum', '%s' % md5_hexdigest(zf.filename))
        #pkg_xml.set('svn_revision', _svn_revision(zf.filename))
def _check_package(pkg_xml, zipfilename, zf):
    # The filename must patch the id given in the XML file.
    uid = os.path.splitext(os.path.split(zipfilename)[1])[0]
    # Zip file must expand to a subdir whose name matches uid.
        raise ValueError('Zipfile %s.zip does not expand to a single '
def _svn_revision(filename):
    number for a given file (by using ``subprocess`` to run ``svn``).
    p = subprocess.Popen(['svn', 'status', '-v', filename],
                         (os.path.split(filename)[1], textwrap.fill(stderr)))
    for dirname, subdirs, files in os.walk(root):
        for filename in files:
            if filename.endswith('.xml'):
                xmlfile = os.path.join(dirname, filename)
                yield ElementTree.parse(xmlfile).getroot()
      - ``zf`` is a ``zipfile.ZipFile`` for the package's contents.
    for dirname, subdirs, files in os.walk(root):
        for filename in files:
            if filename.endswith('.xml'):
                xmlfilename = os.path.join(dirname, filename)
                zipfilename = xmlfilename[:-4]+'.zip'
                try: zf = zipfile.ZipFile(zipfilename)
                    raise ValueError('Error reading file %r!\n%s' %
                                     (zipfilename, e))
                try: pkg_xml = ElementTree.parse(xmlfilename).getroot()
                    raise ValueError('Error reading file %r!\n%s' %
                                     (xmlfilename, e))
                # Check that the UID matches the filename
                uid = os.path.split(xmlfilename[:-4])[1]
                # Check that the zipfile expands to a subdir whose
                    raise ValueError('Zipfile %s.zip does not expand to a '
# READ FROM FILE OR STRING
def filestring(f):
        raise ValueError("Must be called with a filename or file-like object")
        Return the number of tokens in the corpus file underlying this
        file underlying this corpus view, starting at the token number
        Return the *i* th token in the corpus file underlying this
        file underlying this corpus view."""
# Binary Search in a File
def binary_search_file(file, key, cache={}, cacheDepth=-1):
    Return the line from the file with first word key.
    Searches through a sorted file using the binary search algorithm.
    :type file: file
    :param file: the file to be searched through.
    if hasattr(file, 'name'):
        end = os.stat(file.name).st_size - 1
        file.seek(0, 2)
        end = file.tell() - 1
        file.seek(0)
                file.seek(max(0, middle - 1))
                    file.readline()
                offset = file.tell()
                line = file.readline()
            # of the file, which is otherwise difficult to detect
        for root, dirs, files in os.walk(root):
            for f in files:
                    for sent in load_ace_file(os.path.join(root, f), fmt):
def load_ace_file(textfile, fmt):
    print('  - %s' % os.path.split(textfile)[1])
    annfile = textfile+'.tmx.rdc.xml'
    # Read the xml file, and get a list of entities
    xml = ET.parse(open(annfile)).getroot()
    # Read the text file, and mark the entities.
    with open(textfile) as fp:
    outfilename = '/tmp/ne_chunker_%s.pickle' % fmt
    print('Saving chunker to %s...' % outfilename)
    with open(outfilename, 'wb') as out:
        style file for the qtree package.
                "tkinter.filedialog": "tkFileDialog",
# for use in adding /PY3 to the second (filename) argument
# of the file pointers in data.py
Functions to find and load NLTK resource files, such as corpora,
grammars, and saved processing objects.  Resource files are identified
  - ``file:path``: Specifies the file whose path is *path*.
  - ``http://host/path``: Specifies the file stored on the web
  - ``nltk:path``: Specifies the file stored in the NLTK data
    package at *path*.  NLTK will search for these files in the
resource file, given its URL: ``load()`` loads a given resource, and
to a local file.
import zipfile
from gzip import GzipFile, READ as GZ_READ, WRITE as GZ_WRITE
def gzip_open_unicode(filename, mode="rb", compresslevel=9,
                      encoding='utf-8', fileobj=None, errors=None, newline=None):
    if fileobj is None:
        fileobj=GzipFile(filename, mode, compresslevel, fileobj)
    return io.TextIOWrapper(fileobj, encoding, errors, newline)
    >>> windows or split_resource_url('file:///home/nltk') == ('file', '/home/nltk')
    >>> not windows or split_resource_url('file:///C:/home/nltk') == ('file', 'C:/home/nltk')
    elif protocol == 'file':
    >>> normalize_resource_url('file:C:/dir/file')
    'file:///C:/dir/file'
    >>> normalize_resource_url('file:C:\\\\dir\\\\file')
    'file:///C:/dir/file'
    >>> normalize_resource_url('file:C:\\\\dir/file')
    'file:///C:/dir/file'
    >>> normalize_resource_url('file://C:/dir/file')
    'file:///C:/dir/file'
    >>> normalize_resource_url('file:////C:/dir/file')
    'file:///C:/dir/file'
    >>> not windows or normalize_resource_url('nltk:C:/dir/file') == 'file:///C:/dir/file'
    >>> not windows or normalize_resource_url('nltk:C:\\\\dir\\\\file') == 'file:///C:/dir/file'
    'file:///home/nltk'
    >>> normalize_resource_url('http://example.com/dir/file')
    'http://example.com/dir/file'
    >>> normalize_resource_url('dir/file')
    'nltk:dir/file'
    # use file protocol if the path is an absolute path
        protocol = 'file'
    if protocol == 'file':
        protocol = 'file:///'
    >>> windows or normalize_resource_name('dir/file', False) == '/dir/file'
    >>> not windows or normalize_resource_name('C:/file', False) == 'C:/file'
    >>> windows or normalize_resource_name('/dir/file', False) == '/dir/file'
    >>> windows or normalize_resource_name('../dir/file', False) == '/dir/file'
    >>> not windows or normalize_resource_name('/C:/file', False) == 'C:/file'
    >>> not windows or normalize_resource_name('../C:/file', False) == 'C:/file'
    ``FileSystemPathPointer`` identifies a file that can be accessed
    directly via a given absolute path.  ``ZipFilePathPointer``
    identifies a file contained within a zipfile, that can be accessed
    by reading that zipfile.
        the contents of the file identified by this path pointer.
            not contain a readable file.
    def file_size(self):
        Return the size of the file pointed to by this path pointer,
            not contain a readable file.
    def join(self, fileid):
        path given by ``fileid``.  The path components of ``fileid``
        the underlying file system's path seperator character.
class FileSystemPathPointer(PathPointer,compat.text_type):
    A path pointer that identifies a file which can be accessed
            raise IOError('No such file or directory: %r' % _path)
    def file_size(self):
    def join(self, fileid):
        _path = os.path.join(self._path, fileid)
        return FileSystemPathPointer(_path)
        return str('FileSystemPathPointer(%r)' % self._path)
class BufferedGzipFile(GzipFile):
    A ``GzipFile`` subclass that buffers calls to ``read()`` and ``write()``.
    files at the cost of using more memory.
    ``BufferedGzipFile`` is useful for loading large gzipped pickle objects
    as well as writing large encoded feature files for classifier training.
    def __init__(self, filename=None, mode=None, compresslevel=9,
                 fileobj=None, **kwargs):
        Return a buffered gzip file object.
        :param filename: a filesystem path
        :type filename: str
        :param mode: a file mode which can be any of 'r', 'rb', 'a', 'ab',
        :param fileobj: a BytesIO stream to read from instead of a file.
        :type fileobj: BytesIO
        :rtype: BufferedGzipFile
        GzipFile.__init__(self, filename, mode, compresslevel, fileobj)
        # Write the current buffer to the GzipFile.
        GzipFile.write(self, self._buffer.getvalue())
        # GzipFile.close() doesn't actuallly close anything.
        return GzipFile.close(self)
        GzipFile.flush(self, lib_mode)
                blocks = GzipFile.read(self, size)
            return GzipFile.read(self, size)
        :param data: bytes to write to file or buffer
        :param size: buffer at least size bytes before writing to file
class GzipFileSystemPathPointer(FileSystemPathPointer):
    A subclass of ``FileSystemPathPointer`` that identifies a gzip-compressed
    file located at a given absolute path.  ``GzipFileSystemPathPointer`` is
        stream = BufferedGzipFile(self._path, 'rb')
class ZipFilePathPointer(PathPointer):
    A path pointer that identifies a file contained within a zipfile,
    which can be accessed by reading that zipfile.
    def __init__(self, zipfile, entry=''):
        in the given zipfile.
        :raise IOError: If the given zipfile does not exist, or if it
        if isinstance(zipfile, compat.string_types):
            zipfile = OpenOnDemandZipFile(os.path.abspath(zipfile))
                zipfile.getinfo(entry)
                # the zip file.  So if `entry` is a directory name,
                # then check if the zipfile contains any files that
                    [n for n in zipfile.namelist() if n.startswith(entry)]):
                    pass # zipfile contains a file in that directory.
                    raise IOError('Zipfile %r does not contain %r' %
                                  (zipfile.filename, entry))
        self._zipfile = zipfile
    def zipfile(self):
        The zipfile.ZipFile object used to access the zip file
        return self._zipfile
        The name of the file within zipfile that this path
        data = self._zipfile.read(self._entry)
            stream = BufferedGzipFile(self._entry, fileobj=stream)
    def file_size(self):
        return self._zipfile.getinfo(self._entry).file_size
    def join(self, fileid):
        entry = '%s/%s' % (self._entry, fileid)
        return ZipFilePathPointer(self._zipfile, entry)
        return str('ZipFilePathPointer(%r, %r)') % (
            self._zipfile.filename, self._entry)
        return os.path.normpath(os.path.join(self._zipfile.filename, self._entry))
    zip files in paths, where a None or empty string specifies an absolute path.
    Zip File Handling:
        extension, then it is assumed to be a zipfile; and the
        remaining path components are used to look inside the zipfile.
        then it is assumed to be a zipfile.
      - If a given resource name that does not contain any zipfile
        ``corpora/chat80/cities.pl`` to a zip file path pointer to
        zipfile, the resource name must end with the forward slash
    # Check if the resource name includes a zipfile name
    zipfile, zipentry = m.groups()
        # Is the path item a zipfile?
        if _path and (os.path.isfile(_path) and _path.endswith('.zip')):
                return ZipFilePathPointer(_path, resource_name)
                # resource not in zipfile
            if zipfile is None:
                        return GzipFileSystemPathPointer(p)
                        return FileSystemPathPointer(p)
                p = os.path.join(_path, zipfile)
                        return ZipFilePathPointer(p, zipentry)
                        # resource not in zipfile
    # Fallback: if the path doesn't include a zip file, then try
    # zipfile of the same name.
    if zipfile is None:
def retrieve(resource_url, filename=None, verbose=True):
    Copy the given resource to a local file.  If no filename is
    specified, then use the URL's filename.  If there is already a
    file named ``filename``, then raise a ``ValueError``.
        for the file in the the NLTK data package.
    if filename is None:
        if resource_url.startswith('file:'):
            filename = os.path.split(resource_url)[-1]
            filename = re.sub(r'(^\w+:)?.*/', '', resource_url)
    if os.path.exists(filename):
        filename = os.path.abspath(filename)
        raise ValueError("File %r already exists!" % filename)
        print('Retrieving %r, saving to %r' % (resource_url, filename))
    infile = _open(resource_url)
    # Copy infile -> outfile, using 64k blocks.
    with open(filename, "wb") as outfile:
            s = infile.read(1024*64) # 64k blocks.
            outfile.write(s)
    infile.close()
    'raw': "The raw (byte string) contents of a file.",
    'text': "The raw (unicode string) contents of a file. "
#: A dictionary mapping from file extensions to format names, used
      - ``text`` (the file contents as a unicode string)
      - ``raw`` (the raw file contents as a byte string)
    format based on the resource name's file extension.  If that
        for the file in the the NLTK data package.
                             'on its file\nextension; use the "format" '
    Write out a grammar file, ignoring escaped and empty lines.
        for the file in the the NLTK data package.
    Helper function that returns an open file object for a resource,
    uses the 'file' protocol, then open the file with the given mode;
        for the file in the the NLTK data package.
    elif protocol.lower() == 'file':
# Open-On-Demand ZipFile
class OpenOnDemandZipFile(zipfile.ZipFile):
    A subclass of ``zipfile.ZipFile`` that closes its file pointer
    data from the zipfile.  This is useful for reducing the number of
    open file handles when many zip files are being accessed at once.
    ``OpenOnDemandZipFile`` must be constructed from a filename, not a
    file-like object (to allow re-opening).  ``OpenOnDemandZipFile`` is
    def __init__(self, filename):
        if not isinstance(filename, compat.string_types):
            raise TypeError('ReopenableZipFile filename must be a string')
        zipfile.ZipFile.__init__(self, filename)
        assert self.filename == filename
        self.fp = open(self.filename, 'rb')
        value = zipfile.ZipFile.read(self, name)
        """:raise NotImplementedError: OpenOnDemandZipfile is read-only"""
        raise NotImplementedError('OpenOnDemandZipfile is read-only')
        """:raise NotImplementedError: OpenOnDemandZipfile is read-only"""
        raise NotImplementedError('OpenOnDemandZipfile is read-only')
        return repr(str('OpenOnDemandZipFile(%r)') % self.filename)
    able to handle unicode-encoded files.
           file position in the underlying byte stream."""
        """The file position at which the most recent read on the
        Read this file's contents, decode them using this reader's
        Move the stream to a new file position.  If the reader is
        :param whence: If 0, then the offset is from the start of the file
            then the offset is from the end of the file (offset should
        Move the file position forward by ``offset`` characters,
        Return the current file position on the underlying byte
        returned file position will be the position of the beginning
        # If nothing's buffered, then just return our current filepos:
        # Otherwise, we'll need to backtrack the filepos until we
        # Store our original file position, so we can return here.
        orig_filepos = self.stream.tell()
        bytes_read = ( (orig_filepos-len(self.bytebuffer)) -
        filepos = self.stream.tell()
            self.stream.seek(filepos)
        # Return to our original filepos (so we don't have to throw
        self.stream.seek(orig_filepos)
        # Return the calculated filepos
        return filepos
                if not new_bytes: break # end of file.
__all__ = ['path', 'PathPointer', 'FileSystemPathPointer', 'BufferedGzipFile',
           'GzipFileSystemPathPointer', 'GzipFileSystemPathPointer',
           'show_cfg', 'clear_cache', 'LazyLoader', 'OpenOnDemandZipFile',
           'GzipFileSystemPathPointer', 'SeekableUnicodeStreamReader']
from nltk import __file__
        archives, and ZIP archives to search for class files.
        standard input, standard output and standard error file
        an existing file descriptor (a positive integer), an existing
        file object, and None.  ``subprocess.PIPE`` indicates that a
        redirection will occur; the child's file handles will be
        from the applications should be captured into the same file
#: The location of the NLTK jar file, which is used to communicate
NLTK_JAR = os.path.abspath(os.path.join(os.path.split(__file__)[0],
# Search for files/binaries
def find_file(filename, env_vars=(), searchpath=(),
        file_names=None, url=None, verbose=True):
    Search for a file to be used by nltk.
    :param filename: The name or path of the file.
    :param file_names: A list of alternative file names to check.
    :param verbose: Whether or not to print path when a file is found.
    file_names = [filename] + (file_names or [])
    assert isinstance(filename, compat.string_types)
    assert not isinstance(file_names, compat.string_types)
    # File exists, no magic
    for alternative in file_names:
        path_to_file = os.path.join(filename, alternative)
        if os.path.isfile(path_to_file):
            if verbose: print('[Found %s: %s]' % (filename, path_to_file))
            return path_to_file
        if os.path.isfile(alternative):
            if verbose: print('[Found %s: %s]' % (filename, alternative))
        # Check if the alternative is inside a 'file' directory
        path_to_file = os.path.join(filename, 'file', alternative)
        if os.path.isfile(path_to_file):
            if verbose: print('[Found %s: %s]' % (filename, path_to_file))
            return path_to_file
                if os.path.isfile(env_dir):
                    if verbose: print('[Found %s: %s]'%(filename, env_dir))
                for alternative in file_names:
                    path_to_file = os.path.join(env_dir, alternative)
                    if os.path.isfile(path_to_file):
                        if verbose: print('[Found %s: %s]'%(filename, path_to_file))
                        return path_to_file
                    # Check if the alternative is inside a 'file' directory
                    path_to_file = os.path.join(env_dir, 'file', alternative)
                    if os.path.isfile(path_to_file):
                        if verbose: print('[Found %s: %s]' % (filename, path_to_file))
                        return path_to_file
        for alternative in file_names:
            path_to_file = os.path.join(directory, alternative)
            if os.path.isfile(path_to_file):
                return path_to_file
    # to find the file.
        for alternative in file_names:
                    if verbose: print('[Found %s: %s]' % (filename, path))
    msg = ("NLTK was unable to find the %s file!" "\nUse software specific "
           "configuration paramaters" % filename)
                    (filename, url))
    Search for a file to be used by nltk.
    :param name: The name or path of the file.
    :param file_names: A list of alternative file names to check.
    :param verbose: Whether or not to print path when a file is found.
    return find_file(path_to_bin or name, env_vars, searchpath, binary_names,
# Find Java JAR files
    :param name: The name of the jar file
        if os.path.isfile(path_to_jar):
        raise ValueError('Could not find %s jar file at %s' %
                    if os.path.isfile(cp) and os.path.basename(cp) == name:
                if os.path.isfile(path_to_jar) and os.path.basename(path_to_jar) == name:
        if os.path.isfile(path_to_jar):
        filemenu = Menu(menubar, tearoff=0)
        filemenu.add_command(label='Reset Parser', underline=0,
        filemenu.add_command(label='Print to Postscript', underline=0,
        filemenu.add_command(label='Exit', underline=1,
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
        self._cframe.print_to_file()
                lambda: indian.words(files='hindi.pos'),
        filemenu = Menu(menubar, tearoff=0, borderwidth=0)
        filemenu.add_command(label='Exit', underline=1,
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
        filemenu = Menu(menubar, tearoff=0)
        filemenu.add_command(label='Reset Parser', underline=0,
        filemenu.add_command(label='Print to Postscript', underline=0,
        filemenu.add_command(label='Exit', underline=1,
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
        self._cframe.print_to_file()
from tkinter.filedialog import asksaveasfilename, askopenfilename
        self._cframe.print_to_file()
            self._cframe.print_to_file()
    :ivar _left_name: The name ``_left_chart`` (derived from filename)
    :ivar _right_name: The name ``_right_chart`` (derived from filename)
    :ivar _out_name: The name ``_out_chart`` (derived from filename)
    def __init__(self, *chart_filenames):
        for filename in chart_filenames:
            self.load_chart(filename)
        # File menu
        filemenu = tkinter.Menu(menubar, tearoff=0)
        filemenu.add_command(label='Load Chart', accelerator='Ctrl-o',
        filemenu.add_command(label='Save Output', accelerator='Ctrl-s',
        filemenu.add_separator()
        filemenu.add_command(label='Exit', underline=1,
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
    # File
    CHART_FILE_TYPES = [('Pickle file', '.pickle'),
                        ('All files', '*')]
        filename = asksaveasfilename(filetypes=self.CHART_FILE_TYPES,
        if not filename: return
        try: pickle.dump((self._out_chart), open(filename, 'w'))
                                   'Unable to open file: %r\n%s' %
                                   (filename, e))
        filename = askopenfilename(filetypes=self.CHART_FILE_TYPES,
        if not filename: return
        try: self.load_chart(filename)
                                   'Unable to open file: %r\n%s' %
                                   (filename, e))
    def load_chart(self, filename):
        chart = pickle.load(open(filename, 'r'))
        name = os.path.basename(filename)
        filemenu = tkinter.Menu(menubar, tearoff=0)
        filemenu.add_command(label='Save Chart', underline=0,
        filemenu.add_command(label='Load Chart', underline=0,
        filemenu.add_command(label='Reset Chart', underline=0,
        filemenu.add_separator()
        filemenu.add_command(label='Save Grammar',
        filemenu.add_command(label='Load Grammar',
        filemenu.add_separator()
        filemenu.add_command(label='Exit', underline=1,
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
    # File Menu
    CHART_FILE_TYPES = [('Pickle file', '.pickle'),
                        ('All files', '*')]
    GRAMMAR_FILE_TYPES = [('Plaintext grammar file', '.cfg'),
                          ('Pickle file', '.pickle'),
                          ('All files', '*')]
        "Load a chart from a pickle file"
        filename = askopenfilename(filetypes=self.CHART_FILE_TYPES,
        if not filename: return
            chart = pickle.load(open(filename, 'r'))
                                   'Unable to open file: %r' % filename)
        "Save a chart to a pickle file"
        filename = asksaveasfilename(filetypes=self.CHART_FILE_TYPES,
        if not filename: return
            pickle.dump(self._chart, open(filename, 'w'))
                                   'Unable to open file: %r' % filename)
        "Load a grammar from a pickle file"
        filename = askopenfilename(filetypes=self.GRAMMAR_FILE_TYPES,
        if not filename: return
            if filename.endswith('.pickle'):
                grammar = pickle.load(open(filename, 'r'))
                grammar = parse_cfg(open(filename, 'r').read())
                                   'Unable to open file: %r' % filename)
        filename = asksaveasfilename(filetypes=self.GRAMMAR_FILE_TYPES,
        if not filename: return
            if filename.endswith('.pickle'):
                pickle.dump((self._chart, self._tokens), open(filename, 'w'))
                file = open(filename, 'w')
                for prod in start: file.write('%s\n' % prod)
                for prod in rest: file.write('%s\n' % prod)
                file.close()
                                   'Unable to open file: %r' % filename)
    #import profile
    #profile.run('demo2()', '/tmp/profile.out')
    #p = pstats.Stats('/tmp/profile.out')
import tkinter.filedialog, tkinter.font
            display & for save files.  If either the name 'treebank'
        """The name of the development set (for save files)."""
        filemenu = Menu(menubar, tearoff=0)
        filemenu.add_command(label='Reset Application', underline=0,
        filemenu.add_command(label='Save Current Grammar', underline=0,
        filemenu.add_command(label='Load Grammar', underline=0,
        filemenu.add_command(label='Save Grammar History', underline=13,
        filemenu.add_command(label='Exit', underline=1,
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
    def save_grammar(self, filename=None):
        if not filename:
                      ('All files', '*')]
            filename = tkinter.filedialog.asksaveasfilename(filetypes=ftypes,
            if not filename: return
        out = open(filename, 'w')
    def load_grammar(self, filename=None):
        if not filename:
                      ('All files', '*')]
            filename = tkinter.filedialog.askopenfilename(filetypes=ftypes,
            if not filename: return
        grammar = open(filename).read()
    def save_history(self, filename=None):
        if not filename:
                      ('All files', '*')]
            filename = tkinter.filedialog.asksaveasfilename(filetypes=ftypes,
            if not filename: return
        out = open(filename, 'w')
                lambda: indian.tagged_sents(files='hindi.pos'),
                lambda: indian.tagged_sents(files='hindi.pos', tagset='simple'),
        filemenu = Menu(menubar, tearoff=0, borderwidth=0)
        filemenu.add_command(label='Exit', underline=1,
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
    -l <file> or --log-file <file>
        Logs messages to the given file, If this option is not specified
# now included in local file
# If set this is a file object for writting log messages.
logfile = None
        elif sp.endswith('.html'): # Trying to fetch a HTML file TODO:
                if os.path.isfile(usp):
                        '<p>The database info file:'\
                # Handle files here.
        self.wfile.write(page.encode('utf8'))
        global logfile
        if logfile:
            logfile.write(
def wnb(port=8000, runBrowser=True, logfilename=None):
    global server_mode, logfile
    if logfilename:
            logfile = open(logfilename, "a", 1) # 1 means 'line buffering'
                             logfilename, e)
        logfile = None
    if logfile:
        logfile.write(
                # Not all words of exc files are in the database, skip
                              ["logfile=", "port=", "server-mode", "help"])
    logfilename = None
        if (opt == "-l") or (opt == "--logfile"):
            logfilename = str(value)
        wnb(port, not server_mode, logfilename)
    >>> t = AnnotationTask(data=[x.split() for x in open(os.path.join(os.path.dirname(__file__), "artstein_poesio_example.txt"))])
log = logging.getLogger(__file__)
    parser.add_option("-f", "--file", dest="file",
                      help="file to read labelings from, each line with three columns: 'labeler item labels'")
                      help="char/string that separates the three columns in the file, defaults to tab")
    if not options.file:
    # read in data from the specified file
    for l in open(options.file):
def custom_distance(file):
    for l in open(file):
'''This file is adapted from the pattern library.
    MODULE = os.path.dirname(os.path.abspath(__file__))
    """ Returns an iterator over the lines in the file at the given path,
            # From file path.
        self._path       = path   # XML file path.
        """ Loads the XML-file (with sentiment annotations) from the given path.
"""File formats for training and testing data."""
    :param f: A filename.
        '''Detect the file format given a filename.
        Return True if a stream is this file format.
def detect(filename, max_read=1024):
    '''Attempt to detect a file's format, trying each of the supported
    with open(filename, 'r') as fp:
If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
        ``(text, classification)`` or a filename. ``text`` may be either
    :param format: If ``train_set`` is a filename, the file format, e.g.
        file format.
        if isinstance(train_set, basestring):  # train_set is a filename
        '''Reads a data file and returns and iterable that can be used
        # Attempt to detect file format if "format" isn't specified
            filename.
        :param format: If ``test_set`` is a filename, the file format, e.g.
            file format.
        if isinstance(test_set, basestring):  # test_set is a filename
        ``(text, classification)`` or a filename. ``text`` may be either
    :param format: If ``train_set`` is a filename, the file format, e.g.
        file format.
        ``(text, classification)`` or a filename. ``text`` may be either
    :param format: If ``train_set`` is a filename, the file format, e.g.
        file format.
    def __init__(self, csvfile, fieldnames, restval='', extrasaction='raise', dialect='excel', encoding='utf-8', errors='strict', *args, **kwds):
        csv.DictWriter.__init__(self, csvfile, fieldnames, restval, extrasaction, dialect, *args, **kwds)
        self.writer = UnicodeWriter(csvfile, dialect, encoding=encoding, errors=errors, *args, **kwds)
    def __init__(self, csvfile, fieldnames=None, restkey=None, restval=None,
        csv.DictReader.__init__(self, csvfile, fieldnames, restkey, restval, dialect, *args, **kwds)
        self.reader = UnicodeReader(csvfile, dialect, encoding=encoding,
            reader = UnicodeReader(csvfile, dialect, encoding=encoding, *args, **kwds)
HERE = os.path.dirname(os.path.abspath(__file__))
'''This file is based on pattern.en. See the bundled NOTICE file for
    MODULE = os.path.dirname(os.path.abspath(__file__))
        neg_ids = nltk.corpus.movie_reviews.fileids('neg')
        pos_ids = nltk.corpus.movie_reviews.fileids('pos')
            nltk.corpus.movie_reviews.words(fileids=[f])), 'neg') for f in neg_ids]
            nltk.corpus.movie_reviews.words(fileids=[f])), 'pos') for f in pos_ids]
    The pickle file can be obtained from the Github Releases page for TextBlob.
    for path, dirs, files in os.walk(os.path.abspath(directory)):
        for filename in fnmatch.filter(files, glob_pattern):
            filepath = os.path.join(path, filename)
            #backup orig file
                backup_path = filepath + '.bak'
                shutil.copyfile(filepath, backup_path)
            with open(filepath) as f:
            with open(filepath, "w") as f:
                print 'DBG: replacing in file', filepath
# The built-in function `open` opens a file and returns a file object.
# Read mode opens a file for reading only.
    f = open("file.txt", "r")
        # Read the entire contents of a file at once.
# Write mode creates a new file or overwrites the existing content of the file.
# Write mode will _always_ destroy the existing contents of a file.
    # This will create a new file or **overwrite an existing file**.
    f = open("file.txt", "w")
        f.write('blah') # Write a string to a file
        f.writelines(lines) # Write a sequence of strings to a file
# Append mode adds to the existing content, e.g. for keeping a log file. Append
# mode will _never_ harm the existing contents of a file.
    # This tries to open an existing file but creates a new file if necessary.
    logfile = open("log.txt", "a")
        logfile.write('log log log')
        logfile.close()
# This will walk the file system beginning in the directory the script is run from. It
for root, dirs, files in os.walk(os.getcwd()):
##  Prints a dir structure to a text file
    print 'File saved to: %s' % (outName)
# remove profiles
cd /usr/share/qtsixa/profiles
# copy profiles
sudo cp /usr/share/qtsixa/profiles/KDE /home/tazjel/Dropbox/wsixad_key
sudo cp /usr/share/qtsixa/profiles/KDE /home/tazjel/Dropbox/wsixad_key
KEY_FILE                144     /* AL Local Machine Browser */
KEY_SENDFILE            145
KEY_DELETEFILE          146
def regexp_answer(strQuestion, listQuestion, articleFile):
	article = articleFile.read()
    # get a list of the files the directory contains.  os.path.walk
def _paths_from_path_patterns(path_patterns, files=True, dirs="never",
    """_paths_from_path_patterns([<path-patterns>, ...]) -> file paths
    Generate a list of paths (files and/or dirs) represented by the given path
        "files" is boolean (default True) indicating if file paths
        "includes" is a list of file patterns to include in recursive
        "excludes" is a list of file and dir patterns to exclude.
            option which only excludes *files*.  I.e. you cannot exclude
                log.error("`%s': No such file or directory")
      {files=True, dirs='never', recursive=(if '-r' in opts)}
        script FILE     # yield FILE, else call on_error(FILE)
        script PATH*    # yield all files matching PATH*; if none,
        script -r DIR   # yield files (not dirs) recursively under DIR
        script -r PATH* # yield files matching PATH* and files recursively
    Use case #2: like `file -r` (if it had a recursive option)
      {files=True, dirs='if-not-recursive', recursive=(if '-r' in opts)}
        script FILE     # yield FILE, else call on_error(FILE)
        script PATH*    # yield all files and dirs matching PATH*; if none,
        script -r DIR   # yield files (not dirs) recursively under DIR
        script -r PATH* # yield files matching PATH* and files recursively
      {files=True, dirs='always', recursive=(if '-r' in opts)}
        script FILE     # yield FILE, else call on_error(FILE)
        script PATH*    # yield all files and dirs matching PATH*; if none,
        script -r DIR   # yield files and dirs recursively under DIR
        script -r PATH* # yield files and dirs matching PATH* and recursively
                    log.error("`%s': No such file or directory", path_pattern)
                    for dirpath, dirnames, filenames in _walk(path,
                        if files:
                            for filename in sorted(filenames):
                                f = join(dirpath, filename)
            elif files and _should_include_path(path, includes, excludes):
    print("Usage: %s [option] <textfile>" % os.path.basename(sys.argv[0]))
    File = open(sys.argv[1], "rb")
    FileContent = File.read()
    File.close()
    print("%s: Could not open file!" % sys.argv[1])
        for Match in RegExp.finditer(FileContent):
          Line = FileContent.count("\n", 0, Match.start(0)) + 1
if not os.path.isfile('appid.txt'):
  print "OOPS! You're missing Microsoft Academic Search APP ID key in file appid.txt!"
if not os.path.isfile(globaldb): pickle.dump([], open(globaldb, "wb"))
def process2( nameFileIn, nameFileOut):
 reader = open(nameFileIn, 'r' )
 fi = open( nameFileOut , "w")
def read_file(name):
  print 'printing contents of file ' + name
  #We use try...finally to ensure that the file is closed even if there
    f = file(name, 'r')
    '''Return a list of file names in directory 'dir_name'
    Additional arguments, if any, are file extensions to add to the list.
    Example usage: fileList = dir_list(r'H:\TEMP', False, 'txt', 'py', 'dat', 'log', 'jpg')
    fileList = []
    for file in os.listdir(dir_name):
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
                fileList.append(dirfile)
                if os.path.splitext(dirfile)[1][1:] in args:
                    fileList.append(dirfile)
        # recursively access file names in subdirectories
        elif os.path.isdir(dirfile) and subdir:
            # print "Accessing directory:", dirfile
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
def combine_files(fileList, fn):
    for file in fileList:
        print 'Writing file %s' % file
        f.write(open(file).read())
    fileList = dir_list(r'/home/ahmed/alltxt/', True, 'txt' )
    combine_files(fileList, fn)
config_handle = file(os.path.expanduser('~/.smsconf'),"r+")
#. Create a text file ~/.smsconf and write following lines in it
