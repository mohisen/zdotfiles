    ### fix paths in images
data_file = open(path, 'rb')
def get_iter(pattern, text):
    docsum_pattern = re.compile(pattern, re.DOTALL)
    docsum_iter = docsum_pattern.finditer(text)
first_author_initials_pattern = Word(alphas.upper(), min=1, max=3)
#    first_author_lastname_pattern = SkipTo(",")  # | "-"
first_author_lastname_pattern = Word(printables) #+ Suppress(",")
first_author_lastname_pattern.setParseAction(stripComma)
journal_pattern = SkipTo(',')
journal_pattern.setParseAction(stripEmbeddedTags, stripHelip, stripDash)
year_pattern = Word(nums, exact=4)
citation_pattern = Suppress('<span class=gs_a>') + first_author_initials_pattern("initials") + first_author_lastname_pattern("lastname") + Suppress(SkipTo('- ')) + Suppress('- ') + journal_pattern("journal") + Suppress(',') + year_pattern("year") + Suppress("-")
        new_items = get_dict_of_hits(citation_pattern, page_excerpt_text)
        file_prefix = os.path.basename(filename)
    the_dirname = os.path.dirname(filename)
    the_lowest_subdir = os.path.basename(the_dirname)
def get_list_of_hits(pattern, text):
    items = pattern.searchString(text)
def get_dict_of_hits(pattern, text):
    items = pattern.searchString(text)
    folder = os.path.join(request.folder,'sources')
                    if os.path.isdir(os.path.join(folder,f))]
    infofile = os.path.join(FOLDER,subfolder,'info.txt')
    if os.path.exists(infofile):
    filename = os.path.join(FOLDER,subfolder,'chapters.txt')
    filename = os.path.join(FOLDER,subfolder,'%.2i.markmin' % chapter_id)
    dest = os.path.join(request.folder, 'static_chaps', subfolder, '%.2i.html' % chapter_id)
    if (not os.path.isfile(dest)) or FORCE_RENDER:
        if not os.path.exists(os.path.dirname(dest)):
            os.makedirs(os.path.dirname(dest))
        filename = os.path.join(FOLDER,subfolder,'%.2i.markmin' % chapter_id)
    filename = os.path.join(FOLDER,subfolder,'images',key)
    if not os.path.isfile(filename):
    filename = os.path.join(FOLDER,subfolder,'references',key)
    if not os.path.isfile(filename):
from textblob.compat import PY2, request, urlquote
    string_pattern = r"\"(([^\"\\]|\\.)*)\""
    translation_pattern = re.compile(
                           + string_pattern + r"\,"
                           + string_pattern + r"\,"
                           + string_pattern + r"\,"
                           + string_pattern
    detection_pattern = re.compile(
        match = self.detection_pattern.match(content)
            m = self.translation_pattern.match(content, pos)
import os.path
def create_checksum(path):
    fh = open(path)
def build_file_paths(path):
    '''Recursively traverse the filesystem, starting at path, and return a full list
    path_collection = []
    for dirpath, dirnames, filenames in os.walk(path):
            fullpath = os.path.join(dirpath, file)
            path_collection.append(fullpath)
    return path_collection
def find_duplicates(path='.'):
    file_list = build_file_paths(path)
        compound_key = (os.path.getsize(file), create_checksum(file))
    path = '.'
        path = sys.argv[1]
    print "Searching for duplicates in '%s' ..." % path
    duplicates = find_duplicates(path)
    from autokey.common import DOMAIN_SOCKET_PATH, PACKET_SIZE
def gen_find(filepat,top):
    for path, dirlist, filelist in os.walk(top):
        for name in fnmatch.filter(filelist,filepat):
            yield os.path.join(path,name)
pat = re.compile(r"(\S+)|(<[^>]*>)")
	for m in pat.finditer(text):
            print os.path.join(dirname, name)
import os.path
            print(os.path.join(dirname, name))
os.path.walk(topdir, step, exten)
for dirpath, dirnames, files in os.walk(topdir):
            print(os.path.join(dirpath, name))
with open(logpath, 'a') as logfile:
    logfile.write('%s\n' % os.path.join(dirname, name))
import os.path
def step((ext, logpath), dirname, names):
            with open(logpath, 'a') as logfile:
                logfile.write('%s\n' % os.path.join(dirname, name))
os.path.walk(topdir, step, (exten, logname))
for dirpath, dirnames, files in os.walk(topdir):
            results += '%s\n' % os.path.join(dirpath, name)
from gluon.fileutils import listdir, cleanpath, tar, tar_compiled, untar
      'noshade', 'nowrap', 'open', 'optimum', 'pattern', 'ping', 'point-size',
      'linearGradient', 'line', 'marker', 'metadata', 'missing-glyph', 'mpath',
      'path', 'polygon', 'polyline', 'radialGradient', 'rect', 'set', 'stop',
       'overline-position', 'overline-thickness', 'panose-1', 'path',
       'pathLength', 'points', 'preserveAspectRatio', 'r', 'refX', 'refY',
https://github.com/clips/pattern/blob/master/pattern/web/feed/feedparser.py
#https://github.com/clips/pattern/blob/master/pattern/text/search.py
      'noshade', 'nowrap', 'open', 'optimum', 'pattern', 'ping', 'point-size',
      'linearGradient', 'line', 'marker', 'metadata', 'missing-glyph', 'mpath',
      'path', 'polygon', 'polyline', 'radialGradient', 'rect', 'set', 'stop',
       'overline-position', 'overline-thickness', 'panose-1', 'path',
       'pathLength', 'points', 'preserveAspectRatio', 'r', 'refX', 'refY',
    parser_dir = os.path.realpath(os.path.join("contrib", "stanford-parser"))
    parser_dir = os.path.realpath(os.path.join("contrib", "stanford-corenlp"))
    malt_dir = os.path.realpath(os.path.join('contrib', 'malt-parser'))
        dir = os.path.abspath(dir)
            nfile = os.path.join(dir,file)
            if os.path.isdir(nfile):
        dir = os.path.abspath(dir)
            nfile = os.path.join(dir,file)
            if(os.path.isfile(nfile)):
                basename, extention = os.path.splitext(nfile)
            if os.path.isdir(nfile):
            elif(os.path.isdir(nfile)):
            file_list.append(os.path.join(root,name))
# ensure the openshot module directory is in the system path so relative 'import' statements work
base_path = os.path.dirname(os.path.abspath(__file__))
if sys.path.count(base_path) == 0:
	sys.path.insert(0, base_path)
# If the openshot python code is found in the Python path, then
    # for finding modules).  You can add paths to that list.  Note
    # You can extend python path for looking up modules
    #prefs.add('python_path', '~/python/')
        def delete_file(path):
            if os.path.exists(path):
                    size2 = os.path.getsize(path)
                    os.remove(path)
                    ret += "        Deleted {0} ({1} bytes)\n".format(path,size2)
                    ret += "        Failed to delete {0}\n".format(path)
        def clear_data(data,data_str,path):
            delete_file(path)
        def open_file(data,data_str,path):
            file = open(path,"wb")
                    ret += "        Failed to save {0} to {1}\n".format(data_str,path)
                    size = os.path.getsize(path)
                    ret += "        Failed to get the size of {0}\n".format(path)
                    ret += "        {0} was saved to {1} ({2} bytes)\n".format(data_str,path,size)
                ret += "        Failed to open {0} at {1}\n".format(data_str,path)
        def load_file(data,data_str,path):
            if os.path.exists(path):
                file1 = open(path,'rb')
                        size1 = os.path.getsize(path)
                        ret += "        Loaded {0} from {1} ({2} bytes)\n".format(data_str,path,size1)
                        ret += "        Failed to load {0} from {1}\n".format(data_str,path)
                    ret += "        Failed to open {0}\n".format(path)
                ret += "        {0} does not exist\n".format(path)
                    open_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    open_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    open_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    index = load_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    graph = load_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    ranks = load_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    index = clear_data(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    graph = clear_data(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    ranks = clear_data(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
import os.path
pdfpath = os.path.join('db', pid, 'paper.pdf')
if not os.path.isfile(pdfpath):
  print "wat?? %s is missing. Can't extract top words. Exitting." % (pdfpath, )
picklepath = os.path.join('db', pid, 'topwords.p')
if os.path.isfile(pdfpath):
  cmd = "pdftotext %s %s" % (pdfpath, "out.txt")
  pickle.dump(top, open(picklepath, "wb"))
import os.path
def create_checksum(path):
    fh = open(path)
def build_file_paths(path):
    '''Recursively traverse the filesystem, starting at path, and return a full list
    path_collection = []
    for dirpath, dirnames, filenames in os.walk(path):
            fullpath = os.path.join(dirpath, file)
            path_collection.append(fullpath)
    return path_collection
def find_duplicates(path='.'):
    file_list = build_file_paths(path)
        compound_key = (os.path.getsize(file), create_checksum(file))
    path = '.'
        path = sys.argv[1]
    print "Searching for duplicates in '%s' ..." % path
    duplicates = find_duplicates(path)
def load_data(data_path='/Users/maxlikely/data/economist/raw/debates'):
    import path
    filenames = path.path(data_path).files('*.json')
	(dirName, fileName) = os.path.split(name)
	(fileBaseName, fileExtension)=os.path.splitext(fileName)
    raise ValueError(sys.path)
def handle_pygments(pat, content, pygments_lexer):
        data = pat.search(content)
            data = pat.search(content)
                pat = re.compile(r'(\[code-python\])(.*?)(\[/code-python\])',
                clean_content = handle_pygments(pat, clean_content, PythonLexer)
                pat = re.compile(r'(\[code-c#\])(.*?)(\[/code-c#\])',
                clean_content = handle_pygments(pat, clean_content, CSharpLexer)
                pat = re.compile(r'(\[code-c\])(.*?)(\[/code-c\])',
                clean_content = handle_pygments(pat, clean_content, CLexer)
                pat = re.compile(r'(\[code-c\+\+\])(.*?)(\[/code-c\+\+\])',
                clean_content = handle_pygments(pat, clean_content, CppLexer)
                pat = re.compile(r'(\[code-java\])(.*?)(\[/code-java\])',
                clean_content = handle_pygments(pat, clean_content, JavaLexer)
                pat = re.compile(r'(\[code-php\])(.*?)(\[/code-php\])',
                clean_content = handle_pygments(pat, clean_content, PhpLexer)
    #print("Usage: %s [option] <textfile>" % os.path.basename(sys.argv[0]))
                Pattern = RegExp.pattern
                Founds.append((Line, Policy, Pattern, Text))
'''This file is adapted from the pattern library.
URL: http://www.clips.ua.ac.be/pages/pattern-web
from .compat import text_type, basestring, imap, unicode, binary_type, PY2
    MODULE = os.path.dirname(os.path.abspath(__file__))
# The default part-of-speech tagset used in Pattern is Penn Treebank II.
# Pattern's text parsers are based on Brill's algorithm.
def _read(path, encoding="utf-8", comment=";;;"):
    """ Returns an iterator over the lines in the file at the given path,
    if path:
        if isinstance(path, basestring) and os.path.exists(path):
            # From file path.
                f = codecs.open(path, 'r', encoding='utf-8')
                f = open(path, 'r', encoding='utf-8')
        elif isinstance(path, basestring):
            f = path.splitlines()
        elif hasattr(path, "read"):
            f = path.read().splitlines()
            f = path
    def __init__(self, path="", morphology=None, context=None, entities=None, NNP="NNP", language=None):
        self._path = path
        self.morphology = Morphology(self, path=morphology)
        self.context    = Context(self, path=context)
        self.entities   = Entities(self, path=entities, tag=NNP)
        dict.update(self, (x.split(" ")[:2] for x in _read(self._path) if x.strip()))
    def path(self):
        return self._path
    def __init__(self, lexicon={}, path=""):
        self._path = path
    def path(self):
        return self._path
        list.extend(self, (x.split() for x in _read(self._path)))
    def __init__(self, lexicon={}, path=""):
        self._path = path
    def path(self):
        return self._path
        list.extend(self, (x.split() for x in _read(self._path)))
RE_ENTITY1 = re.compile(r"^http://")                            # http://www.domain.com/path
    def __init__(self, lexicon={}, path="", tag="NNP"):
        self._path = path
    def path(self):
        return self._path
        for x in _read(self.path):
        # Note: we could also scan for patterns, e.g.,
    def __init__(self, path="", language=None, synset=None, confidence=None, **kwargs):
        self._path       = path   # XML file path.
    def path(self):
        return self._path
    def load(self, path=None):
        """ Loads the XML-file (with sentiment annotations) from the given path.
            By default, Sentiment.path is lazily loaded.
        if not path:
            path = self._path
        if not os.path.exists(path):
        xml = cElementTree.parse(path)
        # A pattern.en.wordnet.Synset.
        # A pattern.en.Text.
        # A pattern.en.Sentence or pattern.en.Chunk.
        # A pattern.en.Word.
        # A pattern.vector.Document.
# The shallow parser in Pattern is meant to handle the following tasks:
# Pattern.parse() returns a TaggedString: a Unicode string with "tags" and "language" attributes.
# The pattern.text.tree.Text class uses this attribute to determine the token format and
    def __init__(self, path=""):
        self._path = path
        for x in _read(self._path):
    def path(self):
        return self._path
    def train(self, s, path="spelling.txt"):
        """ Counts the words in the given string and saves the probabilities at the given path.
        f = open(path, "w")
#root: Current path which is "walked through"
#And please use os.path.join instead of concatenating with a slash! Your problem is filePath = rootdir + '/' + file - you must concatenate the currently "walked" folder instead of the topmost folder. So that must be filePath = os.path.join(root, file). BTW "file" is a builtin, so you don't normally use it as variable name.
    outfileName = os.path.join(root, "py-outfile.txt")
            filePath = os.path.join(root, filename)
            with open( filePath, 'r' ) as f:
                folderOut.write("The file %s contains %s" % (filePath, toWrite))
                with open(os.path.join(root, file), 'r') as fin:
      matches.append(os.path.join(root, filename))
def find_files(directory, pattern):
            if fnmatch.fnmatch(basename, pattern):
                filename = os.path.join(root, basename)
'analiss analsis analisis', 'patterns': 'pattarns', 'qualities': 'quaties', 'easily':
HERE = os.path.dirname(os.path.abspath(__file__))
sys.path.append(HERE)
            file_path = os.path.join(sublime.packages_path(), 'User', file_name)
            if os.path.exists(file_path):
            file = open(file_path, "wb")
            self.view.window().open_file(file_path)
            [os.path.basename(filepath), filepath]
                for filepath
                    in iglob(os.path.join(sublime.packages_path(), 'User', '*.sublime-snippet'))]
           open(os.path.join("/home/ahmed/Dropbox/Causes.txt")).readlines()
    %s <work_dir> <search_regex> <replace_with> <glob_pattern> [backup]
    """ % (os.path.basename(sys.argv[0]))
def find_replace(directory, search_pattern, replacement, glob_pattern, create_backup=False):
    for path, dirs, files in os.walk(os.path.abspath(directory)):
        for filename in fnmatch.filter(files, glob_pattern):
            pardir = os.path.normpath(os.path.join(path, '..'))
            pardir = os.path.split(pardir)[-1]
            filepath = os.path.join(path, filename)
                backup_path = filepath + '.bak'
                print 'DBG: creating backup', backup_path
                shutil.copyfile(filepath, backup_path)
            with open(filepath) as f:
            with open(filepath, "w") as f:
                print 'DBG: replacing in file', filepath
                # s = s.replace(search_pattern, replacement)
                data = re.sub(search_pattern, replacement, data)
    glob_pattern = sys.argv[4]
    find_replace(work_dir, search_regex, replacement, glob_pattern, dobackup)
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
                if os.path.splitext(dirfile)[1][1:] in args:
        elif os.path.isdir(dirfile) and subdir:
def recursive(path):
	"""Move through all files, directories, and subdirectories of a path"""
	yield path
	for name in os.listdir(path):
		fullpath = os.path.join(path, name)
		if os.path.isdir(fullpath):
			for subpath in recursive(fullpath):
				yield subpath
			yield fullpath
def nonrecursive(path):
	"""Move through all files, directories, and subdirectories of a path"""
	paths = [path]
	while paths:
            dpath = paths.pop(0)
            yield dpath
            for name in os.listdir(dpath):
                    fullpath = os.path.join(path, name)
                    if os.path.isdir(fullpath):
                            paths.append(fullpath)
                            yield fullpath
	# 'pats' is the list of regular expressions used to find an answer.
	ans, index, fi, pats = reset(index, fi)
	pats.append( re.compile(r"(\w+ )*\w*act(iv)? transport( \w+)*" ) )
	ans2, line_match2 = fet(text, pats)
		pats = []
		pats.append( re.compile(r"(\w+ )*\w*e[nv]er\w* (\w+ )?(is )?((requir)|(us)|(need)|(invol))\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*((requir)|(us)|(need)|(take)) (\w+ ){0,5}e[nv]er\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*((against)|(up)|(across)) the concentr gr\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*from (\w+ ){0,3}((low)|(less))\w* (\w+ )?to (\w+ ){0,3}((high)|(great))\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*with the aid\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*us of atp?\w*( \w+)*" ) )
		ans, line_match = fet(text, pats)
	ans, index, fi, pats = reset(index, fi)
	#pats.append( re.compile(r"(\w+ )*\w*select membran\w* (\w+ )*" ) )
	#anst, line_match2 = fet(text, pats)
		pats = []
		pats.append( re.compile(r"(\w+ )*\w*energi is not\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*no energ\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*neither energ\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*no(t|r) (\w+ ){0,4}energ\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*without (\w+ ){0,3}energ\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*doesn\w* (\w+ ){0,3}energ\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*((along)|(with)) the (concentr )?gra\w*( \w+)*" ) )
		#ans, line_match = fet(line_match2, pats)
		ans, line_match = fet(line_match2 , pats)
	ans, index, fi, pats = reset(index, fi)
	#pats.append( re.compile(r"(\w+ )*\w*select membran\w* (\w+ )*" ) )
	#anst, line_match2 = fet(text, pats)
		pats = []
		pats.append( re.compile(r"(\w+ )*\w*sodium potassium pump\w*( \w+)*" ) )
		#ans, line_match = fet(line_match2, pats)
		ans, line_match = fet(line_match2 , pats)
	ans, index, fi, pats = reset(index, fi)
	pats.append( re.compile(r"(\w+ )*\w*di((ff)|(sc))us\w* (\w+ )*" ) )
	anst, line_match2 = fet(text, pats)
		pats = []
		pats.append( re.compile(r"(\w+ )*\w*((along)|(with)|(follow)) (\w+ )concentr gra\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*((high)|(great))\w* (\w+ )?\w*to (\w+ ){0,3}((low)|(less))\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*((high)|(great))\w* (\w+ )*move (\w+ )*to (\w+ ){0,4}((low)|(less))\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*without (\w+ ){0,3}energ" ) )
		#ans, line_match = fet(line_match2, pats)
		ans, line_match = fet(text, pats)
	ans, index, fi, pats = reset(index, fi)
	#pats = []
	#pats.append( re.compile(r"(\w+ )*\w*osmosi\w* (\w+ )*" ) )
	#anst, line_match2 = fet(text, pats)
		pats = []
		pats.append( re.compile(r"(\w+ )*low\w* (\w+ )*high\w* ?(\w+ )*" ) )
		pats.append( re.compile(r"(\w+ ){0,4}((mo)|(transport)|(diff)|(transfer)|(flow)|(pass)|(come))\w* (of )?water (\w+ ){0,6}\w?((cross)|(in(to)? ((and)|(or)) out of)|(th(ro)?u(gh)?)|(between)) (\w{1,3} )?(\w+ ){0,2}((cell)|(membran)|(barrier))" ) )
		pats.append( re.compile(r"(\w+ ){0,4}water (\w+ )?((mo)|(transport)|(diff)|(transfer)|(flow)|(pass)|(come))\w* (\w+ ){0,6}\w?((cross)|(in(to)? ((and)|(or)) out of)|(th(ro)?u(gh)?)|(between)) (\w{1,3} )?(\w+ ){0,2}((cell)|(membran)|(barrier))" ) )
		pats.append( re.compile(r"osmosi (which )?(\w+ )?(the )?((mo)|(transport)|(diff)|(transfer)|(flow)|(pass)|(come))\w* (\w{1,3} )water" ) )
		pats.append( re.compile(r"(\w+ )*water (\w+ )*go\w* ?(\w+ )*" ) )
		pats.append( re.compile(r"hyperto\w* (\w+ )*hypoto\w*" ) )
		pats.append( re.compile(r"hypoto\w* (\w+ )*hyperto\w*" ) )
		pats.append( re.compile(r"(\w+ )*high\w* (\w+ )*low\w* ?(\w+ )*" ) ) #1
		pats.append( re.compile(r"(\w+ )*low\w* (\w+ )*high\w* ?(\w+ )*" ) ) #2
		pats.append( re.compile(r"substanc (\w+ )*pass (\w+ )*membran\w* ?(\w+ )*" ) )
		pats.append( re.compile(r"(\w+ )*semi permeabl membran\w* ?(\w+ )*" ) )
		pats.append( re.compile(r"(\w+ )*gradient ?(\w+ )*" ) ) #1
		pats.append( re.compile(r"(\w+ )*get\w* water ?(\w+ )*" ) )
		pats.append( re.compile(r"(\w+ )*pull\w* water ?(\w+ )*" ) )
		pats.append( re.compile(r"(\w+ )*pass\w* (\w+ )*water ?(\w+ )*" ) )
		pats.append( re.compile(r"(\w+ )*water (\w+ )*pass\w* ?(\w+ )*" ) )
		pats.append( re.compile(r"(\w+ )*let\w* (\w+ )*water ?(\w+ )*" ) ) ########### 1.5
		pats.append( re.compile(r"(\w+ )*\w*water diffus\w*( \w+)*" ) ) ########### 1.5
		ans, line_match = fet(line_match2, pats)
	ans, index, fi, pats = reset(index, fi)
	pats.append( re.compile(r"(\w+ )*\w*fac\w* diffus\w* (\w+ )*" ) )
	anst, line_match2 = fet(text, pats)
		pats = []
		pats.append( re.compile(r"(\w+ )*\w*protein channel\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*((requir)|(us)|(need)|(take)) (\w+ ){0,5}e[nv]er\w*( \w+)*" ) )
		#ans, line_match = fet(line_match2, pats)
		ans, line_match = fet(text, pats)
	ans, index, fi, pats = reset(index, fi)
	pats.append( re.compile(r"(\w+ )*\w*endocytosi\w* (\w+ )*" ) )
	anst, line_match2 = fet(text, pats)
		pats = []
		pats.append( re.compile(r"(\w+ )*\w*surround a substanc\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*cell surround\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*engulf\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*vesicl around\w*( \w+)*" ) )
		#ans, line_match = fet(line_match2, pats)
		ans, line_match = fet(text, pats)
	ans, index, fi, pats = reset(index, fi)
	#pats = []
	#pats.append( re.compile(r"(\w+ )*\w*select membran\w* (\w+ )*" ) )
	#anst, line_match2 = fet(text, pats)
		pats = []
		pats.append( re.compile(r"(\w+ )*\w*decid\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*choose\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*control what\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*allow (\w+ )?certain\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*let certain\w*( \w+)*" ) )
		pats.append( re.compile(r"(\w+ )*\w*let thing\w*( \w+)*" ) )
		#ans, line_match = fet(line_match2, pats)
		ans, line_match = fet(text, pats)
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
                if os.path.splitext(dirfile)[1][1:] in args:
        elif os.path.isdir(dirfile) and subdir:
def matches(pattern, text):
    'String comparison. True if string ``text`` matches regex ``pattern``'
    return re.compile(str(pattern)).match(text)
def search(pattern, text):
    'Regex pattern search. Returns match if ``pattern`` is found in ``text``'
    return re.compile(str(pattern)).search(str(text))
        # Then, retrieve the pattern (into full desc tasks)
        # Then, retrieve the pattern (into full desc tasks)
        GTD> load path/to/todo.txt"""
        self.todotxt = todotxt  # ok, save file path
        GTD> save [path/to/todo.txt]"""
        GTD> archive [path/to/done.txt]"""
        GTD> print [path/to/todo.rest]"""
        def delete_file(path):
            if os.path.exists(path):
                    size2 = os.path.getsize(path)
                    os.remove(path)
                    ret += "        Deleted {0} ({1} bytes)\n".format(path,size2)
                    ret += "        Failed to delete {0}\n".format(path)
        def clear_data(data,data_str,path):
            delete_file(path)
        def open_file(data,data_str,path):
            file = open(path,"wb")
                    ret += "        Failed to save {0} to {1}\n".format(data_str,path)
                    size = os.path.getsize(path)
                    ret += "        Failed to get the size of {0}\n".format(path)
                    ret += "        {0} was saved to {1} ({2} bytes)\n".format(data_str,path,size)
                ret += "        Failed to open {0} at {1}\n".format(data_str,path)
        def load_file(data,data_str,path):
            if os.path.exists(path):
                file1 = open(path,'rb')
                        size1 = os.path.getsize(path)
                        ret += "        Loaded {0} from {1} ({2} bytes)\n".format(data_str,path,size1)
                        ret += "        Failed to load {0} from {1}\n".format(data_str,path)
                    ret += "        Failed to open {0}\n".format(path)
                ret += "        {0} does not exist\n".format(path)
                    open_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    open_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    open_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    index = load_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    graph = load_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    ranks = load_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    index = clear_data(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    graph = clear_data(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    ranks = clear_data(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
grades = [[float(n) for n in l.split()[1:]] for l in open(os.path.join("data/grades.txt")).readlines()[::-1][:-5]]
    url_image = 'http://%s.wikipedia.org/w/index.php?title=Special:FilePath&file=%s'
def get_paths():
    Redefine data_path and submissions_path here to run the benchmarks on your machine
    data_path = os.environ["DATAPATH"]
    submission_path = os.environ["DATAPATH"]
    return data_path, submission_path
def get_train_test_df(data_path = None):
    if data_path is None:
        data_path, submission_path = get_paths()
    train = pd.read_csv(os.path.join(data_path, "train.csv"),
    test = pd.read_csv(os.path.join(data_path, "test.csv"),
globaldb = os.path.join('db', 'papers.p')
  pdir = os.path.join('db', pid)
  if os.path.isdir(pdir):
    refpath = os.path.join(pdir, 'references.p')
    if os.path.isfile(refpath):
      p['r'] = pickle.load(open(refpath, "rb"))
    citpath = os.path.join(pdir, 'citations.p')
    if os.path.isfile(citpath):
      p['c'] = pickle.load(open(citpath, "rb"))
    topWordsPicklePath = os.path.join(pdir, 'topwords.p')
    if os.path.isfile(topWordsPicklePath):
      twslist = pickle.load(open(topWordsPicklePath, "rb"))
    # image paths
      thumbs = [os.path.join('resources', pid, x) for x in thumbfiles]
outfile = os.path.join('client', 'db.json')
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
                if os.path.splitext(dirfile)[1][1:] in args:
        elif os.path.isdir(dirfile) and subdir:
'''The pluralize and singular methods from the pattern library.
See here https://github.com/clips/pattern/blob/master/LICENSE.txt for
        "ibis", "lens", "mantis", "marquis", "metropolis", "pathos", "pelvis", "polis", "rhinoceros",
                Recently, in vitro studies have shown that T.thermophylus EFTu can .... (Patel et al., 1997; Jones et al., 1998).
                In 1990 Patel et al. demonstrated that replacement of H2O with heavy water led to ....
                The first systematic study of X was reported by Patel et al. in 1986.
                Detailed examination of X by Smith and Patel (1961) showed that ....
            X was prepared according to the procedure used by Patel et al. (1957).
            A random sample of patients with .... was recruited from ....
            Blood samples were obtained with consent, from 256 caucasian male patients ....
                    Of the 148 patients who completed the questionnaire, just over half indicated that .......
                One unanticipated finding was that ....
                These findings cannot be extrapolated to all patients.
            One unanticipated finding was that ....
            These findings cannot be extrapolated to all patients.
            A limitation of this study is that the numbers of patients and controls were relatively small.
            Generally, spectratyping provides two types of information: band intensity pattern and band number.
            There are two types of effect which result when a patient undergoes X. These are ....
            Speech functions are less likely to be affected in women because the critical area is less often affected. A similar pattern emerges in studies of the control of hand movements.
            The first systematic study of the X was reported by Patel et al. in 1986.
            In 1990 Patel et al. demonstrated that replacement of H2O with heavy water led to ......
            Of the 148 patients who completed the questionnaire, just over half indicated that .......
# ensure the openshot module directory is in the system path so relative 'import' statements work
base_path = os.path.dirname(os.path.abspath(__file__))
if sys.path.count(base_path) == 0:
	sys.path.insert(0, base_path)
build_dir = os.path.join(docs_dir, '_build')
    run("open %s" % os.path.join(build_dir, 'index.html'))
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('..'))
sys.path.append(os.path.abspath("_themes"))
# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']
# List of patterns, relative to source directory, that match files and
exclude_patterns = ['_build']
# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ['_themes']
# The name of an image file (within the static path) to use as favicon of the
# Add any paths that contain custom static files (such as style sheets) here,
html_static_path = ['_static']
'''text module for backwards compatibility. Importing
from textblob.compat import unicode
HERE = os.path.abspath(os.path.dirname(__file__))
CSV_FILE = os.path.join(HERE, 'data.csv')
JSON_FILE = os.path.join(HERE, "data.json")
TSV_FILE = os.path.join(HERE, "data.tsv")
from textblob.compat import unicode
from textblob.parsers import PatternParser
from textblob.en import parse as pattern_parse
class TestPatternParser(unittest.TestCase):
        self.parser = PatternParser()
        assert_equal(self.parser.parse(self.text), pattern_parse(self.text))
HERE = os.path.abspath(os.path.dirname(__file__))
AP_MODEL_LOC = os.path.join(HERE, 'trontagger.pickle')
class TestPatternTagger(unittest.TestCase):
        self.tagger = textblob.taggers.PatternTagger()
        tagger = textblob.taggers.PatternTagger()
from textblob.compat import unicode
HERE = os.path.abspath(os.path.dirname(__file__))
CSV_FILE = os.path.join(HERE, 'data.csv')
JSON_FILE = os.path.join(HERE, "data.json")
TSV_FILE = os.path.join(HERE, "data.tsv")
from textblob.sentiments import PatternAnalyzer, NaiveBayesAnalyzer, DISCRETE, CONTINUOUS
class TestPatternSentiment(unittest.TestCase):
        self.analyzer = PatternAnalyzer()
from textblob.compat import PY2, unicode, basestring, binary_type
from textblob.taggers import NLTKTagger, PatternTagger
from textblob.sentiments import NaiveBayesAnalyzer, PatternAnalyzer
from textblob.parsers import PatternParser
        pattern = PatternAnalyzer()
        assert_equal(blob.polarity, pattern.analyze(str(blob))[0])
        assert_equal(blob.subjectivity, pattern.analyze(str(blob))[1])
    def test_pos_tagger_defaults_to_pattern(self):
        assert_true(isinstance(blob.pos_tagger, PatternTagger))
        assert_equal(blob.parse(), PatternParser().parse(blob.string))
        tagger = PatternTagger()
        analyzer = PatternAnalyzer
        assert_true(isinstance(blob.pos_tagger, PatternTagger))
        expected = "Blobber(tokenizer=WordTokenizer(), pos_tagger=PatternTagger(), np_extractor=FastNPExtractor(), analyzer=PatternAnalyzer(), parser=PatternParser(), classifier=None)"
    from distutils.util import convert_path
        'where' should be supplied as a "cross-platform" (i.e. URL-style) path; it
        will be converted to the appropriate local path syntax.  'exclude' is a
        stack = [(convert_path(where), '')]
                fn = os.path.join(where, name)
                if ('.' not in name and os.path.isdir(fn) and
                        os.path.isfile(os.path.join(fn, '__init__.py'))):
        for pat in list(exclude)+['ez_setup', 'distribute_setup']:
            out = [item for item in out if not fnmatchcase(item, pat)]
from textblob.compat import PY2
from textblob.compat import PY2, request, urlquote
    string_pattern = r"\"(([^\"\\]|\\.)*)\""
    translation_pattern = re.compile(
                           + string_pattern + r"\,"
                           + string_pattern + r"\,"
                           + string_pattern + r"\,"
                           + string_pattern
    detection_pattern = re.compile(
        match = self.detection_pattern.match(content)
            m = self.translation_pattern.match(content, pos)
PACKAGE_DIR = os.path.dirname(os.path.abspath(__file__))
'''Default parsers to English for backwards compatibility so you can still do
>>> from textblob.parsers import PatternParser
>>> from textblob.en.parsers import PatternParser
from textblob.en.parsers import PatternParser
from textblob.compat import with_metaclass
from textblob.compat import basestring, implements_to_string, PY2, binary_type
from nltk.compat import (string_types, total_ordering, text_type,
                         python_2_unicode_compatible, unicode_repr)
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk import compat
@compat.python_2_unicode_compatible
        language = compat.raw_input("Please enter the name of the language " +
2- Adding the pattern (تفاعيل) to ISRI pattern set.
                    2: ['\u0627', '\u0648', '\u064A'], 3:['\u0629']}   # groups of length four patterns
                     6: ['\u0627', '\u0645']}   # Groups of length five patterns and length three roots
        """process length four patterns and extract length three roots"""
        """process length five patterns and extract length three roots"""
        """process length five patterns and extract length four roots"""
        """process length six patterns and extract length three roots"""
        elif (self.stm[0]== '\u062a' and self.stm[2]== '\u0627' and self.stm[4]== '\u064a'):      #     تفاعيل    new pattern
        """process length six patterns and extract length four roots"""
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
        if not hasattr(regexp, 'pattern'):
        return '<RegexpStemmer: %r>' % self._regexp.pattern
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
def brown_tagset(tagpattern=None):
    _format_tagset("brown_tagset", tagpattern)
def claws5_tagset(tagpattern=None):
    _format_tagset("claws5_tagset", tagpattern)
def upenn_tagset(tagpattern=None):
    _format_tagset("upenn_tagset", tagpattern)
def _format_tagset(tagset, tagpattern=None):
    if not tagpattern:
    elif tagpattern in tagdict:
        _print_entries([tagpattern], tagdict)
        tagpattern = re.compile(tagpattern)
        tags = [tag for tag in sorted(tagdict) if tagpattern.match(tag)]
    version_file = os.path.join(os.path.dirname(__file__), 'VERSION')
NLTK_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, NLTK_ROOT)
NLTK_TEST_DIR = os.path.join(NLTK_ROOT, 'nltk')
        with a patched version.
from nltk.compat import PY3
        raise SkipTest("compat.doctest is for Python 2.x")
import os.path
    #print "-----", glob(os.path.join(os.path.dirname(__file__), '*.doctest'))
    dir = os.path.dirname(__file__)
    paths = glob(os.path.join(dir, '*.doctest'))
    files = [ os.path.basename(path) for path in paths ]
#if os.path.split(path)[-1] != 'index.rst'
from nltk.compat import PY3
Unit tests for nltk.compat.
See also nltk/test/compat.doctest.
from nltk.compat import PY3, python_2_unicode_compatible
        raise SkipTest("test_2x_compat is for testing nltk.compat under Python 2.x")
            # Patch all test_ methods to raise SkipText exception.
                patched_method = skip(reason)(getattr(test_item, meth_name))
                setattr(test_item, meth_name, patched_method)
            name = os.path.basename(filename)
                base, ext = os.path.splitext(name)
                dirname = os.path.dirname(filename)
                sys.path.append(dirname)
                        "Could not import %s: %s (%s)", fixt_mod, e, sys.path)
                yield ContextList([self._patchTestCase(c) for c in case], case.context)
                yield self._patchTestCase(case)
            cases = [self._patchTestCase(case) for case in suite._get_tests()]
    def _patchTestCase(self, case):
from nltk import compat
        for f in compat.itervalues(self.fragments):
@compat.python_2_unicode_compatible
     /path/to/candc/
from nltk.compat import python_2_unicode_compatible
        self._candc_models_path = os.path.normpath(os.path.join(self._candc_bin[:-5], '../models'))
        args = ['--models', os.path.join(self._candc_models_path, ['boxer','questions'][question]),
            path_to_bin=bin_dir,
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import string_types, python_2_unicode_compatible
@python_2_unicode_compatible
        path = nltk.data.find(dbname)
        connection =  sqlite3.connect(str(path))
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import string_types, python_2_unicode_compatible
    # imports are fixed for Python 2.x by nltk.compat
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
    Check that batch_interpret() is compatible with legacy grammars that use
from nltk.compat import (total_ordering, string_types,
                         python_2_unicode_compatible)
@python_2_unicode_compatible
def unique_variable(pattern=None, ignore=None):
    :param pattern: ``Variable`` that is being replaced.  The new variable must
    if pattern is not None:
        if is_indvar(pattern.name):
        elif is_funcvar(pattern.name):
        elif is_eventvar(pattern.name):
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
                self = self.alpha_convert(unique_variable(pattern=self.variable))
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import htmlentitydefs
    #pattern = re.compile("&(\w+?);")
    #s = pattern.sub(descape_entity, s)
def extract_rels(subjclass, objclass, doc, corpus='ace', pattern=None, window=10):
    Filter the output of ``mk_reldicts`` according to specified NE classes and a filler pattern.
    :param pattern: a regular expression for filtering the fillers of
    :type pattern: SRE_Pattern
                           pattern.match(x['filler']) and
            for rel in extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern=IN):
            for rel in extract_rels('PER', 'ORG', doc, corpus='ieer', pattern=ROLES):
        for rel in extract_rels('PER', 'ORG', doc, corpus='conll2002', pattern=VAN, window=10):
            for rel in extract_rels('ORG', 'LOC', doc, corpus='conll2002', pattern = DE)]
        print(extract_rels('ORG', 'LOC', sent, corpus='ace', pattern = IN))
from nltk.compat import string_types, python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import string_types, python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import string_types
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
                os.path.join('grammars', 'sample_grammars',
from nltk import compat  # this fixes tkinter imports for Python 2.x
from nltk import compat
@compat.python_2_unicode_compatible
        X, y = list(compat.izip(*labeled_featuresets))
from nltk import compat
@compat.python_2_unicode_compatible
        if not isinstance(f_id, compat.integer_types):
            if isinstance(fval, (compat.integer_types, float)):
        if not isinstance(f_id, compat.integer_types):
    >>> megam.config_megam() # pass path to megam if not found in PATH # doctest: +SKIP
import os.path
from nltk import compat
    :param bin: The full path to the ``megam`` binary.  If not specified,
    if isinstance(args, compat.string_types):
    if isinstance(stdout, compat.string_types):
from nltk import compat
    if isinstance(args, compat.string_types):
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk import compat
_weka_classpath = None
def config_weka(classpath=None):
    global _weka_classpath
    if classpath is not None:
        _weka_classpath = classpath
    if _weka_classpath is None:
        searchpath = _weka_search
            searchpath.insert(0, os.environ['WEKAHOME'])
        for path in searchpath:
            if os.path.exists(os.path.join(path, 'weka.jar')):
                _weka_classpath = os.path.join(path, 'weka.jar')
                version = _check_weka_version(_weka_classpath)
                           (_weka_classpath, version)))
                    print('[Found Weka: %s]' % _weka_classpath)
                _check_weka_version(_weka_classpath)
    if _weka_classpath is None:
            test_filename = os.path.join(temp_dir, 'test.arff')
            (stdout, stderr) = java(cmd, classpath=_weka_classpath,
                os.remove(os.path.join(temp_dir, f))
            train_filename = os.path.join(temp_dir, 'train.arff')
            java(cmd, classpath=_weka_classpath, stdout=stdout)
                os.remove(os.path.join(temp_dir, f))
                elif issubclass(type(fval), (compat.integer_types, float, bool)):
                elif issubclass(type(fval), compat.string_types):
        elif isinstance(fval, (bool, compat.integer_types)):
    >>> mallet.config_mallet() # pass path to mallet as argument if needed # doctest: +SKIP
import os.path
_mallet_classpath = None
    :param mallet_home: The full path to the mallet directory. If not
    global _mallet_home, _mallet_classpath
    bin_dir = os.path.split(mallethon_bin)[0]
    _mallet_home = os.path.split(bin_dir)[0]
    # Construct a classpath for using mallet.
    lib_dir = os.path.join(_mallet_home, 'lib')
    if not os.path.isdir(lib_dir):
    _mallet_classpath = os.path.pathsep.join(os.path.join(lib_dir, filename)
def call_mallet(cmd, classpath=None, stdin=None, stdout=None, stderr=None,
    Call `nltk.internals.java` with the given command, and with the classpath
    if _mallet_classpath is None:
    # Set up the classpath
    if classpath is None:
        classpath = _mallet_classpath
        classpath += os.path.pathsep + _mallet_classpath
    return java(cmd, classpath, stdin, stdout, stderr, blocking)
import nltk.compat
    a way that's incompatible with the fact that ``Table`` behaves as a
import nltk.compat
    import nltk.compat
import nltk.compat
import nltk.compat
    def expanded_tree(self, *path_to_tree):
        :param path_to_tree: A list of indices i1, i2, ..., in, where
            For the root, the path is ``()``.
        return self._expanded_trees[path_to_tree]
    def collapsed_tree(self, *path_to_tree):
        :param path_to_tree: A list of indices i1, i2, ..., in, where
            For the root, the path is ``()``.
        return self._collapsed_trees[path_to_tree]
import nltk.compat
from nltk import compat
                elif (isinstance(node, compat.string_types) and
with the Viterbi algorithm, which efficiently computes the optimal path
probability of each distinct path through the model. Similarly, the highest
from nltk.compat import python_2_unicode_compatible, izip, imap
@python_2_unicode_compatible
        uses the best_path method to find the Viterbi path.
        path = self._best_path(unlabeled_sequence)
        return list(izip(unlabeled_sequence, path))
    def best_path(self, unlabeled_sequence):
        Returns the state sequence of the optimal (most probable) path through
        return self._best_path(unlabeled_sequence)
    def _best_path(self, unlabeled_sequence):
    def best_path_simple(self, unlabeled_sequence):
        Returns the state sequence of the optimal (most probable) path through
        return self._best_path_simple(unlabeled_sequence)
    def _best_path_simple(self, unlabeled_sequence):
from os import path, sep
from nltk import compat
    - path to the directory that contains SENNA executables.
    def __init__(self, senna_path, operations, encoding='utf-8'):
        self._path = path.normpath(senna_path) + sep
                return path.join(self._path, 'senna-linux64')
            return path.join(self._path, 'senna-linux32')
            return path.join(self._path, 'senna-win32.exe')
            return path.join(self._path, 'senna-osx')
        return path.join(self._path, 'senna')
        if not path.isfile(self.executable):
        _senna_cmd = [self.executable, '-path', self._path, '-usrtokens', '-iobtags']
        if isinstance(_input, compat.text_type) and encoding:
    - path to the directory that contains SENNA executables.
    def __init__(self, path, encoding='utf-8'):
        super(POSTagger, self).__init__(path, ['pos'], encoding)
    - path to the directory that contains SENNA executables.
    def __init__(self, path, encoding='utf-8'):
        super(NERTagger, self).__init__(path, ['ner'], encoding)
    - path to the directory that contains SENNA executables.
    def __init__(self, path, encoding='utf-8'):
        super(CHKTagger, self).__init__(path, ['chk'], encoding)
    if not path.isfile(tagger.executable):
from nltk import compat
@compat.python_2_unicode_compatible
                '--model-file', os.path.abspath(self.crf_info.model_filename),
                   '--model-file', os.path.abspath(filename),
        if isinstance(fval, compat.string_types):
        if isinstance(feature_detector, compat.string_types):
                if isinstance(self.src, compat.string_types):
                if isinstance(self.dst, compat.string_types):
from os.path import join
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
from nltk import compat
    def __init__(self, path_to_model, path_to_jar=None, encoding='ascii', verbose=False, java_options='-mx1000m'):
                self._JAR, path_to_jar,
                searchpath=(), url=_stanford_url,
        self._stanford_model = find_file(path_to_model,
        _input_fh, self._input_file_path = tempfile.mkstemp(text=True)
        if isinstance(_input, compat.text_type) and encoding:
        stanpos_output, _stderr = java(self._cmd,classpath=self._stanford_jar, \
        if (not compat.PY3) and encoding == 'ascii':
        os.unlink(self._input_file_path)
    A class for pos tagging with Stanford Tagger. The input is the paths to:
     - (optionally) the path to the stanford tagger jar file. If not specified here,
       then this jar file must be specified in the CLASSPATH envinroment variable.
                self._input_file_path, '-tokenize', 'false']
    A class for ner tagging with Stanford Tagger. The input is the paths to:
    - (optionally) the path to the stanford tagger jar file. If not specified here,
      then this jar file must be specified in the CLASSPATH envinroment variable.
                self._input_file_path, '-outputFormat', self._FORMAT]
from nltk import compat
    A class for pos tagging with HunPos. The input is the paths to:
     - (optionally) the path to the hunpos-tag binary
    def __init__(self, path_to_model, path_to_bin=None,
        :param path_to_model: The model file.
        :param path_to_bin: The hunpos-tag binary.
        hunpos_paths = ['.', '/usr/bin', '/usr/local/bin', '/opt/local/bin',
        hunpos_paths = list(map(os.path.expanduser, hunpos_paths))
                'hunpos-tag', path_to_bin,
                searchpath=hunpos_paths,
        self._hunpos_model = find_file(path_to_model,
            if isinstance(token, compat.text_type):
    r'(?!\.).*\.txt', cat_pattern=r'([a-z]*)/.*', encoding='latin-1')
    r'(?!\.).*\.txt', cat_pattern=r'(neg|pos)/.*',
ppattach = LazyCorpusLoader(
    'ppattach', PPAttachmentCorpusReader, ['training', 'test', 'devset'])
    ppattach.demo()
from nltk import compat
				if isinstance(textids, compat.string_types): textids = [textids]
		if isinstance(fileids, compat.string_types):
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
			return concat([TEICorpusView(self.abspath(fileid),
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
			return concat([TEICorpusView(self.abspath(fileid),
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
			return concat([TEICorpusView(self.abspath(fileid),
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
			return concat([TEICorpusView(self.abspath(fileid),
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
			return concat([TEICorpusView(self.abspath(fileid),
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
			return concat([TEICorpusView(self.abspath(fileid),
			return concat([TEICorpusView(self.abspath(fileid),
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
from nltk import compat
@compat.python_2_unicode_compatible
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
        for path, encoding, fileid in self.abspaths(include_encoding=True, include_fileid=True):
            with open(path) as lin_file:
                           for fileid in self.abspaths(fileids)])
                           for fileid in self.abspaths(fileids)])
                       for fileid in self.abspaths(fileids)])
                       for fileid in self.abspaths(fileids)])
from nltk import compat
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.CorpusView(path, self._read_word_block, encoding=enc)
                       for (path, enc, fileid)
                       in self.abspaths(fileids, True, True)])
        return concat([self.CorpusView(path, self._read_sent_block, encoding=enc)
                       for (path, enc, fileid)
                       in self.abspaths(fileids, True, True)])
        return concat([self.CorpusView(path, self._read_para_block, encoding=enc)
                       for (path, enc, fileid)
                       in self.abspaths(fileids, True, True)])
        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to
                       for (fileid, enc) in self.abspaths(fileids, True)])
from nltk.corpus.reader.ppattach import *
    'PPAttachmentCorpusReader', 'SensevalCorpusReader',
import os.path, codecs
from nltk import compat
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
                       for (f, enc) in self.abspaths(fileids, True)])
                       for (f, enc) in self.abspaths(fileids, True)])
                       for (f, enc) in self.abspaths(fileids, True)])
                       for (f, enc) in self.abspaths(fileids, True)])
                       for (f, enc) in self.abspaths(fileids, True)])
                       for (f, enc) in self.abspaths(fileids, True)])
                       for (f, enc) in self.abspaths(fileids, True)])
                       for (f, enc) in self.abspaths(fileids, True)])
                       for (f, enc) in self.abspaths(fileids, True)])
from nltk import compat
        if isinstance(fileids, compat.string_types):
        elif isinstance(documents, compat.string_types):
            if isinstance(documents, compat.string_types):
                           for fileid in self.abspaths(fileids)])
                           for fileid in self.abspaths(fileids)])
                           for fileid in self.abspaths(fileids)])
                           for fileid in self.abspaths(fileids)])
                           for fileid in self.abspaths(fileids)])
                           for fileid in self.abspaths(fileids)])
                           for fileid in self.abspaths(fileids)])
                           for fileid in self.abspaths(fileids)])
        (C{cat_pattern}, C{cat_map}, and C{cat_file}) are passed to
from nltk.compat import total_ordering, python_2_unicode_compatible, string_types
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return StreamBackedCorpusView(self.abspath(self._nomfile),
        return StreamBackedCorpusView(self.abspath(self._nomfile),
        etree = ElementTree.parse(self.abspath(framefile).open()).getroot()
            etree = ElementTree.parse(self.abspath(framefile).open()).getroot()
        return StreamBackedCorpusView(self.abspath(self._nounsfile),
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
from nltk import compat
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
from nltk import compat
        return concat([ToolboxData(path, enc).parse(key=key)
                       for (path, enc) in self.abspaths(fileids, True)])
                       in self.abspaths(fileids, include_encoding=True)])
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
from nltk import compat
    fileids = [f for f in find_corpus_fileids(FileSystemPathPointer(root), ".*")
    assert isinstance(knbc.words()[0], compat.string_types)
    assert isinstance(knbc.sents()[0][0], compat.string_types)
from nltk import compat
@compat.python_2_unicode_compatible
                       for (fileid, enc) in self.abspaths(fileids, True)])
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
    # and remove the & for those patterns that aren't regular XML
from nltk import compat
from nltk.data import PathPointer, FileSystemPathPointer, ZipFilePathPointer
    a corpus fileid (specified as a string or as a ``PathPointer``);
    In order to increase efficiency for random access patterns that
        :param fileid: The path to the file that is read by this
            ``PathPointer``.
            if isinstance(self._fileid, PathPointer):
        :type: str or PathPointer""")
        if isinstance(self._fileid, PathPointer):
    if all(isinstance(doc, compat.string_types) for doc in docs):
            if os.path.exists(self._fileid):
        if isinstance(output_file, compat.string_types):
    assert encoding is not None or isinstance(block, compat.text_type)
    if not isinstance(root, PathPointer):
        raise TypeError('find_corpus_fileids: expected a PathPointer')
    if isinstance(root, ZipFilePathPointer):
    # or symlinked) subdirectories, and match paths against the regexp.
    elif isinstance(root, FileSystemPathPointer):
        for dirname, subdirs, fileids in os.walk(root.path, **kwargs):
            prefix = ''.join('%s/' % p for p in _path_from(root.path, dirname))
def _path_from(parent, child):
    if os.path.split(parent)[1] == '':
        parent = os.path.split(parent)[0]
    path = []
        child, dirname = os.path.split(child)
        path.insert(0, dirname)
        assert os.path.split(child)[0] != child
    return path
from nltk import compat
        if not isinstance(fileid, compat.string_types):
        elt = ElementTree.parse(self.abspath(fileid).open()).getroot()
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
    paths, where a tag path is a list of element tag names, separated
        if isinstance(fileid, PathPointer):
from nltk import compat
        if isinstance(channels, compat.string_types):
        if isinstance(domains, compat.string_types):
        if isinstance(categories, compat.string_types):
        return [f for f in self.abspaths(fileids)]
            fp = self.abspath(f).replace('morph.xml', 'header.xml')
from nltk import compat
        if isinstance(encoding, compat.string_types):
        if isinstance(dialect, compat.string_types): dialect = [dialect]
        if isinstance(sex, compat.string_types): sex = [sex]
        if isinstance(spkrid, compat.string_types): spkrid = [spkrid]
        if isinstance(sent_type, compat.string_types): sent_type = [sent_type]
        if isinstance(sentid, compat.string_types): sentid = [sentid]
        if isinstance(utterances, compat.string_types): utterances = [utterances]
        if isinstance(utterances, compat.string_types): utterances = [utterances]
@compat.python_2_unicode_compatible
from nltk import compat
@compat.python_2_unicode_compatible
        return StreamBackedCorpusView(self.abspath('tagged'),
        return StreamBackedCorpusView(self.abspath('tagged'),
        return StreamBackedCorpusView(self.abspath('tagged'),
        return StreamBackedCorpusView(self.abspath('tagged'),
        return StreamBackedCorpusView(self.abspath('tagged'),
        return StreamBackedCorpusView(self.abspath('tagged'),
from nltk import compat
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
            for (fileid, enc) in self.abspaths(fileids, True)])
            for (fileid, enc) in self.abspaths(fileids, True)])
            for (fileid, enc) in self.abspaths(fileids, True)])
            for (fileid, enc) in self.abspaths(fileids, True)])
            for (fileid, enc) in self.abspaths(fileids, True)])
            for (fileid, enc) in self.abspaths(fileids, True)])
    assert isinstance(jeita.tagged_words()[0][1], compat.string_types)
from nltk.compat import xrange, python_2_unicode_compatible, total_ordering
@python_2_unicode_compatible
@python_2_unicode_compatible
        :return: The length of the longest hypernym path from this
        :return: The length of the shortest hypernym path from this
    def hypernym_paths(self):
        Get the path(s) from this synset to the root, where each path is a
        paths = []
            paths = [[self]]
            for ancestor_list in hypernym.hypernym_paths():
                paths.append(ancestor_list)
        return paths
        minimum depth and appear(s) in both paths is/are returned.
            (eg: 'chef.n.01', 'fireman.n.01') but is retained for backwards compatibility
        Get the path(s) from this synset to the root, counting the distance
    def shortest_path_distance(self, other, simulate_root=False):
        Returns the distance of the shortest path linking the two synsets (if
        :param other: The Synset to which the shortest path will be found.
        :return: The number of edges in the shortest path connecting the two
            nodes, or None if no path exists.
        path_distance = None
        # paths to the root) the duplicate with the shortest distance from
        # connecting path length. Return the shortest of these.
                    if path_distance is None or path_distance < 0 or new_distance < path_distance:
                        path_distance = new_distance
        return path_distance
    def path_similarity(self, other, verbose=False, simulate_root=True):
        Path Distance Similarity:
        shortest path that connects the senses in the is-a (hypernym/hypnoym)
        a path cannot be found (will only be true for verbs as there are many
            normally between 0 and 1. None is returned if no connecting path
        distance = self.shortest_path_distance(other, simulate_root=simulate_root and self._needs_root())
        shortest path that connects the senses (as above) and the maximum depth
        -log(p/2d) where p is the shortest path length and d is the taxonomy
            normally greater than 0. None is returned if no connecting path
        distance = self.shortest_path_distance(other, simulate_root=simulate_root and need_root)
        The LCS does not necessarily feature in the shortest path connecting
        whose shortest path to the root node is the longest will be selected.
        Where the LCS has multiple paths to the root, the longer path is used
            normally greater than zero. If no connecting path between the two
        # Get the longest path from the LCS to the root,
        # Get the shortest path from the LCS to each of the synsets it is
        # subsuming.  Add this to the LCS path length to get the path
        len1 = self.shortest_path_distance(subsumer, simulate_root=simulate_root and need_root)
        len2 = other.shortest_path_distance(subsumer, simulate_root=simulate_root and need_root)
    def path_similarity(self, synset1, synset2, verbose=False, simulate_root=True):
        return synset1.path_similarity(synset2, verbose, simulate_root)
    path_similarity.__doc__ = Synset.path_similarity.__doc__
def path_similarity(synset1, synset2, verbose=False, simulate_root=True):
    return synset1.path_similarity(synset2, verbose, simulate_root)
path_similarity.__doc__ = Synset.path_similarity.__doc__
    print(S('dog.n.01').path_similarity(S('cat.n.01')))
from nltk import compat
                       for fileid, enc in self.abspaths(None, True)])
        if isinstance(fileids, compat.string_types):
from nltk import compat
        if isinstance(chunk_types, compat.string_types):
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
                       for (fileid, enc) in self.abspaths(fileids, True)])
                        isinstance(child[0], compat.string_types)):
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
        if len(tree) == 1 and isinstance(tree[0], compat.string_types):
from nltk import compat
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
from nltk import compat
@compat.python_2_unicode_compatible
        if isinstance(fileids, compat.string_types): fileids = [fileids]
# based on PPAttachmentCorpusReader
from nltk import compat
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
                       for (fileid, enc) in self.abspaths(fileids, True)])
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
from nltk.compat import string_types
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])
        return [self._get_corpus(fileid) for fileid in self.abspaths(fileids)]
                            for fileid in self.abspaths(fileids)]
        pat = dictOfDicts()
                pat[participant.get('id')][key] = value
        return pat
                for fileid in self.abspaths(fileids)]
        for pat in xmldoc.findall('.//{%s}Participants/{%s}participant'
                if pat.get('id') == speaker:
                    age = pat.get('age')
                for fileid in self.abspaths(fileids)]
        on the path consisting of <corpus root>+fileid; then if
            path = urlbase+"/"+fileid
                path = re.findall(r'(?i)/childes(?:/data-xml)?/(.*)\.xml', full)[0]
                path = 'Eng-USA/' + re.findall(r'/(?i)Eng-USA/(.*)\.xml', full)[0]
                path = fileid
        if path.endswith('.xml'):
            path = path[:-4]
        if not path.endswith('.cha'):
            path = path+'.cha'
        url = self.childes_url_base + path
            Alternately, you can call the demo with the path to a portion of the CHILDES corpus, e.g.:
        demo('/path/to/childes/data-xml/Eng-USA/")
from nltk import compat
        elif isinstance(vnclass_ids, compat.string_types):
        if isinstance(vnclass, compat.string_types):
        if isinstance(vnclass, compat.string_types):
        if isinstance(vnclass, compat.string_types):
        if isinstance(vnclass, compat.string_types):
from nltk import compat
@compat.python_2_unicode_compatible
class PPAttachment(object):
        return ('PPAttachment(sent=%r, verb=%r, noun1=%r, prep=%r, '
class PPAttachmentCorpusReader(CorpusReader):
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
            return [PPAttachment(*line.split())]
                       for fileid in self.abspaths(fileids)])
        for fileid, encoding in self.abspaths(fileids, include_encoding=True):
            if isinstance(fileid, PathPointer):
                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])
                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])
                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])
                           for fileid, enc in self.abspaths(fileids, include_encoding=True)])
                      for fileid, enc in self.abspaths(fileids, include_encoding=True)])
from nltk import compat
from nltk.compat import total_ordering
        if isinstance(framefiles, compat.string_types):
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return StreamBackedCorpusView(self.abspath(self._propfile),
        return StreamBackedCorpusView(self.abspath(self._propfile),
        etree = ElementTree.parse(self.abspath(framefile).open()).getroot()
            etree = ElementTree.parse(self.abspath(framefile).open()).getroot()
        return StreamBackedCorpusView(self.abspath(self._verbsfile),
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
        if not isinstance(s, compat.string_types):
from nltk.compat import text_type, string_types, python_2_unicode_compatible
@python_2_unicode_compatible
        # @python_2_unicode_compatible decorator (because non-ASCII characters 
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
        for f in XMLCorpusView(self.abspath("frameIndex.xml"),
        for doclist in XMLCorpusView(self.abspath("fulltextIndex.xml"),
        for lu in XMLCorpusView(self.abspath("luIndex.xml"),
        freltypes = PrettyList(x for x in XMLCorpusView(self.abspath("frRelation.xml"),
        # construct the path name for the xml file containing the document info
        locpath = os.path.join(
        elt = XMLCorpusView(locpath, 'fullTextAnnotation')[0]
        # construct the path name for the xml file containing the Frame info
        locpath = os.path.join(
        #print(locpath, file=sys.stderr)
            elt = XMLCorpusView(locpath, 'frame')[0]
    def frames_by_lemma(self, pat):
        ``pat``. Note that LU names are composed of "lemma.POS", where
        return PrettyList(f for f in self.frames() if any(re.search(pat, luName) for luName in f.lexUnit))
        needed. In principle, valence patterns could be loaded here too, 
        locpath = os.path.join("{0}".format(self._root), self._lu_dir, fname)
        #print(locpath, file=sys.stderr)
            elt = XMLCorpusView(locpath, 'lexUnit')[0]
        semtypeXML = [x for x in XMLCorpusView(self.abspath("semTypes.xml"),
        :param name: A regular expression pattern used to match against
        :param name: A regular expression pattern used to search the LU
        :param name: A regular expression pattern used to search the
    # regexp pattern.
from nltk import compat
            >>> root = '/...path to corpus.../'
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to
from nltk import compat
from nltk.data import PathPointer, FileSystemPathPointer, ZipFilePathPointer
@compat.python_2_unicode_compatible
    identified by its ``file identifier``, which is the relative path
        :type root: PathPointer or str
        :param root: A path pointer identifying the root directory for
            converted to a ``PathPointer`` automatically.
            paths.  The absolute path for each file will be constructed
        # Convert the root to a path pointer, if necessary.
        if isinstance(root, compat.string_types) and not isinstance(root, PathPointer):
                root = ZipFilePathPointer(zipfile, zipentry)
                root = FileSystemPathPointer(root)
        elif not isinstance(root, PathPointer):
            raise TypeError('CorpusReader: expected a string or a PathPointer')
        if isinstance(fileids, compat.string_types):
        """A list of the relative paths for the fileids that make up
        if isinstance(self._root, ZipFilePathPointer):
            path = '%s/%s' % (self._root.zipfile.filename, self._root.entry)
            path = '%s' % self._root.path
        return '<%s in %r>' % (self.__class__.__name__, path)
    def abspath(self, fileid):
        Return the absolute path for the given file.
        :param file: The file identifier for the file whose path
        :rtype: PathPointer
    def abspaths(self, fileids=None, include_encoding=False,
        Return a list of the absolute paths for all fileids in this corpus;
        :param fileids: Specifies the set of fileids for which paths should
            value is always a list of paths, even if ``fileids`` is a
            ``(path_pointer, encoding)`` tuples.
        :rtype: list(PathPointer)
        elif isinstance(fileids, compat.string_types):
        paths = [self._root.join(f) for f in fileids]
            return zip(paths, [self.encoding(f) for f in fileids], fileids)
            return zip(paths, fileids)
            return zip(paths, [self.encoding(f) for f in fileids])
            return paths
        :type: PathPointer""")
          - cat_pattern: A regular expression pattern used to find the
            category for each file identifier.  The pattern will be
        self._pattern = None #: regexp specifying the mapping
        if 'cat_pattern' in kwargs:
            self._pattern = kwargs['cat_pattern']
            del kwargs['cat_pattern']
            raise ValueError('Expected keyword argument cat_pattern or '
        if ('cat_pattern' in kwargs or 'cat_map' in kwargs or
            raise ValueError('Specify exactly one of: cat_pattern, '
        if self._pattern is not None:
                category = re.match(self._pattern, file_id).group(1)
        if isinstance(fileids, compat.string_types):
        elif isinstance(categories, compat.string_types):
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
                       for fileid, enc in self.abspaths(fileids, True)])
                       for fileid, enc in self.abspaths(fileids, True)])
                       for fileid, enc in self.abspaths(fileids, True)])
                       for fileid, enc in self.abspaths(fileids, True)])
                       for fileid, enc in self.abspaths(fileids, True)])
from nltk import compat
            >>> root = '/...path to corpus.../'
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
                       for (fileid, enc) in self.abspaths(fileids, True)])
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
    package (or modified ``nltk.data.path`` to point to its location),
from nltk import compat
@compat.python_2_unicode_compatible
        Create an ngram language model to capture patterns in n consecutive
            Note: For backward-compatibility, if no arguments are specified, the
        if (train is not None) and isinstance(train[0], compat.string_types):
from nltk.compat import python_2_unicode_compatible, text_type
@python_2_unicode_compatible
        The text is a list of tokens, and a regexp pattern to match
@python_2_unicode_compatible
        The text is a list of tokens, and a regexp pattern to match
from nltk.compat import StringIO, u
from nltk.data import PathPointer, ZipFilePathPointer, find
        if isinstance(sfm_file, PathPointer):
            #      (PathPointer.open doesn't take a mode option)
        first_line_pat = re.compile(line_regexp % '(?:\xef\xbb\xbf)?')
        line_pat = re.compile(line_regexp % '')
        mobj = re.match(first_line_pat, line)
            mobj = re.match(line_pat, line)
        unwrap_pat = re.compile(r'\n+')
                val = unwrap_pat.sub(' ', val)
#    zip_path = find('corpora/toolbox.zip')
#    lexicon = ToolboxData(ZipFilePathPointer(zip_path, 'toolbox/rotokas.dic')).parse()
    file_path = find('corpora/toolbox/rotokas.dic')
    lexicon = ToolboxData(file_path).parse()
    file_path = find('corpora/toolbox/MDF/MDF_AltH.typ')
    settings.open(file_path)
#    settings.open(ZipFilePathPointer(zip_path, entry='toolbox/MDF/MDF_AltH.typ'))
from nltk import compat
@compat.python_2_unicode_compatible
        pylab.xticks(range(len(samples)), [compat.text_type(s) for s in samples], rotation=90)
            sample_iter = compat.iteritems(samples)
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
        :param bins: Included for compatibility with nltk.tag.hmm
@compat.python_2_unicode_compatible
        return sum(fdist.N() for fdist in compat.itervalues(self))
        pylab.xticks(range(len(samples)), [compat.text_type(s) for s in samples], rotation=90)
@compat.python_2_unicode_compatible
        #if there's nothing left in the agenda, and we haven't closed the path
        # Since 'current' is of type '~(a=b)', the path is closed if 'a' == 'b'
        #if there are accessible_vars on the path
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
        anything on the path.  The second is same thing for the 'other' Clause.
        newclauses = _iterate_first(self, other, bindings, used, skipped, _complete_unify_path, debug)
def _complete_unify_path(first, second, bindings, used, skipped, debug):
    if used[0] or used[1]: #if bindings were made along the path
@python_2_unicode_compatible
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
                                  path_to_bin=binary_location,
            self._binary_location = self._prover9_bin.rsplit(os.path.sep, 1)
            searchpath=binary_locations,
    p._executable_path = None
from nltk.data import ZipFilePathPointer
        :param bin: The full path to the ``malt`` binary.  If not
        _malt_path = ['.',
        # Expand wildcards in _malt_path:
        malt_path = reduce(add, map(glob.glob, _malt_path))
            searchpath=malt_path, env_vars=['MALTPARSERHOME'],
        # If conll_file is a ZipFilePathPointer, then we need to do some extra
        if isinstance(conll_file, ZipFilePathPointer):
from nltk.compat import total_ordering, python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import unicode_repr
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
                    path = self.get_cycle_path(self.get_by_address(pair[0]), pair[0]) #self.nodelist[pair[0]], pair[0])
                    return path
    def get_cycle_path(self, curr_node, goal_node_index):
            path = self.get_cycle_path(self.get_by_address(dep), goal_node_index)#self.nodelist[dep], goal_node_index)
            if len(path) > 0:
                path.insert(0, curr_node['address'])
                return path
from nltk.compat import xrange, python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import unicode_repr
    path from the root of the tree to a subtree or a leaf; see the
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import xrange
    def collapse_nodes(self, new_node, cycle_path, g_graph, b_graph, c_graph):
        :type cycle_path: A list of integers.
        :param cycle_path: A list of node addresses, each of which is in the cycle.
        for cycle_node_index in cycle_path:
        g_graph.redirect_arcs(cycle_path, new_node['address'])
    def update_edge_scores(self, new_node, cycle_path):
        :type cycle_path: A list of integers.
        :param cycle_path: A list of node addresses that belong to the cycle.
        print('cycle', cycle_path)
        cycle_path = self.compute_original_indexes(cycle_path)
        print('old cycle ', cycle_path)
                if j in cycle_path and not i in cycle_path and len(self.scores[i][j]) > 0:
                    subtract_val = self.compute_max_subtract_score(j, cycle_path)
                if i in cycle_path and j in cycle_path:
            cycle_path = b_graph.contains_cycle()
            if cycle_path:
                self.update_edge_scores(new_node, cycle_path)
                self.collapse_nodes(new_node, cycle_path, g_graph, b_graph, c_graph)
                for cycle_index in cycle_path:
                self.inner_nodes[new_node['address']] = cycle_path
                for cycle_node_address in cycle_path:
        traversals.  All possible paths through the lattice are then enumerated
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
#         # Find the best path from S to each nonterminal
from nltk.compat import xrange
from nltk import compat
from nltk.compat import (total_ordering, python_2_unicode_compatible,
@python_2_unicode_compatible
@python_2_unicode_compatible
                    not isinstance(child_choice, compat.string_types):
@python_2_unicode_compatible
            for index in compat.xrange(chart.num_leaves() + 1):
from nltk import compat
@compat.python_2_unicode_compatible
        ext = os.path.splitext(url.split('/')[-1])[1]
        self.filename = os.path.join(subdir, id+ext)
        if isinstance(xml, compat.string_types):
            xml.attrib[key] = compat.text_type(xml.attrib[key])
@compat.python_2_unicode_compatible
        if isinstance(xml, compat.string_types):
            xml.attrib[key] = compat.text_type(xml.attrib[key])
                    user_input = compat.raw_input("Hit Enter to continue: ")
        if isinstance(info_or_id, compat.string_types):
        filepath = os.path.join(download_dir, info.filename)
        if os.path.exists(filepath):
            os.remove(filepath)
        if not os.path.exists(download_dir):
        if not os.path.exists(os.path.join(download_dir, info.subdir)):
            os.mkdir(os.path.join(download_dir, info.subdir))
            infile = compat.urlopen(info.url)
            with open(filepath, 'wb') as outfile:
            zipdir = os.path.join(download_dir, info.subdir)
            if info.unzip or os.path.exists(os.path.join(zipdir, info.id)):
                for msg in _unzip_iter(filepath, zipdir, verbose=False):
                        choice = compat.raw_input().strip()
            filepath = os.path.join(download_dir, info.filename)
                status = self._pkg_status(info, filepath)
                                                                   filepath)
    def _pkg_status(self, info, filepath):
        if not os.path.exists(filepath):
        try: filestat = os.stat(filepath)
        if md5_hexdigest(filepath) != info.checksum:
        if filepath.endswith('.zip'):
            unzipdir = filepath[:-4]
            if not os.path.exists(unzipdir):
            if not os.path.isdir(unzipdir):
            unzipped_size = sum(os.stat(os.path.join(d, f)).st_size
            ElementTree.parse(compat.urlopen(self._url)).getroot())
        for nltkdir in nltk.data.path:
            if (os.path.exists(nltkdir) and
            homedir = os.path.expanduser('~/')
        return os.path.join(homedir, 'nltk_data')
            user_input = compat.raw_input('Downloader> ').strip()
            except compat.HTTPError as e:
            except compat.URLError as e:
                user_input = compat.raw_input('  Identifier> ')
                user_input = compat.raw_input('  Identifier> ')
            user_input = compat.raw_input('Config> ').strip().lower()
                new_dl_dir = compat.raw_input('  New Directory> ').strip().lower()
                elif os.path.isdir(new_dl_dir):
                new_url = compat.raw_input('  New URL> ').strip().lower()
        except compat.HTTPError as e:
        except compat.URLError as e:
        except compat.HTTPError as e:
        except compat.URLError as e:
            if isinstance(val, compat.string_types): return '  %s' % val
        except compat.HTTPError as e:
        except compat.URLError as e:
                except compat.HTTPError as e:
                except compat.URLError as e:
                except compat.HTTPError as e:
                except compat.URLError as e:
        except compat.HTTPError as e:
        except compat.URLError as e:
    if isinstance(file, compat.string_types):
        sys.stdout.write('Unzipping %s' % os.path.split(filename)[1])
    if not os.path.exists(root):
            dirpath = os.path.join(root, *pieces[:i+1])
            if not os.path.exists(dirpath):
                os.mkdir(dirpath)
        filepath = os.path.join(root, *filename.split('/'))
        with open(filepath, 'wb') as out:
    path to a directory containing the package xml and zip files; and
    for pkg_xml, zf, subdir in _find_packages(os.path.join(root, 'packages')):
        url = '%s/%s/%s' % (base_url, subdir, os.path.split(zf.filename)[1])
    collections = list(_find_collections(os.path.join(root, 'collections')))
    # The filename must patch the id given in the XML file.
    uid = os.path.splitext(os.path.split(zipfilename)[1])[0]
                         (os.path.split(filename)[1], textwrap.fill(stderr)))
                xmlfile = os.path.join(dirname, filename)
    from nltk.corpus.reader.util import _path_from
        relpath = '/'.join(_path_from(root, dirname))
                xmlfilename = os.path.join(dirname, filename)
                uid = os.path.split(xmlfilename[:-4])[1]
                yield pkg_xml, zf, relpath
from nltk import compat
from nltk.compat import (class_types, text_type, string_types, total_ordering,
                         python_2_unicode_compatible)
@python_2_unicode_compatible
    from nltk import compat
            proxy = compat.getproxies()['http']
    proxy_handler = compat.ProxyHandler({'http': proxy})
    opener = compat.build_opener(proxy_handler)
        password_manager = compat.HTTPPasswordMgrWithDefaultRealm()
        opener.add_handler(compat.ProxyBasicAuthHandler(password_manager))
        opener.add_handler(compat.ProxyDigestAuthHandler(password_manager))
    compat.install_opener(opener)
A class for simple chatbots.  These perform simple pattern matching on sentences
from nltk import compat
        Initialize the chatbot.  Pairs is a list of patterns and responses.  Each
        pattern is a regular expression matching the user's statement or question,
        e.g. r'I like (.*)'.  For each such pattern a list of possible responses
        which is matched by parenthesized sections of the patterns (e.g. .*) is mapped to
        :param pairs: The patterns and responses
        # check each pattern
        for (pattern, response) in self._pairs:
            match = pattern.match(str)
            # did the pattern match?
            try: input = compat.raw_input(">")
    ( "The path to enlightenment is often difficult to see.",
    ( "Desires of the heart will distract you from the path to enlightenment.",
    ( "My path is not of conern to you.",
    ( "Farewell. The obstacle is the path.",
Features can be specified using "feature paths", or tuples of feature
identifiers that specify path through the nested feature structures to
accessed via multiple feature paths.  Unification preserves the
of its feature paths.
from nltk.compat import (string_types, integer_types, total_ordering,
                         python_2_unicode_compatible, unicode_repr)
    identifiers or 'feature paths.'  A feature path is a sequence
    object that can be accessed via multiple feature paths.  Feature
    if there is any feature path from the feature structure to itself.
        ``self[p]==other[p]`` for every feature path *p* such
@python_2_unicode_compatible
    :see: ``FeatStruct`` for information about feature paths, reentrance,
    _INDEX_ERROR = str("Expected feature name or path.  Got %r.")
    def __getitem__(self, name_or_path):
        """If the feature with the given name or path exists, return
        if isinstance(name_or_path, (string_types, Feature)):
            return dict.__getitem__(self, name_or_path)
        elif isinstance(name_or_path, tuple):
                for fid in name_or_path:
                        raise KeyError # path contains base value
                raise KeyError(name_or_path)
            raise TypeError(self._INDEX_ERROR % name_or_path)
    def get(self, name_or_path, default=None):
        """If the feature with the given name or path exists, return its
        try: return self[name_or_path]
    def __contains__(self, name_or_path):
        """Return true if a feature with the given name or path exists."""
        try: self[name_or_path]; return True
    def has_key(self, name_or_path):
        """Return true if a feature with the given name or path exists."""
        return name_or_path in self
    def __delitem__(self, name_or_path):
        """If the feature with the given name or path exists, delete
        if isinstance(name_or_path, (string_types, Feature)):
            return dict.__delitem__(self, name_or_path)
        elif isinstance(name_or_path, tuple):
            if len(name_or_path) == 0:
                raise ValueError("The path () can not be set")
                parent = self[name_or_path[:-1]]
                    raise KeyError(name_or_path) # path contains base value
                del parent[name_or_path[-1]]
            raise TypeError(self._INDEX_ERROR % name_or_path)
    def __setitem__(self, name_or_path, value):
        """Set the value for the feature with the given name or path
        to ``value``.  If ``name_or_path`` is an invalid path, raise
        if isinstance(name_or_path, (string_types, Feature)):
            return dict.__setitem__(self, name_or_path, value)
        elif isinstance(name_or_path, tuple):
            if len(name_or_path) == 0:
                raise ValueError("The path () can not be set")
                parent = self[name_or_path[:-1]]
                    raise KeyError(name_or_path) # path contains base value
                parent[name_or_path[-1]] = value
            raise TypeError(self._INDEX_ERROR % name_or_path)
    multiple feature paths.  Feature lists may also be cyclic.
    :see: ``FeatStruct`` for information about feature paths, reentrance,
    _INDEX_ERROR = "Expected int or feature path.  Got %r."
    def __getitem__(self, name_or_path):
        if isinstance(name_or_path, integer_types):
            return list.__getitem__(self, name_or_path)
        elif isinstance(name_or_path, tuple):
                for fid in name_or_path:
                        raise KeyError # path contains base value
                raise KeyError(name_or_path)
            raise TypeError(self._INDEX_ERROR % name_or_path)
    def __delitem__(self, name_or_path):
        """If the feature with the given name or path exists, delete
        if isinstance(name_or_path, (integer_types, slice)):
            return list.__delitem__(self, name_or_path)
        elif isinstance(name_or_path, tuple):
            if len(name_or_path) == 0:
                raise ValueError("The path () can not be set")
                parent = self[name_or_path[:-1]]
                    raise KeyError(name_or_path) # path contains base value
                del parent[name_or_path[-1]]
            raise TypeError(self._INDEX_ERROR % name_or_path)
    def __setitem__(self, name_or_path, value):
        """Set the value for the feature with the given name or path
        to ``value``.  If ``name_or_path`` is an invalid path, raise
        if isinstance(name_or_path, (integer_types, slice)):
            return list.__setitem__(self, name_or_path, value)
        elif isinstance(name_or_path, tuple):
            if len(name_or_path) == 0:
                raise ValueError("The path () can not be set")
                parent = self[name_or_path[:-1]]
                    raise KeyError(name_or_path) # path contains base value
                parent[name_or_path[-1]] = value
            raise TypeError(self._INDEX_ERROR % name_or_path)
@python_2_unicode_compatible
    ``fstruct2`` specify incompatible values for some feature), then
                         trace, fail, fs_class, path):
    :param path: The feature path that led us to this unification
        if trace: _trace_unify_identity(path, fstruct1)
                    forward, trace, fail, fs_class, path+(fname,))
                forward, trace, fail, fs_class, path+(findex,))
                          trace, fail, fs_class, fpath):
    if trace: _trace_unify_start(fpath, fval1, fval2)
                                      trace, fail, fs_class, fpath)
        if fail is not None: result = fail(fval1, fval2, fpath)
        if trace: _trace_unify_fail(fpath[:-1], result)
    if trace: _trace_unify_succeed(fpath, result)
        _trace_bindings(fpath, bindings)
def _trace_unify_start(path, fval1, fval2):
    if path == ():
        fullname = '.'.join("%s" % n for n in path)
        print('  '+'|   '*(len(path)-1)+'|')
        print('  '+'|   '*(len(path)-1)+'| Unify feature: %s' % fullname)
    print('  '+'|   '*len(path)+' / '+_trace_valrepr(fval1))
    print('  '+'|   '*len(path)+'|\\ '+_trace_valrepr(fval2))
def _trace_unify_identity(path, fval1):
    print('  '+'|   '*len(path)+'|')
    print('  '+'|   '*len(path)+'| (identical objects)')
    print('  '+'|   '*len(path)+'|')
    print('  '+'|   '*len(path)+'+-->'+unicode_repr(fval1))
def _trace_unify_fail(path, result):
    print('  '+'|   '*len(path)+'|   |')
    print('  '+'X   '*len(path)+'X   X <-- FAIL'+resume)
def _trace_unify_succeed(path, fval1):
    print('  '+'|   '*len(path)+'|')
    print('  '+'|   '*len(path)+'+-->'+unicode_repr(fval1))
def _trace_bindings(path, bindings):
        print('  '+'|   '*len(path)+'    Bindings: '+bindstr)
    Return a list of the feature paths of all features which are
    assigned incompatible values by ``fstruct1`` and ``fstruct2``.
    def add_conflict(fval1, fval2, path):
        conflict_list.append(path)
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
        _BARE_PREFIX_RE.pattern, _START_FSTRUCT_RE.pattern,
        _FEATURE_NAME_RE.pattern, _FEATURE_NAME_RE.pattern))
        """Mainly included for backwards compat."""
Tag Patterns
expression patterns, called "tag patterns".  Tag patterns are
used to match sequences of tags.  Examples of tag patterns are::
The differences between regular expression patterns and tag
patterns are:
    - In tag patterns, ``'<'`` and ``'>'`` act as parentheses; so
    - Whitespace in tag patterns is ignored.  So
    - In tag patterns, ``'.'`` is equivalant to ``'[^{}<>]'``; so
The function ``tag_pattern2re_pattern`` can be used to transform
a tag pattern to an equivalent regular expression pattern.
:type CHUNK_TAG_PATTERN: regexp
:var CHUNK_TAG_PATTERN: A regular expression to test whether a tag
     pattern is valid.
                    for sent in load_ace_file(os.path.join(root, f), fmt):
    print('  - %s' % os.path.split(textfile)[1])
    train_paths = [find('corpora/ace_data/ace.dev'),
    train_trees = load_ace_data(train_paths, fmt)
    eval_paths = [find('corpora/ace_data/ace.eval')]
    eval_trees = load_ace_data(eval_paths, fmt)
from nltk.compat import python_2_unicode_compatible
# Patched for increased performance by Yoav Goldberg <yoavg@cs.bgu.ac.il>, 2006-01-13
patient NN I-NP
from nltk.compat import python_2_unicode_compatible, string_types, unicode_repr
@python_2_unicode_compatible
    :cvar IN_CHUNK_PATTERN: A zero-width regexp pattern string that
    :cvar IN_CHINK_PATTERN: A zero-width regexp pattern string that
    IN_CHUNK_PATTERN = r'(?=[^\{]*\})'
    IN_CHINK_PATTERN = r'(?=[^\}]*(\{|$))'
@python_2_unicode_compatible
            normal regular expression, not a tag pattern.
        return ('<RegexpChunkRule: '+unicode_repr(self._regexp.pattern)+
        # Pattern bodies: chunk, chink, split, merge
                raise ValueError('Empty chunk pattern')
                raise ValueError('Illegal chunk pattern: %s' % rule)
            raise ValueError('Illegal chunk pattern: %s' % rule)
@python_2_unicode_compatible
    matching tag pattern.  When applied to a ``ChunkString``, it will
    find any substring that matches this tag pattern and that is not
    def __init__(self, tag_pattern, descr):
        :type tag_pattern: str
        :param tag_pattern: This rule's tag pattern.  When
            chunk any substring that matches this tag pattern and that
        self._pattern = tag_pattern
                            (tag_pattern2re_pattern(tag_pattern),
                             ChunkString.IN_CHINK_PATTERN))
        return '<ChunkRule: '+unicode_repr(self._pattern)+'>'
@python_2_unicode_compatible
    using a matching tag pattern.  When applied to a
    tag pattern and that is contained in a chunk, and remove it
    def __init__(self, tag_pattern, descr):
        :type tag_pattern: str
        :param tag_pattern: This rule's tag pattern.  When
            find any substring that matches this tag pattern and that
        self._pattern = tag_pattern
                            (tag_pattern2re_pattern(tag_pattern),
                             ChunkString.IN_CHUNK_PATTERN))
        return '<ChinkRule: '+unicode_repr(self._pattern)+'>'
@python_2_unicode_compatible
    using a matching tag pattern.  When applied to a
    tag pattern, and un-chunk it.
    def __init__(self, tag_pattern, descr):
        :type tag_pattern: str
        :param tag_pattern: This rule's tag pattern.  When
            find any complete chunk that matches this tag pattern,
        self._pattern = tag_pattern
                            tag_pattern2re_pattern(tag_pattern))
        return '<UnChunkRule: '+unicode_repr(self._pattern)+'>'
@python_2_unicode_compatible
    two matching tag patterns: a left pattern, and a right pattern.
    matches left pattern, and immediately followed by a chunk whose
    beginning matches right pattern.  It will then merge those two
    def __init__(self, left_tag_pattern, right_tag_pattern, descr):
        :type right_tag_pattern: str
        :param right_tag_pattern: This rule's right tag
            pattern.  When applied to a ``ChunkString``, this
            ``left_tag_pattern``, and immediately followed by a chunk
            whose beginning matches this pattern.  It will
        :type left_tag_pattern: str
        :param left_tag_pattern: This rule's left tag
            pattern.  When applied to a ``ChunkString``, this
            this pattern, and immediately followed by a chunk
            whose beginning matches ``right_tag_pattern``.  It will
        # Ensure that the individual patterns are coherent.  E.g., if
        re.compile(tag_pattern2re_pattern(left_tag_pattern))
        re.compile(tag_pattern2re_pattern(right_tag_pattern))
        self._left_tag_pattern = left_tag_pattern
        self._right_tag_pattern = right_tag_pattern
                            (tag_pattern2re_pattern(left_tag_pattern),
                             tag_pattern2re_pattern(right_tag_pattern)))
        return ('<MergeRule: '+unicode_repr(self._left_tag_pattern)+', '+
                unicode_repr(self._right_tag_pattern)+'>')
@python_2_unicode_compatible
    two matching tag patterns: a left pattern, and a right pattern.
    matches the left pattern followed by the right pattern.  It will
    two pattern matches.
    def __init__(self, left_tag_pattern, right_tag_pattern, descr):
        :type right_tag_pattern: str
        :param right_tag_pattern: This rule's right tag
            pattern.  When applied to a ``ChunkString``, this rule will
            ``left_tag_pattern`` followed by this pattern.  It will
            between these two matching patterns.
        :type left_tag_pattern: str
        :param left_tag_pattern: This rule's left tag
            pattern.  When applied to a ``ChunkString``, this rule will
            pattern followed by ``right_tag_pattern``.  It will then
            these two matching patterns.
        # Ensure that the individual patterns are coherent.  E.g., if
        re.compile(tag_pattern2re_pattern(left_tag_pattern))
        re.compile(tag_pattern2re_pattern(right_tag_pattern))
        self._left_tag_pattern = left_tag_pattern
        self._right_tag_pattern = right_tag_pattern
                            (tag_pattern2re_pattern(left_tag_pattern),
                             tag_pattern2re_pattern(right_tag_pattern)))
        return ('<SplitRule: '+unicode_repr(self._left_tag_pattern)+', '+
                unicode_repr(self._right_tag_pattern)+'>')
@python_2_unicode_compatible
    using two matching tag patterns: a left pattern, and a right pattern.
    matches right pattern, and immediately preceded by a chink whose
    end matches left pattern.  It will then expand the chunk to incorporate
    def __init__(self, left_tag_pattern, right_tag_pattern, descr):
        :type right_tag_pattern: str
        :param right_tag_pattern: This rule's right tag
            pattern.  When applied to a ``ChunkString``, this
            ``right_tag_pattern``, and immediately preceded by a chink
            whose end matches this pattern.  It will
        :type left_tag_pattern: str
        :param left_tag_pattern: This rule's left tag
            pattern.  When applied to a ``ChunkString``, this
            this pattern, and immediately preceded by a chink
            whose end matches ``left_tag_pattern``.  It will
        # Ensure that the individual patterns are coherent.  E.g., if
        re.compile(tag_pattern2re_pattern(left_tag_pattern))
        re.compile(tag_pattern2re_pattern(right_tag_pattern))
        self._left_tag_pattern = left_tag_pattern
        self._right_tag_pattern = right_tag_pattern
                            (tag_pattern2re_pattern(left_tag_pattern),
                             tag_pattern2re_pattern(right_tag_pattern)))
        return ('<ExpandLeftRule: '+unicode_repr(self._left_tag_pattern)+', '+
                unicode_repr(self._right_tag_pattern)+'>')
@python_2_unicode_compatible
    right, using two matching tag patterns: a left pattern, and a
    right pattern.  When applied to a ``ChunkString``, it will find any
    chunk whose end matches left pattern, and immediately followed by
    a chink whose beginning matches right pattern.  It will then
    def __init__(self, left_tag_pattern, right_tag_pattern, descr):
        :type right_tag_pattern: str
        :param right_tag_pattern: This rule's right tag
            pattern.  When applied to a ``ChunkString``, this
            ``left_tag_pattern``, and immediately followed by a chink
            whose beginning matches this pattern.  It will
        :type left_tag_pattern: str
        :param left_tag_pattern: This rule's left tag
            pattern.  When applied to a ``ChunkString``, this
            this pattern, and immediately followed by a chink
            whose beginning matches ``right_tag_pattern``.  It will
        # Ensure that the individual patterns are coherent.  E.g., if
        re.compile(tag_pattern2re_pattern(left_tag_pattern))
        re.compile(tag_pattern2re_pattern(right_tag_pattern))
        self._left_tag_pattern = left_tag_pattern
        self._right_tag_pattern = right_tag_pattern
                            (tag_pattern2re_pattern(left_tag_pattern),
                             tag_pattern2re_pattern(right_tag_pattern)))
        return ('<ExpandRightRule: '+unicode_repr(self._left_tag_pattern)+', '+
                unicode_repr(self._right_tag_pattern)+'>')
@python_2_unicode_compatible
    three matching tag patterns: one for the left context, one for the
    tag pattern, is surrounded by substrings that match the two
    context patterns, and is not already part of a chunk; and create a
    pattern.
    def __init__(self, left_context_tag_pattern, chunk_tag_pattern,
                 right_context_tag_pattern, descr):
        :type left_context_tag_pattern: str
        :param left_context_tag_pattern: A tag pattern that must match
            the left context of ``chunk_tag_pattern`` for this rule to
        :type chunk_tag_pattern: str
        :param chunk_tag_pattern: A tag pattern that must match for this
            rule to apply.  If the rule does apply, then this pattern
        :type right_context_tag_pattern: str
        :param right_context_tag_pattern: A tag pattern that must match
            the right context of ``chunk_tag_pattern`` for this rule to
        # Ensure that the individual patterns are coherent.  E.g., if
        re.compile(tag_pattern2re_pattern(left_context_tag_pattern))
        re.compile(tag_pattern2re_pattern(chunk_tag_pattern))
        re.compile(tag_pattern2re_pattern(right_context_tag_pattern))
        self._left_context_tag_pattern = left_context_tag_pattern
        self._chunk_tag_pattern = chunk_tag_pattern
        self._right_context_tag_pattern = right_context_tag_pattern
                            (tag_pattern2re_pattern(left_context_tag_pattern),
                             tag_pattern2re_pattern(chunk_tag_pattern),
                             tag_pattern2re_pattern(right_context_tag_pattern),
                             ChunkString.IN_CHINK_PATTERN))
            self._left_context_tag_pattern, self._chunk_tag_pattern,
            self._right_context_tag_pattern)
##  Tag Pattern Format Conversion
CHUNK_TAG_PATTERN = re.compile(r'^((%s|<%s>)*)$' %
def tag_pattern2re_pattern(tag_pattern):
    Convert a tag pattern to a regular expression pattern.  A "tag
    pattern" is a modified version of a regular expression, designed
    expression patterns and tag patterns are:
        - In tag patterns, ``'<'`` and ``'>'`` act as parentheses; so
        - Whitespace in tag patterns is ignored.  So
        - In tag patterns, ``'.'`` is equivalant to ``'[^{}<>]'``; so
    In particular, ``tag_pattern2re_pattern`` performs the following
    transformations on the given pattern:
        - Check to make sure the resulting pattern is valid.
    :type tag_pattern: str
    :param tag_pattern: The tag pattern to convert to a regular
        expression pattern.
    :raise ValueError: If ``tag_pattern`` is not a valid tag pattern.
        In particular, ``tag_pattern`` should not include braces; and it
    :return: A regular expression pattern corresponding to
        ``tag_pattern``.
    tag_pattern = re.sub(r'\s', '', tag_pattern)
    tag_pattern = re.sub(r'<', '(<(', tag_pattern)
    tag_pattern = re.sub(r'>', ')>)', tag_pattern)
    if not CHUNK_TAG_PATTERN.match(tag_pattern):
        raise ValueError('Bad tag pattern: %r' % tag_pattern)
    # confuse CHUNK_TAG_PATTERN.
    # the pattern backwards (with lookahead assertions).  This can be
    reversed = reverse_str(tag_pattern)
    tag_pattern = reverse_str(reversed)
    return tag_pattern
@python_2_unicode_compatible
@python_2_unicode_compatible
    regular expression patterns to specify the behavior of the parser.
    The patterns of a clause are executed in order.  An earlier
    pattern may introduce a chunk boundary that prevents a later
    pattern from executing.  Sometimes an individual pattern will
    each time the corresponding pattern is applied.
        and set of chunk patterns.
        :param loop: The number of times to run through the patterns
from nltk.compat import string_types, python_2_unicode_compatible, unicode_repr
@python_2_unicode_compatible
              node_pattern=None, leaf_pattern=None,
            default, use the ``node_pattern`` and ``leaf_pattern``
        :type node_pattern: str
        :type leaf_pattern: str
        :param node_pattern, leaf_pattern: Regular expression patterns
            default, both nodes patterns are defined to match any
        open_pattern, close_pattern = (re.escape(open_b), re.escape(close_b))
        if node_pattern is None:
            node_pattern = '[^\s%s%s]+' % (open_pattern, close_pattern)
        if leaf_pattern is None:
            leaf_pattern = '[^\s%s%s]+' % (open_pattern, close_pattern)
            open_pattern, node_pattern, close_pattern, leaf_pattern))
        Returns a representation of the tree compatible with the
        tracing all possible parent paths until trees with no parents
@python_2_unicode_compatible
    # We have to patch up these methods to make them work right:
@python_2_unicode_compatible
    # We have to patch up these methods to make them work right:
from nltk import compat
@compat.python_2_unicode_compatible
@compat.python_2_unicode_compatible
        if isinstance(string_or_pairs, compat.string_types):
# Natural Language Toolkit: Compatibility
# Python 2/3 compatibility layer. Based on six.
            self.__path__ = ["nltk_py2_tkinter_package_path"]
        def find_module(self, name, path=None):
    sys.meta_path = [TkinterLoader()]
# ======= Compatibility for datasets that care about Python versions ========
            path = args[1]
                if item in str(path):
                    pos = path.index(item) + len(item)
                    if path[pos:pos+4] == ".zip":
                    path = path[:pos] + "/PY3" + path[pos:]
                    args = (args[0], path) + args[2:]
# ======= Compatibility layer for __str__ and __repr__ ==========
def python_2_unicode_compatible(klass):
    For classes that was fixed with @python_2_unicode_compatible
    if hasattr(method, "_nltk_compat_7bit"):
        wrapper._nltk_compat_7bit = method._nltk_compat_7bit
    wrapper._nltk_compat_transliterated = True
    if hasattr(method, "_nltk_compat_transliterated"):
        wrapper._nltk_compat_transliterated = method._nltk_compat_transliterated
    wrapper._nltk_compat_7bit = True
    return (getattr(method, "_nltk_compat_7bit", False) or
            getattr(method, "_nltk_compat_transliterated", False))
stylistic patterns that Chomsky is noted for.
from nltk.compat import izip
  - ``file:path``: Specifies the file whose path is *path*.
    Both relative and absolute paths may be used.
  - ``http://host/path``: Specifies the file stored on the web
    server *host* at path *path*.
  - ``nltk:path``: Specifies the file stored in the NLTK data
    package at *path*.  NLTK will search for these files in the
    directories specified by ``nltk.data.path``.
from nltk import compat
from nltk.compat import py3_data
# Search Path
path = []
path += [d for d in os.environ.get('NLTK_DATA', str('')).split(os.pathsep) if d]
if os.path.expanduser('~/') != '~/':
    path.append(os.path.expanduser(str('~/nltk_data')))
    path += [
        os.path.join(sys.prefix, str('nltk_data')),
        os.path.join(sys.prefix, str('lib'), str('nltk_data')),
        os.path.join(os.environ.get(str('APPDATA'), str('C:\\')), str('nltk_data'))
    path += [
    Splits a resource url into "<protocol>:<path>".
    protocol, path = resource_url.split(':', 1)
        path = path.lstrip('/')
            path = '/' + path
        path = re.sub(r'^/{0,2}', '', path)
    return protocol, path
    # use file protocol if the path is an absolute path
    if protocol == 'nltk' and os.path.isabs(name):
        Resource names are posix-style relative path names, such as
        be converted to a platform-appropriate path separator.
    is_dir = bool(re.search(r'[\\/]$',resource_name)) or resource_name.endswith(os.path.sep)
    resource_name = os.path.normpath(resource_name).replace('\\','/').replace(os.path.sep,'/')
# Path Pointers
class PathPointer(object):
    An abstract base class for 'path pointers,' used by NLTK's data
    package to identify specific paths.  Two subclasses exist:
    ``FileSystemPathPointer`` identifies a file that can be accessed
    directly via a given absolute path.  ``ZipFilePathPointer``
        the contents of the file identified by this path pointer.
        :raise IOError: If the path specified by this pointer does
        Return the size of the file pointed to by this path pointer,
        :raise IOError: If the path specified by this pointer does
        Return a new path pointer formed by starting at the path
        path given by ``fileid``.  The path components of ``fileid``
        the underlying file system's path seperator character.
class FileSystemPathPointer(PathPointer,compat.text_type):
    A path pointer that identifies a file which can be accessed
    directly via a given absolute path.
    def __init__(self, _path):
        Create a new path pointer for the given absolute path.
        :raise IOError: If the given path does not exist.
        _path = os.path.abspath(_path)
        if not os.path.exists(_path):
            raise IOError('No such file or directory: %r' % _path)
        self._path = _path
    def path(self):
        """The absolute path identified by this path pointer."""
        return self._path
        stream = open(self._path, 'rb')
        return os.stat(self._path).st_size
        _path = os.path.join(self._path, fileid)
        return FileSystemPathPointer(_path)
        # @python_2_unicode_compatible is not used.
        return str('FileSystemPathPointer(%r)' % self._path)
        return self._path
        :param filename: a filesystem path
        self._buffer = compat.BytesIO()
        self._buffer = compat.BytesIO()
            contents = compat.BytesIO()
class GzipFileSystemPathPointer(FileSystemPathPointer):
    A subclass of ``FileSystemPathPointer`` that identifies a gzip-compressed
    file located at a given absolute path.  ``GzipFileSystemPathPointer`` is
        stream = BufferedGzipFile(self._path, 'rb')
class ZipFilePathPointer(PathPointer):
    A path pointer that identifies a file contained within a zipfile,
        Create a new path pointer pointing at the specified entry
        if isinstance(zipfile, compat.string_types):
            zipfile = OpenOnDemandZipFile(os.path.abspath(zipfile))
        containing the entry identified by this path pointer.
        The name of the file within zipfile that this path
        stream = compat.BytesIO(data)
        return ZipFilePathPointer(self._zipfile, entry)
        return str('ZipFilePathPointer(%r, %r)') % (
        return os.path.normpath(os.path.join(self._zipfile.filename, self._entry))
def find(resource_name, paths=None):
    zip files in paths, where a None or empty string specifies an absolute path.
    Returns a corresponding path name.  If the given resource is not
        remaining path components are used to look inside the zipfile.
      - If any element of ``nltk.data.path`` has a ``.zip`` extension,
        component *p* in the path with *p.zip/p*.  For example, this
        ``corpora/chat80/cities.pl`` to a zip file path pointer to
        Resource names are posix-style relative path names, such as
        automatically converted to a platform-appropriate path separator.
    # Resolve default paths at runtime in-case the user overrides nltk.data.path
    if paths is None:
        paths=path
    # Check each item in our path
    for _path in paths:
        # Is the path item a zipfile?
        if _path and (os.path.isfile(_path) and _path.endswith('.zip')):
                return ZipFilePathPointer(_path, resource_name)
        # Is the path item a directory or is resource_name an absolute path?
        elif not _path or os.path.isdir(_path):
                p = os.path.join(_path, resource_name)
                if os.path.exists(p):
                        return GzipFileSystemPathPointer(p)
                        return FileSystemPathPointer(p)
                p = os.path.join(_path, zipfile)
                if os.path.exists(p):
                        return ZipFilePathPointer(p, zipentry)
    # Fallback: if the path doesn't include a zip file, then try
    # again, assuming that one of the path components is inside a
                return find(modified_name, paths)
    msg += '\n  Searched in:' + ''.join('\n    - %r' % d for d in paths)
            filename = os.path.split(resource_url)[-1]
    if os.path.exists(filename):
        filename = os.path.abspath(filename)
    its path, and open it with the given mode; if the resource URL
    protocol, _path = split_resource_url(resource_url)
        return find(_path, path + ['']).open()
        return find(_path, ['']).open()
        return compat.urlopen(resource_url)
# We shouldn't apply @python_2_unicode_compatible
    def __init__(self, _path):
        self._path = _path
        resource = load(self._path)
        if not isinstance(filename, compat.string_types):
__all__ = ['path', 'PathPointer', 'FileSystemPathPointer', 'BufferedGzipFile',
           'GzipFileSystemPathPointer', 'GzipFileSystemPathPointer',
           'GzipFileSystemPathPointer', 'SeekableUnicodeStreamReader']
import os.path
from nltk import compat
def convert_regexp_to_nongrouping(pattern):
    Convert all grouping parentheses in the given regexp pattern to
    :type pattern: str
    for s in re.findall(r'\\.|\(\?P=', pattern):
                             'are not supported: %r' % pattern)
        ''', subfunc, pattern)
# [xx] add classpath option to config_java?
    :param bin: The full path to the Java binary.  If not specified,
        if isinstance(options, compat.string_types):
def java(cmd, classpath=None, stdin=None, stdout=None, stderr=None,
    :param classpath: A ``':'`` separated list of directories, JAR
    :type classpath: str
    if isinstance(cmd, compat.string_types):
    # Set up the classpath.
    if classpath is None:
        classpath = NLTK_JAR
        classpath += os.path.pathsep + NLTK_JAR
    cmd = ['-cp', classpath] + cmd
NLTK_JAR = os.path.abspath(os.path.join(os.path.split(__file__)[0],
    #     classpath='/Users/edloper/Desktop/weka/weka.jar')
                 classpath='/Users/edloper/Desktop/weka/weka.jar')
    if isinstance(method, types.MethodType) and compat.get_im_class(method) is not None:
                 for cls in _mro(compat.get_im_class(method))
def find_file(filename, env_vars=(), searchpath=(),
    :param filename: The name or path of the file.
    :param searchpath: List of directories to search.
    :param verbose: Whether or not to print path when a file is found.
    assert isinstance(filename, compat.string_types)
    assert not isinstance(file_names, compat.string_types)
    assert not isinstance(searchpath, compat.string_types)
    if isinstance(env_vars, compat.string_types):
        path_to_file = os.path.join(filename, alternative)
        if os.path.isfile(path_to_file):
            if verbose: print('[Found %s: %s]' % (filename, path_to_file))
            return path_to_file
        if os.path.isfile(alternative):
        path_to_file = os.path.join(filename, 'file', alternative)
        if os.path.isfile(path_to_file):
            if verbose: print('[Found %s: %s]' % (filename, path_to_file))
            return path_to_file
            for env_dir in os.environ[env_var].split(os.pathsep):
                # Check if the environment variable contains a direct path to the bin
                if os.path.isfile(env_dir):
                    path_to_file = os.path.join(env_dir, alternative)
                    if os.path.isfile(path_to_file):
                        if verbose: print('[Found %s: %s]'%(filename, path_to_file))
                        return path_to_file
                    path_to_file = os.path.join(env_dir, 'file', alternative)
                    if os.path.isfile(path_to_file):
                        if verbose: print('[Found %s: %s]' % (filename, path_to_file))
                        return path_to_file
    # Check the path list.
    for directory in searchpath:
            path_to_file = os.path.join(directory, alternative)
            if os.path.isfile(path_to_file):
                return path_to_file
                path = stdout.strip()
                if path.endswith(alternative) and os.path.exists(path):
                    if verbose: print('[Found %s: %s]' % (filename, path))
                    return path
    if searchpath:
        msg += ''.join('\n    - %s' % d for d in searchpath)
def find_binary(name, path_to_bin=None, env_vars=(), searchpath=(),
    :param name: The name or path of the file.
    :param path_to_bin: The user-supplied binary location (deprecated)
    :param searchpath: List of directories to search.
    :param verbose: Whether or not to print path when a file is found.
    return find_file(path_to_bin or name, env_vars, searchpath, binary_names,
def find_jar(name, path_to_jar=None, env_vars=(),
        searchpath=(), url=None, verbose=True):
    :param path_to_jar: The user-supplied jar location, or None.
                     in addition to the CLASSPATH variable which is
    :param searchpath: List of directories to search.
    assert isinstance(name, compat.string_types)
    assert not isinstance(searchpath, compat.string_types)
    if isinstance(env_vars, compat.string_types):
    # Make sure we check the CLASSPATH first
    env_vars = ['CLASSPATH'] + list(env_vars)
    if path_to_jar is not None:
        if os.path.isfile(path_to_jar):
            return path_to_jar
                         (name, path_to_jar))
            if env_var == 'CLASSPATH':
                classpath = os.environ['CLASSPATH']
                for cp in classpath.split(os.path.pathsep):
                    if os.path.isfile(cp) and os.path.basename(cp) == name:
                path_to_jar = os.environ[env_var]
                if os.path.isfile(path_to_jar) and os.path.basename(path_to_jar) == name:
                    if verbose: print('[Found %s: %s]' % (name, path_to_jar))
                    return path_to_jar
    # Check the path list.
    for directory in searchpath:
        path_to_jar = os.path.join(directory, name)
        if os.path.isfile(path_to_jar):
            if verbose: print('[Found %s: %s]' % (name, path_to_jar))
            return path_to_jar
    if searchpath:
        msg += ''.join('\n    - %s' % d for d in searchpath)
    current directory is included at the beginning of the search path.
    old_path = sys.path
    sys.path = [d for d in sys.path if d not in ('', '.')]
    sys.path = old_path
@compat.python_2_unicode_compatible
        if isinstance(etree, compat.string_types):
    def find(self, path):
        elt = self._etree.find(path)
    def findall(self, path):
        return [ElementWrapper(elt) for elt in self._etree.findall(path)]
def is_writable(path):
    if not os.path.exists(path):
        statdata = os.stat(path)
import nltk.compat
import nltk.compat
import nltk.compat
if nltk.compat.PY3:
import nltk.compat
import nltk.compat
import nltk.compat
import os.path
        name = os.path.basename(filename)
        if not self._checkcompat(): return
        if not self._checkcompat(): return
        if not self._checkcompat(): return
    def _checkcompat(self):
import nltk.compat
         "<h1>Pattern\t\tMatches...</h1>\n"
                    pattern = '(?s)(<%s>)(.*?)(</%s>)' % (tag, tag)
                    for m in re.finditer(pattern, text):
import nltk.compat
if nltk.compat.PY3:
from sys import path
from nltk import compat
if compat.PY3:
#    get_static_index_page, get_static_page_by_path, \
        sp = self.path[1:]
        if compat.unquote_plus(sp) == 'SHUTDOWN THE SERVER':
            usp = compat.unquote_plus(sp)
                if os.path.isfile(usp):
                page = get_static_page_by_path(usp)
def get_static_page_by_path(path):
    Return a static HTML page from the path given.
    if path == "index_2.html":
    elif path == "index.html":
    elif path == "NLTK Wordnet Browser Database Info.html":
    elif path == "upper_2.html":
    elif path == "upper.html":
    elif path == "web_help.html":
    elif path == "wx_help.html":
        return "Internal error: Path for static page '%s' is unknown" % path
    f = open(path)
    module_path = '.'.join(components[:-1])
    mod = __import__(module_path)
def metaloader(classpath):
        classref = custom_import(classpath)
def register_tag(tag, classpath):
    yaml.add_constructor('!'+tag, metaloader(classpath))
                         metaloader(classpath))
old_sys_path = sys.path[:]
sys.path = [p for p in sys.path if "nltk" not in p]
sys.path = old_sys_path
    # pypy compatibility
    co-occurrence patterns.
        pattern = re.compile("[ \t\r\f\v]*\n[ \t\r\f\v]*\n[ \t\r\f\v]*")
        matches = pattern.finditer(text)
    >>> regexp_tokenize(s, pattern='\w+|\$[\d\.]+|\S+')
first argument, and the regular expression pattern as its second
``re`` functions, where the pattern is always the first argument.
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
    :type pattern: str
    :param pattern: The pattern used to build this tokenizer.
        (This pattern may safely contain grouping parentheses.)
    :param gaps: True if this tokenizer's pattern should be used
        tokenizer's pattern should be used to find the tokens
        tokenizer's pattern.  By default, the following flags are
    def __init__(self, pattern, gaps=False, discard_empty=True,
        # If they gave us a regexp object, extract the pattern.
        pattern = getattr(pattern, 'pattern', pattern)
        self._pattern = pattern
        nongrouping_pattern = convert_regexp_to_nongrouping(pattern)
            self._regexp = re.compile(nongrouping_pattern, flags)
                             (pattern, e))
        return ('%s(pattern=%r, gaps=%r, discard_empty=%r, flags=%r)' %
                (self.__class__.__name__, self._pattern, self._gaps,
def regexp_tokenize(text, pattern, gaps=False, discard_empty=True,
    tokenizer = RegexpTokenizer(pattern, gaps, discard_empty, flags)
from nltk.compat import unicode_repr, python_2_unicode_compatible, string_types
    # Retained for backward compatibility
@python_2_unicode_compatible
        with different case patterns (i) overall, (ii) at
                pat = '\s*'.join(re.escape(c) for c in tok)
                m = re.compile(pat).match(text,pos)
from nltk.compat import xrange, izip
    >>> import os.path
    >>> t = AnnotationTask(data=[x.split() for x in open(os.path.join(os.path.dirname(__file__), "artstein_poesio_example.txt"))])
from nltk import compat
@compat.python_2_unicode_compatible
            for j, nj in compat.iteritems(label_freqs):
                for l, nl in compat.iteritems(label_freqs):
        for k, f in compat.iteritems(label_freqs):
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import xrange
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import python_2_unicode_compatible, string_types
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
from nltk.compat import (total_ordering, python_2_unicode_compatible,
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
@python_2_unicode_compatible
'''This file is adapted from the pattern library.
URL: http://www.clips.ua.ac.be/pages/pattern-web
from .compat import text_type, basestring, imap, unicode, binary_type, PY2
    MODULE = os.path.dirname(os.path.abspath(__file__))
# The default part-of-speech tagset used in Pattern is Penn Treebank II.
# Pattern's text parsers are based on Brill's algorithm.
def _read(path, encoding="utf-8", comment=";;;"):
    """ Returns an iterator over the lines in the file at the given path,
    if path:
        if isinstance(path, basestring) and os.path.exists(path):
            # From file path.
                f = codecs.open(path, 'r', encoding='utf-8')
                f = open(path, 'r', encoding='utf-8')
        elif isinstance(path, basestring):
            f = path.splitlines()
        elif hasattr(path, "read"):
            f = path.read().splitlines()
            f = path
    def __init__(self, path="", morphology=None, context=None, entities=None, NNP="NNP", language=None):
        self._path = path
        self.morphology = Morphology(self, path=morphology)
        self.context    = Context(self, path=context)
        self.entities   = Entities(self, path=entities, tag=NNP)
        dict.update(self, (x.split(" ")[:2] for x in _read(self._path) if x.strip()))
    def path(self):
        return self._path
    def __init__(self, lexicon={}, path=""):
        self._path = path
    def path(self):
        return self._path
        list.extend(self, (x.split() for x in _read(self._path)))
    def __init__(self, lexicon={}, path=""):
        self._path = path
    def path(self):
        return self._path
        list.extend(self, (x.split() for x in _read(self._path)))
RE_ENTITY1 = re.compile(r"^http://")                            # http://www.domain.com/path
    def __init__(self, lexicon={}, path="", tag="NNP"):
        self._path = path
    def path(self):
        return self._path
        for x in _read(self.path):
        # Note: we could also scan for patterns, e.g.,
    def __init__(self, path="", language=None, synset=None, confidence=None, **kwargs):
        self._path       = path   # XML file path.
    def path(self):
        return self._path
    def load(self, path=None):
        """ Loads the XML-file (with sentiment annotations) from the given path.
            By default, Sentiment.path is lazily loaded.
        if not path:
            path = self._path
        if not os.path.exists(path):
        xml = cElementTree.parse(path)
        # A pattern.en.wordnet.Synset.
        # A pattern.en.Text.
        # A pattern.en.Sentence or pattern.en.Chunk.
        # A pattern.en.Word.
        # A pattern.vector.Document.
# The shallow parser in Pattern is meant to handle the following tasks:
# Pattern.parse() returns a TaggedString: a Unicode string with "tags" and "language" attributes.
# The pattern.text.tree.Text class uses this attribute to determine the token format and
    def __init__(self, path=""):
        self._path = path
        for x in _read(self._path):
    def path(self):
        return self._path
    def train(self, s, path="spelling.txt"):
        """ Counts the words in the given string and saves the probabilities at the given path.
        f = open(path, "w")
'''Default sentiment analyzers are English for backwards compatibility, so
>>> from textblob.sentiments import PatternAnalyzer
>>> from textblob.en.sentiments import PatternAnalyzer
                                PatternAnalyzer, NaiveBayesAnalyzer)
compatibility, so you can still do
from textblob.compat import PY2, csv
from textblob.compat import basestring
from textblob.compat import izip
HERE = os.path.dirname(os.path.abspath(__file__))
sys.path.append(HERE)
from textblob.compat import unicode, basestring
from textblob.taggers import PatternTagger
from textblob.sentiments import PatternAnalyzer
from textblob.parsers import PatternParser
        (http://norvig.com/spell-correct.html) as implemented in the pattern
        # This is included for Python 2.* compatibility
        defaults to :class:`PatternTagger <textblob.en.taggers.PatternTagger>`.
        defaults to :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.
        :class:`PatternParser <textblob.en.parsers.PatternParser>`.
    pos_tagger = PatternTagger()
    analyzer = PatternAnalyzer()
    parser = PatternParser()
        return PatternAnalyzer().analyze(self.raw)[0]
        return PatternAnalyzer().analyze(self.raw)[1]
        :class:`PatternTagger <textblob.en.taggers.PatternTagger>`.
        :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.
            compatibility that was broken after version 0.4.0.
        defaults to :class:`PatternTagger <textblob.en.taggers.PatternTagger>`.
        defaults to :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.
        :class:`PatternParser <textblob.en.parsers.PatternParser>`.
    pos_tagger = PatternTagger()
    analyzer = PatternAnalyzer()
    parser = PatternParser()
compatibility so you can still import text.inflect.
'''This file is based on pattern.en. See the bundled NOTICE file for
from textblob.compat import text_type, unicode
    MODULE = os.path.dirname(os.path.abspath(__file__))
        path = os.path.join(MODULE, "en-spelling.txt")
    def load(self, path=None):
        _Sentiment.load(self, path)
        if not path:
        path = os.path.join(MODULE, "en-lexicon.txt"),
  morphology = os.path.join(MODULE, "en-morphology.txt"),
     context = os.path.join(MODULE, "en-context.txt"),
    entities = os.path.join(MODULE, "en-entities.txt"),
        path = os.path.join(MODULE, "en-sentiment.xml"),
from textblob.en import parse as pattern_parse
class PatternParser(BaseParser):
    '''Parser that uses the implementation in Tom de Smedt's pattern library.
    http://www.clips.ua.ac.be/pages/pattern-en#parser
        return pattern_parse(text)
from textblob.en import sentiment as pattern_sentiment
class PatternAnalyzer(BaseSentimentAnalyzer):
    pattern library. Returns results as a tuple of the form:
        return pattern_sentiment(text)
from textblob.taggers import PatternTagger
    POS_TAGGER = PatternTagger()
'''The pluralize and singular methods from the pattern library.
See here https://github.com/clips/pattern/blob/master/LICENSE.txt for
        "ibis", "lens", "mantis", "marquis", "metropolis", "pathos", "pelvis", "polis", "rhinoceros",
import os.path
from textblob.en import tag as pattern_tag
class PatternTagger(BaseTagger):
    Tom de Smedt's pattern library
    (http://www.clips.ua.ac.be/pattern).
        return pattern_tag(sentence, tokenize)
'''Default taggers to the English taggers for backwards incompatiblity, so you
from textblob.en.taggers import PatternTagger, NLTKTagger, PerceptronTagger
    %s <work_dir> <search_regex> <replace_with> <glob_pattern> [backup]
    """ % (os.path.basename(sys.argv[0]))
def find_replace(directory, search_pattern, replacement, glob_pattern, create_backup=False):
    for path, dirs, files in os.walk(os.path.abspath(directory)):
        for filename in fnmatch.filter(files, glob_pattern):
            pardir = os.path.normpath(os.path.join(path, '..'))
            pardir = os.path.split(pardir)[-1]
            filepath = os.path.join(path, filename)
                backup_path = filepath + '.bak'
                print 'DBG: creating backup', backup_path
                shutil.copyfile(filepath, backup_path)
            with open(filepath) as f:
            with open(filepath, "w") as f:
                print 'DBG: replacing in file', filepath
                # s = s.replace(search_pattern, replacement)
                data = re.sub(search_pattern, replacement, data)
    glob_pattern = sys.argv[4]
    find_replace(work_dir, search_regex, replacement, glob_pattern, dobackup)
essays = [[line.strip() for line in open(os.path.join("/home/ahmed/alltxt/02whole.txt")).readlines() if len(line.strip()) > 1] for essay in range(1, 21)]
            os.rmdir(os.path.join(root, name))
            print 'Skipping', os.path.join(root, name)
##  Usage : python printdir.py [path ala z:/mypath or /mypath/foo
outName = 'dirpaths.txt'
    curPath = item.split('/')
        curPath = item
        curPath = '\t' * (len(curPath)-1) + curPath[len(curPath)-1]
    f.write(curPath + '\n')
    print curPath
if os.path.exists(outName) :
    Yes, we’re in the process of filing a patent for it. But basically the system works with a Natural Language Processing Engine. Actually, there are several parts for the content matching, but besides analyzing what topics the articles are talking about, we have machine learning algorithms that match you to the relevant suggested stuff. For example, if you shared an article about Zuck that got a good reaction from your followers, we might offer you another one about Kevin Systrom (just a simple example).
def _should_include_path(path, includes, excludes):
    """Return True iff the given path should be included."""
    from os.path import basename
    base = basename(path)
                    log.debug("include `%s' (matches `%s')", path, include)
                log.debug("exclude `%s' (matches no includes)", path)
                log.debug("exclude `%s' (matches `%s')", path, exclude)
    from os.path import join, isdir, islink, abspath
    # get a list of the files the directory contains.  os.path.walk
            path = join(top, name)
            if islink(path):
            elif isdir(path):
        path = join(top, name)
        if follow_symlinks and islink(path):
            # Only walk this path if it links deeper in the same tree.
            top_abs = abspath(top)
            link_abs = abspath(join(top, os.readlink(path)))
        for x in _walk(path, topdown, onerror, follow_symlinks=follow_symlinks):
def _paths_from_path_patterns(path_patterns, files=True, dirs="never",
    """_paths_from_path_patterns([<path-patterns>, ...]) -> file paths
    Generate a list of paths (files and/or dirs) represented by the given path
    patterns.
        "path_patterns" is a list of paths optionally using the '*', '?' and
            '[seq]' glob patterns.
        "files" is boolean (default True) indicating if file paths
              always            yield all dirs matching given patterns
        "recursive" is boolean (default True) indicating if paths should
        "includes" is a list of file patterns to include in recursive
        "excludes" is a list of file and dir patterns to exclude.
        "on_error" is an error callback called when a given path pattern
                on_error(PATH_PATTERN)
    of paths as arguments. (For Unix-heads: the shell on Windows does
        script PATH*    # yield all files matching PATH*; if none,
                        # call on_error(PATH*) callback
        script -r PATH* # yield files matching PATH* and files recursively
                        # under dirs matching PATH*; if none, call
                        # on_error(PATH*) callback
        script PATH*    # yield all files and dirs matching PATH*; if none,
                        # call on_error(PATH*) callback
        script -r PATH* # yield files matching PATH* and files recursively
                        # under dirs matching PATH*; if none, call
                        # on_error(PATH*) callback
        script PATH*    # yield all files and dirs matching PATH*; if none,
                        # call on_error(PATH*) callback
        script -r PATH* # yield files and dirs matching PATH* and recursively
                        # under dirs; if none, call on_error(PATH*)
    from os.path import basename, exists, isdir, join, normpath, abspath, \
                        lexists, islink, realpath
    assert not isinstance(path_patterns, basestring), \
        "'path_patterns' must be a sequence, not a string: %r" % path_patterns
    for path_pattern in path_patterns:
        # Determine the set of paths matching this path_pattern.
            if glob_char in path_pattern:
                paths = glob(path_pattern)
                paths = exists(path_pattern) and [path_pattern] or []
                paths = lexists(path_pattern) and [path_pattern] or []
        if not paths:
                    log.error("`%s': No such file or directory", path_pattern)
                on_error(path_pattern)
        for path in paths:
            if (follow_symlinks or not islink(path)) and isdir(path):
                    canon_path = normpath(abspath(path))
                        canon_path = realpath(canon_path)
                    if canon_path in searched_dirs:
                        searched_dirs.add(canon_path)
                   ) and _should_include_path(path, includes, excludes):
                    yield path
                if recursive and _should_include_path(path, [], excludes):
                    for dirpath, dirnames, filenames in _walk(path,
                            d = join(dirpath, dirname)
                                canon_d = normpath(abspath(d))
                                    canon_d = realpath(canon_d)
                               and _should_include_path(d, includes, excludes):
                            if not _should_include_path(d, [], excludes):
                                f = join(dirpath, filename)
                                if _should_include_path(f, includes, excludes):
            elif files and _should_include_path(path, includes, excludes):
                yield path
    print("Usage: %s [option] <textfile>" % os.path.basename(sys.argv[0]))
          Pattern = RegExp.pattern
          Founds.append((Line, Policy, Pattern, Text))
import os.path
if not os.path.isfile('appid.txt'):
if not os.path.isdir('db'): os.mkdir('db')
globaldb = os.path.join('db', 'papers.p')
if not os.path.isfile(globaldb): pickle.dump([], open(globaldb, "wb"))
  dirpath = os.path.join('db', idstr)
  havethis = os.path.isdir(dirpath)
  print "Creating folder %s..." % (dirpath, )
  os.mkdir(dirpath)
jsonpath = os.path.join(dirpath, 'json.p')
pickle.dump(pub, open(jsonpath, "wb"))
print "Writing ", jsonpath
  refPicklePath = os.path.join('db', idstr, fname)
  print "writing ", refPicklePath
  pickle.dump(ids, open(refPicklePath, "wb"))
pdfpath = os.path.join('db', idstr, 'paper.pdf')
    urllib.urlretrieve(u, pdfpath)
    print "saved pdf at ", pdfpath
      os.system(opencommand + " " + pdfpath)
      print "%s failed. Make sure the downloaded %s pdf is correct." % (opencommand, pdfpath, )
  print "Couldn't get the paper pdf. Please download manually and save as %s." % (pdfpath, )
  thumbpath = os.path.join('db', idstr, 'thumb.png')
  cmd = "convert %s -thumbnail 150 -trim %s" % (pdfpath, thumbpath)
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
                if os.path.splitext(dirfile)[1][1:] in args:
        elif os.path.isdir(dirfile) and subdir:
config_handle = file(os.path.expanduser('~/.smsconf'),"r+")
