                    html = '\\begin{lstlisting}[keywords={}]\n%s\n\\end{lstlisting}' % code
                    html = '\\begin{lstlisting}\n%s\n\\end{lstlisting}' % code
\\usepackage{listings}
zlist = []
    zlist.append(xp)
print zlist
    def good_append(new_item, a_list=None):
    if a_list is None:
        a_list = []
    a_list.append(new_item)
    return a_list
Filter out blank rows from a CSV reader (or items from a list):
def get_list_of_hits(pattern, text):
    flat_list = [item for [item] in items.asList()]
    return(flat_list)
    unique_strings = list(set(strings))
    return folder, [f for f in os.listdir(folder)
retCode, choice = dialog.list_menu(choices)
retCode, choice = dialog.list_menu(choices)
retCode, choice = dialog.list_menu(choices)
retCode, choice = dialog.list_menu(choices)
    '''Recursively traverse the filesystem, starting at path, and return a full list
    file_list = build_file_paths(path)
    print "Traversed %d files." % len(file_list)
    for file in file_list:
folder = engine.get_folder("wlist")
folder = engine.get_folder("py_list")
for zf in list_files:
    for path, dirlist, filelist in os.walk(top):
        for name in fnmatch.filter(filelist,filepat):
retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])
retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])
retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[""])
# dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[], **kwargs)
retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=["w"])
retCode, choice = dialog.list_menu_multi(wwoptions, title="Choose one or more values", message="Choose one or more values", defaults=[])
retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])
from gluon.fileutils import listdir, cleanpath, tar, tar_compiled, untar
        if len(items)==6 and items[5] in ['LISTEN','ESTABILISHED','CLOSE_WAIT',
    os.system('service --list  > applications/sysadmin/cache/service.tmp')
        'command', 'datagrid', 'datalist', 'dd', 'del', 'details', 'dfn',
      'ismap', 'keytype', 'label', 'leftspacing', 'lang', 'list', 'longdesc',
        'command', 'datagrid', 'datalist', 'dd', 'del', 'details', 'dfn',
      'ismap', 'keytype', 'label', 'leftspacing', 'lang', 'list', 'longdesc',
        for file in [file for file in os.listdir(dir) if not file in [".",".."]]:
        for file in [file for file in os.listdir(dir) if not file in [".",".."]]:
file_list = []
            file_list.append(os.path.join(root,name))
print file_list
            words = line.split()    # list of words
blob.noun_phrases   # WordList(['titular threat', 'blob',
    # for finding modules).  You can add paths to that list.  Note
    # Add all standard c-extensions to extension_modules list.
# for sorting a nested list
def split_string(source,splitlist):
    return ''.join([ w if w not in splitlist else ' ' for w in source]).split()
            word_list = split_string(words, """ ,"!-.()<>[]{};:?!-=`&""")
            for word in word_list:
retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])
#Itâ€™s faster to build a new list with list comprehension.
my_list = []
#Adding List Items
my_list.extend([a, b, c...])
  txtlst = open("out.txt").read().split() # get all words in a giant list
  # top is a list of (word, frequency) items. Save it
    '''Recursively traverse the filesystem, starting at path, and return a full list
    file_list = build_file_paths(path)
    print "Traversed %d files." % len(file_list)
    for file in file_list:
    Loads the economist data as dict of lists of debates. Each debate is a
    new_debates = defaultdict(list)
newlist = []
for word in oldlist:
    newlist.append(word.upper())
        self.endTagList = []
                    self.endTagList.insert(0, endTag)
                    self.endTagList.remove(endTag)
        self.result.extend(self.endTagList)
class lazylist(list):
        # Must load data with list.append(self, v) instead of lazylist.append(v).
        """ If the list is empty, calls lazylist.load().
            Replaces lazylist.method() with list.method() and calls it.
        if list.__len__(self) == 0:
            setattr(self, method, types.MethodType(getattr(list, method), self))
        return getattr(list, method)(self, *args)
    """ Returns a list of sentences. Each sentence is a space-separated string of tokens (words).
    for a, b in list(replace.items()):
        """ Applies the rule to the given token or list of tokens.
class Morphology(lazylist, Rules):
        """ A list of rules based on word morphology (prefix, suffix).
        cmd.update(("f" + k, v) for k, v in list(cmd.items()))
        list.extend(self, (x.split() for x in _read(self._path)))
        """ Applies lexical rules to the given token, which is a [word, tag] list.
        lazylist.insert(self, i, r)
class Context(lazylist, Rules):
        """ A list of rules based on context (preceding and following words).
        list.extend(self, (x.split() for x in _read(self._path)))
        """ Applies contextual rules to the given list of tokens,
            where each token is a [word, tag] list.
        lazylist.insert(self, i, [tag1, tag2, cmd, x or "", y or ""])
        """ Applies the named entity recognizer to the given list of tokens,
            where each token is a [word, tag] list.
# Sentiment().assessments(txt) returns a list of (chunk, polarity, subjectivity, label)-tuples.
def avg(list):
    return sum(list) / float(len(list) or 1)
        for w, pos in list(words.items()):
            as a function that takes a list of words and returns a weight.
        # A list of words.
        elif isinstance(s, list):
        """ Returns a list of (chunk, polarity, subjectivity, label)-tuples for the given list of words:
            where chunk is a list of successive words: a known word optionally
    """ Returns a list of [token, tag]-items for the given list of tokens:
        tagged = [list(map(token, tag)) or [token, default[0]] for token, tag in tagged]
    """ The input is a list of [token, tag]-items.
        The output is a list of [token, tag, chunk]-items:
    """ The input is a list of [token, tag, chunk]-items.
        The output is a list of [token, tag, chunk, preposition]-items.
        """ Returns a list of sentences from the given string.
        """ Annotates the given list of tokens with part-of-speech tags.
            Returns a list of tokens, where each token is now a [word, tag]-list.
        """ Annotates the given list of tokens with chunk tags.
        """ Annotates the given list of tokens with prepositional noun phrase tags.
        """ Annotates the given list of tokens with verb/predicate tags.
        """ Annotates the given list of tokens with word lemmata.
        if isinstance(s, (list, tuple)):
        # With collapse=False (or split=True), returns raw list
        # Collapse raw list.
        # From a TaggedString.split(TOKENS) list:
        if isinstance(string, list):
        s.tags = list(tags)
        """ Returns a list of sentences, where each sentence is a list of tokens,
            where each token is a list of word + tags.
        """ Returns the given list of words filtered by known words.
        """ Return a list of (word, confidence) spelling corrections for the given word,
newlist = []
for word in oldlist:
    newlist.append(word.upper())
#Similar to other solutions, but using fnmatch.fnmatch instead of glob, since os.walk already listed the filenames:
uec = "This program will neatly format a programming comment block so that it's surrounded by pound signs (#). It does this by splitting the comment into a list and then concatenating strings each no longer than 76 characters long including the correct amount of right side space padding."
    wordList = comment.split()                            # load the comment into a list
    last = wordList.index(wordList[-1])                   # numerical position of last element
    for word in wordList:
            tmpList = tmpString.split()
            tmpString = tmpList[-1] + ' '                 # before popping last element load it for the beginning of the next cycle
            tmpList.pop()
            for tmp in tmpList:
            tmpList = tmpString.split()
            tmpList.pop()
            for tmp in tmpList:
  against filename2 and returns list of the differences and a count of
  list1 = f1.readlines()
  list2 = f2.readlines()
    for i in range(0,len(list1)):
      list1[i] = list1[i].lower()
    for i in range(0,len(list2)):
      list2[i] = list2[i].lower()
  diffs = set(list1) - set(list2)
#List of directories and files to backup
retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])
def getVocabList():
  '''reads the fixed vocabulary list in vocab.txt and returns a dictionary mapping strings to integers'''
  # Read the fixed vocabulary list
  vocabList = {}
      vocabList[vocab.rstrip()]=i;
  return vocabList, vocabArray
xx , xp = getVocabList()
# regular expression, and a list of possible responses,
# gListLatLong.py - X.Jacobs (xiancobs@gmail.com) - 05/11/2012
# Get lat/long coordinates of an address list from Google Maps
address_list    = [    "106 S. Main Street, North East, MD",
def coordinate(address_list):
    if len(address_list) > 25:
                                                for record in address_list
    list_mark   = page.split(",latlng:{")[1:]
    list_coordinate = [ mark[0:mark.find('},image:')].replace("lat:","").replace("lng:","")
                        for mark in list_mark
    return list_coordinate
print coordinate(address_list)
    # It creates lexicon (list of right words)
    # It creates a list of special words depending on the set
    # It creates a list of extra words
    self.special = [] #It creates a  list of special words
      self.special.extend( [ 'generalist','federalist', 'generalists','federalists','python', 'pythons','mice','panda','koala','achenbach','koalas','pandas','alligator','alligators','lizzard','lizzards','crockadile','crockadiles'])
    #Create list of extra words
    #It creates list of lexicons so that each object of a list has word with the same length
      # candidates is the list of words is set to the list of right words
      # If previous word is not empty, It forms a list of bigrams
def dir_list(dir_name, subdir, *args):
    '''Return a list of file names in directory 'dir_name'
    Additional arguments, if any, are file extensions to add to the list.
    Example usage: fileList = dir_list(r'H:\TEMP', False, 'txt', 'py', 'dat', 'log', 'jpg')
    fileList = []
    for file in os.listdir(dir_name):
                fileList.append(dirfile)
                    fileList.append(dirfile)
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
def combine_files(fileList, fn):
    for file in fileList:
    fileList = dir_list(r'/home/ahmed/alltxt/', True, 'txt' )
    combine_files(fileList, fn)
	for name in os.listdir(path):
            for name in os.listdir(dpath):
	# 'answer' is the list of having answer or not.
	# 'pats' is the list of regular expressions used to find an answer.
	#  'fi' is the list of answers as vectors of words. 'fi' is updated
	# 'matchs' is a list of matches for each answer.
	# essay given the list of regular expressions and the text of the essay.
	# 'update_ans' updates the list
    x = section_names.index(section_name) # Get the position of this section name in the section_names list
    label = section_labels[x] # so you can find its label in the section_labels list
        mylist2 = []
                mylist1 = []
                        mylist1.append(temp)
                        mylist1.append(temp)
##                        mylist1.append(temp)
                        mylist1.append(sent)
                mylist2.append(mylist1)
        result = mylist2
# Return a list containing the found headings and their indices.
    mylist = []
            mylist.append(item) # but now I have had to add digital headings because sometimes section headings have numbers in front of them. I've added letter headings for the same reason.
    return mylist
    list_this_heading = count_this_heading('word count', headings, text)
    list_this_heading.reverse() # Reverse the order of headings with word count in them in case there is more than one, because you want the one at the end of the essay.
    heading_count = len(list_this_heading)
    if heading_count > 0 and list_this_heading[0][1] > (len(text)/2): # If there is a word count heading,and it occurs in the second half of the essay, then re-set word count heading to the new value
        word_count_index = list_this_heading[0][1]
    captions = [] # Get a list of all the captions
    mylist = [] # Get every caption from the captions list that is a 'contents' heading.
            mylist.append(item)
    heading_count = len(mylist)
    if mylist != []:
        print '\nTable of contents headings:', mylist
        first = mylist[0][1] # set the index of the contents heading to the variable 'first'
    list_this_heading = count_this_heading(section_name, headings, text)
    if list_this_heading != [] and dev == 'DGF':
        print 'All headings like this:', list_this_heading
    heading_count = len(list_this_heading) # heading_count is the number of 'section_name' headings found. Typically 2, 1, or 0. If 2, the first is probably in a contents section.
        first_heading = list_this_heading[0][1] # So take it as heading for body intro section
        print '3 A heading has been found for section name:', section_name, first, last, list_this_heading[0][0]
        first_heading = list_this_heading[0][1] # So take it as heading for body concl section
            print '4 A heading has been found for section name:', section_name, first, last, list_this_heading[0][0]
        first_heading = list_this_heading[0][1] # So take it as heading for body intro section
            print '5 A heading has been found for section name:', section_name, first, last, list_this_heading[0][0]
    return (para_indices, title_indices, headingQ), list_this_heading
# Returns a list of all the sentences so far marked as headings.
but here I need a list that will be used to locate the beginning and end of the introduction and conclusion.
    mylist2 = []
            mylist1 = []
                        mylist1.append((sent, counter_p))
            mylist2 = mylist2 + mylist1
    result = [para for para in mylist2 if para != []]
# This takes the list of headings found so far and goes through the essay
    mylist1 = headings
                                mylist1.append(temp)
        return mylist1
    mylist3 = []
                mylist3.append(para2)
            # LOOK BACK ONE PARAGRAPH, IF BOTH DIGITAL HEADINGS, THIS IS LIST ITEM
                sentlist = []
                    if counter_s <= len(para)-1: # # label all the sentences in this para as list items
                        sentlist.append(temp)
                para2 = sentlist
                mylist3.append(para2)
            # LOOK BACK ONE PARAGRAPH, IF BOTH LETTER HEADINGS, THIS IS LIST ITEM
                sentlist = []
                while 1: # label all the sentences in this para as list items
                        sentlist.append(temp)
                para2 = sentlist
                mylist3.append(para2)
                mylist3.append(para)
    text = mylist3
    mylist3 = []
                mylist3.append(para2)
            # LOOK BACK ONE PARAGRAPH, AND FORWARD ONE PARAGRAPH. IF BOTH ARE LIST ITEMS, THIS IS PROBABLY ALSO A LIST ITEM REGARDLESS OF LENGTH OR LABEL.
                  and text[nextpara][0][0] == '#-s:b#' # and the next para is a LIST ITEM
                  and text[prevpara][0][0] == '#-s:b#' # and the previous para is A LIST ITEM
                sentlist = []
                while 1: # label all the sentences in this para as list items
                        sentlist.append(temp)
                para2 = sentlist
                mylist3.append(para2)
                mylist3.append(para2)
    text = mylist3
    paralist = []
            mylist2 = []
                mylist2.append(sentlen)
            paralen = sum(mylist2)
                sentlist = []
                            sentlist.append(temp)
                            #print '\n1.', sentlist
##                            sentlist.append(temp)
                            sentlist.append(temp)
                            #print '\n2.', sentlist
                paralist.append(sentlist)
                paralist.append(para)
    text = paralist
    mylist2 = []
            mylist1 = []
                    mylist1.append(temp)
            mylist2.append(mylist1)
    text = mylist2
    mylist3 = []
                mylist3.append(para2)
            # LOOK AHEAD TWO PARAGRAPHS. IF ALL THREE ARE DIGITAL HEADINGS, THIS IS A LIST OF ITEMS.
                sentlist = []
                        temp = ['#-s:b#'] + s[1:] # label this sentence as a list item
                        sentlist.append(temp)
                        para2 = sentlist
                mylist3.append(para2)
            # LOOK AHEAD TWO PARAGRAPHS. IF ALL THREE ARE LETTER HEADINGS, THIS IS A LIST OF ITEMS.
                sentlist = []
                        temp = ['#-s:b#'] + s[1:] # label this sentence as a list item
                        sentlist.append(temp)
                        para2 = sentlist
                mylist3.append(para2)
                mylist3.append(para2)
    tempA = mylist3
    someheadings = get_headings(tempC)     # Get a list of all the headings you have found, each with its index.
    mylist = []
              or firstword in letters)): # or another kind of list itemiser
##                            mylist1.append(temp)
##                            mylist1.append(temp)
##                            mylist1.append(temp)
##                            mylist1.append(temp)
        # even if they are quite long. But they can also be listed prose points
    ((c_first, c_last),title_indices,conclheaded), list_this_heading1 = find_section_paras(text50, 'Conclusion', headings, countTextChars, nf, nf2, dev)
    ((first, last),(title_first1,title_last1),headingQ), list_this_heading2 = find_section_paras(text50, 'Summary', headings, countTextChars, nf, nf2, dev)
    if list_this_heading2 == list_this_heading1:
    ((first, last),(title_first2,title_last2),headingQ), list_this_heading3 = find_section_paras(text50, 'Preface', headings, countTextChars, nf, nf2, dev)
    ((first, last),(title_first3,title_last3),headingQ), list_this_heading4 = find_section_paras(text50, 'Abstract', headings, countTextChars, nf, nf2, dev)
    ((i_first, i_last),(title_first4,title_last4),introheaded), list_this_heading5 = find_section_paras(text50,'Introduction', headings, countTextChars, nf, nf2, dev)
ARABIC_GLYPHS_LIST = [
	list_word = list(unshaped_word)
					lam_alef = get_lam_alef(list_word[haraka_position], candidate_lam, False)
					lam_alef = get_lam_alef(list_word[haraka_position], candidate_lam, True)
					list_word[lam_position] = lam_alef
					list_word[haraka_position] = u' '
	return u''.join(list_word).replace(u' ', u'')
		l = list(u'\0' * (len(self.stripped_harakat) + len(reshaped_word)))
def dir_list(dir_name, subdir, *args):
    '''Return a list of file names in directory 'dir_name'
    Additional arguments, if any, are file extensions to add to the list.
    Example usage: fileList = dir_list(r'H:\TEMP', False, 'txt', 'py', 'dat', 'log', 'jpg')
    fileList = []
    for file in os.listdir(dir_name):
                fileList.append(dirfile)
                    fileList.append(dirfile)
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
def combine_files(fileList, fn):
    for file in fileList:
#List of directories and files to backup
retCode, choice = dialog.list_menu(choices)
    dialog.list_menu(options, title="Choose a value", message="Choose a value", default=None, **kwargs) Show a single-selection list menu
    dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[], **kwargs) Show a multiple-selection list menu
    st = s.strip(n).expandtabs(sz).split(n)   # list of lines
# To-do list manager.
A primitive Getting Things Done to-do list manager.
        self.todo = ToDo()  # the to-do list is here!
            for attr in Task.attributes_list[3:]:
        for attrib in Task.attributes_list[1:]:
            # And, add it to the to-do list
            # And, replace it into the to-do list
    def do_listall(self, nb):
        """List #nb tasks (close or not and ordered by #id):
        GTD> listall [#nb]"""
    do_la = do_listall
    def do_list(self, nb):
        """List #nb open tasks (ordered by #id):
        GTD> list [#nb]"""
    do_ls = do_list
    def do_listinbox(self, nb):
        """List #nb quick added tasks (i.e. without context; ordered by #id):
        GTD> listinbox [#nb]"""
    do_li = do_listinbox
    def do_listref(self, nb):
        """List #nb ref tasks (ordered by #id):
        GTD> listref [#nb]"""
    do_lr = do_listref
    def do_listpri(self, regexp):
        GTD> listpri [regexp]"""
    do_lp = do_listpri
        # List done/close tasks
                    self.todo.supp(t['id'])  # remove done/close tasks from current list
                print "Getting Things Done to-do list manager"
                prt_list = ( { 'name': "context",
                for attr in prt_list:
                            try:  # ok, remove from unprinted list
    sort, order, list, listall, status
    """A primitive Getting Things Done to-do list manager.
    option_list = [
                          description="A primitive Getting Things Done to-do list manager.",
                          option_list=option_list)
# for sorting a nested list
def split_string(source,splitlist):
    return ''.join([ w if w not in splitlist else ' ' for w in source]).split()
            word_list = split_string(words, """ ,"!-.()<>[]{};:?!-=`&""")
            for word in word_list:
        #Set up lists
        #Write lists to file
    url_search = 'http://%s.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&sroffset=%d&srlimit=%d&format=yaml'
retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])
# build up list of papers
      twslist = pickle.load(open(topWordsPicklePath, "rb"))
      p['tw'] = [x[0] for x in twslist]
    imfiles = [f for f in os.listdir(pdir) if re.match(r'thumb.*\.png', f)]
def dir_list(dir_name, subdir, *args):
    '''Return a list of file names in directory 'dir_name'
    Additional arguments, if any, are file extensions to add to the list.
    Example usage: fileList = dir_list(r'H:\TEMP', False, 'txt', 'py', 'dat', 'log', 'jpg')
    fileList = []
    for file in os.listdir(dir_name):
                fileList.append(dirfile)
                    fileList.append(dirfile)
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
def combine_files(fileList, fn):
    for file in fileList:
fileList = dir_list(r'/home/ahmed/Dropbox', False, 'txt', 'py')
for wxp in fileList:
#print fileList
#combine_files(fileList, fn)
    n = list(range(len(plural_rules)))
    if word in list(custom.keys()):
    for w in list(singular_irregular.keys()):
                Examples of phrases which are commonly employed to realise these functions are listed below. Note that there may be a certain amount of overlap between some of the categories under which the phrases are listed.
            Introductory sections for research dissertations, are normally much more complex than this and, as well as the elements above, may include the following: a synopsis of key literature/current state of knowledge, synopsis of methods, lists of research questions or hypotheses to be tested, significance of the study, recognition of the limitations of the study, reasons for personal interest in the topic.
                    one important characteristic of academic writing is that all the sources of information that the writer has used need to be indicated, not just as a bibliography or list of references, but also in or alongside the text. in some cases the source will be the main subject of the sentence, in others the sources may be mentioned parenthetically (in brackets) or via a notation system (e.g. footnotes). the more common verbs and verb phrases used in academic writing for referring to sources are given below. note that different referencing systems are used in different disciplines. in the examples, the harvard in-text referencing system has been used. also note that the "author as subject" style is less common in the sciences.
                Rao (2013) lists three reasons why the English language has become so dominant. These are: ....
            Critics question the ability of poststructuralist theory to provide ....
            In the Methods section of a dissertation or research article, writers give an account of how they carried out their research.The Materials and Methods section should be clear and detailed enough for another experienced person to repeat the research and reproduce the results. Typical features with examples of this language are listed below.
            The term discussion has a variety of meanings in English. In academic writing, however, it usually refers to two types of activity: a) considering both sides of an issue, or question, b) considering the results of research and the implications of these. Discussion sections in dissertations and research articles are probably the most complex in terms of their elements. The most common elements and some of the language that is typically associated with them are listed below:
        The term discussion has a variety of meanings in English. In academic writing, however, it usually refers to two types of activity: a) considering both sides of an issue, or question, b) considering the results of research and the implications of these. Discussion sections in dissertations and research articles are probably the most complex in terms of their elements. The most common elements and some of the language that is typically associated with them are listed below:
            Writers may give specific examples as evidence to support their general claims or arguments. Examples can also be used to help the reader or listener understand unfamiliar or difficult concepts, and they tend to be easier to remember. For this reason, they are often used in teaching. Finally, students may be required to give examples in their work to demonstrate that they have understood a complex problem or concept.
        Classifying and Listing
            When we classify things, we group and name them on the basis of something that they have in common. By doing this we can understand certain qualities and features which they shares as a class. Classifying is also a way of understanding differences between things. In writing, classifying is often used as a way of introducing a reader to a new topic. Along with writing definitions, the function of classification may be used in the early part of an essay, or longer piece of writing. We list things when we want to treat and present a series of items or different pieces of information systematically. The order of a list may indicate rank importance.
            Introducing Lists:
            The key aspects of management can be listed as follows: X, Y and Z.
            Refering to other people's lists
            Smith and Jones (1991) list X, Y and Z as the major causes of infant mortality.
            Smith (2003) lists the main features of X as follows: it is X; it is Y; and it has Z.
            A great deal of academic work involves understanding and suggesting solutions to problems. At postgraduate level, particularly in applied fields, students search out problems to study. In fact, one could say that problems are the food for a significant proportion of academic activity. However, solutions cannot be suggested unless the problem is fully analysed, and this involves a thorough understanding of the causes. Some of the language that you may find useful for explaining causes and effects is listed below:
dialog.list_menu(options, title="Choose a value", message="Choose a value", default=None, **kwargs) Show a single-selection list menu
dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[], **kwargs) Show a multiple-selection list menu
retCode, choice = dialog.list_menu(choices)
retCode, choice = dialog.list_menu(choices)
retCode, choice = dialog.list_menu(choices)
retCode, choice = dialog.list_menu(choices)
retCode, choice = dialog.list_menu(choices)
# for a list of supported languages.
# List of patterns, relative to source directory, that match files and
# A list of ignored prefixes for module index sorting.
# a list of builtin themes.
# further.  For a list of options available for each theme, see the
# Grouping the document tree into LaTeX files. List of tuples
# One entry per manual page. List of tuples
# Grouping the document tree into Texinfo files. List of tuples
    def test_classify_a_list_of_words(self):
    def test_train_from_lists_of_words(self):
        # classifier can be trained on lists of words instead of strings
        assert_true(isinstance(feats, list))
def test_basic_extractor_with_list():
def test_contains_extractor_with_list():
class WordListTest(TestCase):
        wl = tb.WordList(['Beautiful', 'is', 'better'])
        wl = tb.WordList(self.words)
        assert_true(isinstance(dogs, tb.WordList))
        assert_equal(dogs, tb.WordList(['Beautiful', 'is']))
        wl = tb.WordList(['Beautiful', 'is', 'better'])
            assert_equal(repr(wl), "WordList([u'Beautiful', u'is', u'better'])")
            assert_equal(repr(wl), "WordList(['Beautiful', 'is', 'better'])")
        wl = tb.WordList(self.words)
        wl = tb.WordList(['dogs', 'cats', 'buffaloes', 'men', 'mice'])
        assert_equal(wl.singularize(), tb.WordList(['dog', 'cat', 'buffalo', 'man', 'mouse'
        wl = tb.WordList(['dog', 'cat', 'buffalo'])
        assert_equal(wl.pluralize(), tb.WordList(['dogs', 'cats', 'buffaloes']))
        wl = tb.WordList(["cat", "dogs", "oxen"])
        assert_equal(wl.lemmatize(), tb.WordList(['cat', 'dog', 'ox']))
        wl = tb.WordList(self.words)
        assert_equal(wl.upper(), tb.WordList([w.upper() for w in self.words]))
        wl = tb.WordList(['Zen', 'oF', 'PYTHON'])
        assert_equal(wl.lower(), tb.WordList(['zen', 'of', 'python']))
        wl = tb.WordList(['monty', 'python', 'Python', 'Monty'])
    def test_convert_to_list(self):
        wl = tb.WordList(self.words)
        assert_equal(list(wl), self.words)
        wl = tb.WordList(['dog'])
        wl = tb.WordList(["cats", "dogs"])
                tb.WordList(('I', 'am', 'eating')),
                tb.WordList(('am', 'eating', 'a')),
                tb.WordList(('eating', 'a', 'pizza'))
            tb.WordList(('I', 'am', 'eating', 'a')),
            tb.WordList(('am', 'eating', 'a', 'pizza'))
        assert_true(isinstance(blob.words, tb.WordList))
        assert_equal(blob.words, tb.WordList([
        assert_equal(short.words, tb.WordList([
        assert_equal(blob.words, tb.WordList(['Let', "'s", "test", "this"]))
        assert_equal(blob2.words, tb.WordList(['I', 'ca', "n't", "believe",
        assert_equal(blob.upper().words, tb.WordList(['BEAUTIFUL', 'IS', 'BETTER'
        assert_equal(blob.split(), tb.WordList(['Beautiful', 'is', 'better']))
        wl = tb.WordList(l)
        assert_equal(blob.strip().words, tb.WordList(['Beautiful', 'is', 'better'
            tb.WordList(WordTokenizer().tokenize(self.text)))
        assert_equal(blob.tokens, tb.WordList(["This is", "text."]))
        assert_equal(blob.tokenize(), tb.WordList(["This", "is", "text", "."]))
        assert_equal(blob.tokenize(tokenizer), tb.WordList(["This is", "text."]))
        assert_true(isinstance(w.synsets, (list, tuple)))
        assert_equal(blob.tokens, tb.WordList(["How now?", "Brown cow?"]))
        """Return a list all Python packages found within directory 'where'
            for name in os.listdir(where):
        for pat in list(exclude)+['ez_setup', 'distribute_setup']:
from .blob import TextBlob, Word, Sentence, Blobber, WordList
        '''Return a list of tuples of the form (word, tag)
    that returns a list of noun phrases as strings.
        '''Return a list of noun phrases (strings) for a body of text.'''
    that returns a list of noun phrases as strings.
        '''Return a list of tokens (strings) for a body of text.
        :rtype: list
        '''Return a list of word tokens.
        '''Return a list of sentences.'''
            return sentences  # return the 1-element list
node type for a potential parent; and the "right hand side" is a list
that specifies allowable children for that parent.  This lists
from nltk.probability import ImmutableProbabilisticMixIn
    Given a string containing a list of symbol names, return a list of
    :return: A list of ``Nonterminals`` constructed from the symbol
    :rtype: list(Nonterminal)
    if ',' in symbols: symbol_list = symbols.split(',')
    else: symbol_list = symbols.split()
    return [Nonterminal(s.strip()) for s in symbol_list]
            raise TypeError('production right hand side should be a list, '
    head word to an unordered list of one or more modifier words.
class WeightedProduction(Production, ImmutableProbabilisticMixIn):
    A probabilistic context free grammar production.
        ImmutableProbabilisticMixIn.__init__(self, **prob)
        :param productions: The list of productions that defines the grammar
        :type productions: list(Production)
        :return: A list of productions matching the given constraints.
        :rtype: list(Production)
        Check whether the grammar rules cover the given list of tokens.
        :type tokens: list(str)
        :param productions: The list of productions that defines the grammar
        :type productions: list(Production)
        :rtype: list(Production)
        :param productions: The list of productions that defines the grammar
        :type productions: list(Production)
    # def contains_exactly(self, head, modlist):
    #       if(len(production._rhs) == len(modlist)):
    #               set2 = Set(modlist)
    A probabilistic context-free grammar.  A Weighted Grammar consists
        :param productions: The list of productions that defines the grammar
        :type productions: list(Production)
    Induce a PCFG grammar from a list of productions.
    :param productions: The list of productions that defines the grammar
    :type productions: list(Production)
    Return a list of context-free ``Productions``.
        as a list of strings.
# Parsing Probabilistic CFGs
    Return a list of PCFG ``WeightedProductions``.
    return parse_production(input, standard_nonterm_parser, probabilistic=True)
    Return a probabilistic ``WeightedGrammar`` corresponding to the
        as a list of strings.
                                       probabilistic=True, encoding=encoding)
    Return a list of feature-based ``Productions``.
        as a list of strings.
def parse_production(line, nonterm_parser, probabilistic=False):
    a list of productions.
        if probabilistic and m:
    if probabilistic:
def parse_grammar(input, nonterm_parser, probabilistic=False, encoding=None):
    Return a pair consisting of a starting category and a list of
        as a list of strings.
    :param probabilistic: are the grammar rules probabilistic?
    :type probabilistic: bool
                productions += parse_production(line, nonterm_parser, probabilistic)
# suggest using NLTK's mailing list for Portuguese for any discussion.
# e/ou melhor para o portuguÃªs. TambÃ©m sugiro utilizar-se a lista de discussÃ£o
    # The rule list is static since it doesn't change between instances
                raise ValueError("%r has no list of stopwords. Please set"
    # and organized by length, are listed in tuples.
        return '<EMClusterer means=%s>' % list(self._means)
            vectors = list(map(self._normalise, vectors))
        :param leaf_labels: an optional list of strings to use for labeling the leaves
        :type leaf_labels: list
            child_left_leaf = list(map(lambda c: c.leaves(False)[0], node._children))
            indices = list(map(leaves.index, child_left_leaf))
        return list(range(self.num_clusters()))
    seq = list(zip(seq, [None]*len(seq)))
            self.assertEqual(list(v), file_data.split())
            self.assertEqual(list(v), self.linetok.tokenize(file_data))
                words = list(udhr.words(name))
from nose.suite import ContextList
from nose.util import tolist, anyp
                    yield ContextList((case,), context=fixture_context)
            if isinstance(case, ContextList):
                yield ContextList([self._patchTestCase(c) for c in case], case.context)
        self.extension = tolist(options.doctestExtension)
    def _find_top_nodes(self, node_list):
        top_nodes = node_list.copy()
                if arg in node_list:
        Each element of `queue' is a tuple of the node to plug and the list of
        `record' is a list of all the complete pluggings that we have found in
        # Add the current hole we're trying to plug into the list of ancestors.
        readings = list(map(hole_sem.formula_tree, pluggings))
etc. A ``Valuation`` is initialized with a list of (symbol, value)
        :param input: list of str Input sentences to parse as a single discourse
        :param inputs: list of str Input sentences to parse as individual discourses
        :param discourse_ids: list of str Identifiers to be inserted to each occurrence-indexed predicate.
        :return: list of ``drt.AbstractDrs``
        :param inputs: list of list of str Input discourses to parse
        :param discourse_ids: list of str Identifiers to be inserted to each occurrence-indexed predicate.
            discourse_ids = list(map(str, range(len(inputs))))
        :param inputs: list of list of str Input discourses to parse
        :param discourse_ids: list of str Identifiers to be inserted to each occurrence-indexed predicate.
        :param args: A list of command-line arguments.
        :return: list of ``AbstractDrs``
        :param indices: list of int
        :return: list of ``AbstractDrs``
        (sent_index, word_indices), = self._sent_and_word_indices(self._parse_index_list())
        (sent_index, word_indices), = self._sent_and_word_indices(self._parse_index_list())
        (sent_index, word_indices), = self._sent_and_word_indices(self._parse_index_list())
        (sent_index, word_indices), = self._sent_and_word_indices(self._parse_index_list())
        self._parse_index_list()
        self._parse_index_list()
        self._parse_index_list()
    def _parse_index_list(self):
            indices = self._parse_index_list()
            indices = self._parse_index_list()
        return BoxerDrs(label, list(refs), conds)
        :return: list of (sent_index, word_indices) tuples
                refs = list(map(int, self.handle_refs()))
                word_ids = list(map(int, self.handle_refs()))
                word_ids = list(map(int, self.handle_refs()))
                word_ids = list(map(int, self.handle_refs()))
                word_ids = list(map(int, self.handle_refs()))
                word_ids = list(map(int, self.handle_refs()))
            drs = DRS([Variable('x%d' % r) for r in ex.refs], list(map(self.interpret, ex.conds)))
it uses a list rather than a string in the second field.
According to this, the file ``city['filename']`` contains a list of
        :type altLabels: list
            (list items can be ``symmetric``, ``reflexive``, ``transitive``)
        :type closures: list
        #public access is via a list (for slicing)
        self.extension = sorted(list(extension))
        self.extension = sorted(list(self._extension))
        Convert a set of pairs into an adjacency linked list encoding of a graph.
        Compute the transitive closure of a graph represented as a linked list.
        Convert an adjacency linked list back into a set of pairs.
        self.extension = sorted(list(self._extension))
    Convert a file of Prolog clauses into a list of ``Concept`` objects.
    :type schema: list
    :type closures: list
    :return: a list of ``Concept`` objects
    :rtype: list
    # convert a file into a list of lists
    Read a file into memory and convert each relation clause into a list.
    A record is a list of entities in some relation, such as
    :param records: a list of records
    :type records: list of lists
    A record is a list of entities in some relation, such as
    :type closures: list
    :param records: a list of records
    :type records: list of lists
    Given a list of relation metadata bundles, make a corresponding
    :type rels: list of dict
        concept_list = clause2concepts(filename, rel_name, schema, closures)
        for c in concept_list:
    Convert a list of ``Concept`` objects into a list of (label, extension) pairs;
    :type concepts: list(Concept)
    :rtype: list or Valuation
    Make a ``Valuation`` from a list of relation metadata bundles and dump to
    :type rels: list of dict
    # convert the domain into a sorted list of alphabetic terms
    :param symbols: a list of individual constants in the semantic representation
    :rtype: list
    Build a list of concepts corresponding to the relation names in ``items``.
    :type items: list of strings
    :rtype: list
        Append 'item' to the list at 'key'.  If no list exists for 'key', then
    def to_glueformula_list(self, glue_dict):
        return glue_dict.to_glueformula_list(depgraph)
        nodelist = depgraph.nodelist
        self._to_depgraph(nodelist, 0, 'ROOT')
        for node_addr, node in enumerate(nodelist):
            for n2 in nodelist[1:]:
        depgraph.root = nodelist[1]
    def _to_depgraph(self, nodelist, head, rel):
        index = len(nodelist)
        nodelist.append({'address': index,
                    item._to_depgraph(nodelist, index, feature)
                    nodelist.append({'address': len(nodelist),
                elif isinstance(item, list):
                        n._to_depgraph(nodelist, index, feature)
                    raise Exception('feature %s is not an FStruct, a list, or a tuple' % feature)
            children = [depgraph.nodelist[idx] for idx in node['deps']]
        :param value: where to index into the list of characters
                elif isinstance(item, list):
                    raise Exception('feature %s is not an FStruct, a list, or a tuple' % feature)
        :return: list of ``Variable`` objects
        :param refs: list of ``DrtIndividualVariableExpression`` for the
        :param conds: list of ``Expression`` for the conditions
        parts = list(map(function, self.conds))
        return combinator(self.refs, list(map(function, self.conds)), consequent)
                drs = DRS(list(set(drs.refs)-set([cond.second.variable])),
        length = max([len(refs_line)] + list(map(len, cond_lines)))
        first_second_lines = list(zip(first_lines, second_lines))
        func_args_lines = list(zip(function_lines, list(zip(*args_lines))))
class PossibleAntecedents(list, AbstractDrs, Expression):
                               [(x,1) for x in DrtTokens.LAMBDA_LIST]             + \
                               [(x,2) for x in DrtTokens.NOT_LIST]                + \
                               [(x,4) for x in DrtTokens.EQ_LIST+Tokens.NEQ_LIST] + \
                               [(x,7) for x in DrtTokens.OR_LIST]                 + \
                               [(x,8) for x in DrtTokens.IMP_LIST]                + \
        if tok in DrtTokens.NOT_LIST:
        elif tok in DrtTokens.LAMBDA_LIST:
        elif tok in DrtTokens.OR_LIST:
        elif tok in DrtTokens.IMP_LIST:
    :type inputs: list of str
    :return: a mapping from input sentences to a list of ``Tree``s
    :param inputs: a list of sentences
    :return: a mapping from sentences to lists of pairs (parse-tree, semantic-representations)
    :param inputs: a list of sentences
    :return: a mapping from sentences to lists of triples (parse-tree, semantic-representations, evaluation-in-model)
    LAMBDA = '\\';     LAMBDA_LIST = ['\\']
    EXISTS = 'exists'; EXISTS_LIST = ['some', 'exists', 'exist']
    ALL = 'all';       ALL_LIST = ['all', 'forall']
    NOT = '-';         NOT_LIST = ['not', '-', '!']
    AND = '&';         AND_LIST = ['and', '&', '^']
    OR = '|';          OR_LIST = ['or', '|']
    IMP = '->';        IMP_LIST = ['implies', '->', '=>']
    IFF = '<->';       IFF_LIST = ['iff', '<->', '<=>']
    EQ = '=';          EQ_LIST = ['=', '==']
    NEQ = '!=';        NEQ_LIST = ['!=']
    BINOPS = AND_LIST + OR_LIST + IMP_LIST + IFF_LIST
    QUANTS = EXISTS_LIST + ALL_LIST
    TOKENS = BINOPS + EQ_LIST + NEQ_LIST + QUANTS + LAMBDA_LIST + PUNCT + NOT_LIST
        for v in list(univ_scope):
        :return: A list of all variables in this object.
        sig = defaultdict(list)
        :param signature: dict(str -> list(AbstractVariableExpression))
        :param combinator: ``Function<list<T>,R>`` to combine the results of the
    ``ConstantExpression`` as the predicate and a list of Expressions as the
            signature = defaultdict(list)
            return list(unique)[0]
        return: A tuple (base-function, arg-list)
        Return uncurried arg-list
            signature = defaultdict(list)
            signature = defaultdict(list)
            signature = defaultdict(list)
            signature = defaultdict(list)
            signature = defaultdict(list)
            signature = defaultdict(list)
            signature = defaultdict(list)
            signature = defaultdict(list)
        """A list of tuples of quote characters.  The 4-tuple is comprised
                           [(x,1) for x in Tokens.LAMBDA_LIST]             + \
                           [(x,2) for x in Tokens.NOT_LIST]                + \
                           [(x,4) for x in Tokens.EQ_LIST+Tokens.NEQ_LIST] + \
                           [(x,6) for x in Tokens.AND_LIST]                + \
                           [(x,7) for x in Tokens.OR_LIST]                 + \
                           [(x,8) for x in Tokens.IMP_LIST]                + \
                           [(x,9) for x in Tokens.IFF_LIST]                + \
        elif tok in Tokens.NOT_LIST:
        elif tok in Tokens.LAMBDA_LIST:
        if tok in Tokens.EXISTS_LIST:
        elif tok in Tokens.ALL_LIST:
            if tok in Tokens.EQ_LIST + Tokens.NEQ_LIST and self.has_priority(tok, context):
                if tok in Tokens.NEQ_LIST:
        if tok in Tokens.AND_LIST:
        elif tok in Tokens.OR_LIST:
        elif tok in Tokens.IMP_LIST:
        elif tok in Tokens.IFF_LIST:
        a list of arguments in parens, then the argument expression is a
        if isinstance(expected, list):
        if isinstance(expected, list):
    Convert a file of First Order Formulas into a list of {Expression}s.
    :return: a list of parsed formulas.
    :rtype: list(Expression)
    Join a list into a string, turning tags tuples into tag strings or just words.
    :type lst: list
    #new = list2sym(l)
def list2sym(lst):
    Convert a list of strings into a canonical symbol.
    :type lst: list
    Group a chunk structure into a list of pairs of the form (list(str), ``Tree``)
    identifies pairs whose first member is a list (possibly empty) of terminal
    :return: a list of pairs (list(str), ``Tree``)
    :rtype: list of tuple
    :param pairs: a pair of list(str) and ``Tree``, as generated by
    :rtype: list(defaultdict)
        reldict['subjsym'] = list2sym(pairs[0][1].leaves())
        reldict['objsym'] = list2sym(pairs[1][1].leaves())
    :type doc: ieer document or a list of chunk trees
    :rtype: list(defaultdict)
    return list(filter(relfilter, reldicts))
    return len(list(rel)[0])
    is not in its list of symbols.
        :param iter: a list of (symbol, value) pairs.
    :param assign: a list of (varname, value) associations
    :type assign: list
        list = []
            list.append(pair)
        self.variant = list
        :return: An iterator over the permutations of the input list
        :type lst: list
        Each permutation of the store (i.e. list of binding operators) is
        :param dependencies: list of int for the indices on which this atom is dependent
        :return: (``Expression``,list of ``GlueFormula``) for the compiled linear logic and any newly created glue formulas
            list [(``VariableExpression``, ``AtomicExpression``)] to initialize the dictionary
                        ' VariableBindingsLists: %s, %s' % (self, other))
                            glue_formulas.append([meaning_term, glue_term])     # add the GlueFormula to the list
    def to_glueformula_list(self, depgraph, node=None, counter=None, verbose=False):
            top = depgraph.nodelist[0]
            root = depgraph.nodelist[top['deps'][0]]
            return self.to_glueformula_list(depgraph, root, Counter(), verbose)
            dep = depgraph.nodelist[dep_idx]
            glueformulas.extend(self.to_glueformula_list(depgraph, dep, counter, verbose))
            headnode = depgraph.nodelist[node['head']]
        relationships = frozenset(depgraph.nodelist[dep]['rel'].lower()
                                   if depgraph.nodelist[dep]['rel'].lower()
        Based on the node, return a list of plausible semtypes in order of
                return self.find_label_name(after_dot, depgraph.nodelist[node['head']], depgraph, unique_index)
            elif name=='super': return self.get_label(depgraph.nodelist[node['head']])
        :param value: where to index into the list of characters
        deps = [depgraph.nodelist[dep] for dep in node['deps']
                if depgraph.nodelist[dep]['rel'].lower() == rel.lower()]
                    self._add_to_reading_list(gf, readings)
                    self._add_to_reading_list(gf, readings)
    def _add_to_reading_list(self, glueformula, reading_list):
            for reading in reading_list:
            reading_list.append(glueformula.meaning)
        return self.get_glue_dict().to_glueformula_list(depgraph)
        return_list = []
            return_list.extend(gf.compile(index_counter))
            for cgf in return_list:
        return return_list
                     Listbox, Menu, Scrollbar, Tk)
        self._init_exampleListbox(self._top)
        self._init_readingListbox(self._top)
    def _init_exampleListbox(self, parent):
        self._exampleFrame = listframe = Frame(parent)
        self._exampleList_label = Label(self._exampleFrame, font=self._boldfont,
        self._exampleList_label.pack()
        self._exampleList = Listbox(self._exampleFrame, selectmode='single',
        self._exampleList.pack(side='right', fill='both', expand=1)
            self._exampleList.insert('end', ('  %s' % example))
        self._exampleList.config(height=min(len(self._examples), 25), width=40)
            listscroll = Scrollbar(self._exampleFrame,
            self._exampleList.config(yscrollcommand = listscroll.set)
            listscroll.config(command=self._exampleList.yview)
            listscroll.pack(side='left', fill='y')
        self._exampleList.bind('<<ListboxSelect>>', self._exampleList_select)
    def _init_readingListbox(self, parent):
        self._readingFrame = listframe = Frame(parent)
        self._readingList_label = Label(self._readingFrame, font=self._boldfont,
        self._readingList_label.pack()
        self._readingList = Listbox(self._readingFrame, selectmode='single',
        self._readingList.pack(side='right', fill='both', expand=1)
        listscroll = Scrollbar(self._readingFrame,
        self._readingList.config(yscrollcommand = listscroll.set)
        listscroll.config(command=self._readingList.yview)
        listscroll.pack(side='right', fill='y')
        self._populate_readingListbox()
    def _populate_readingListbox(self):
        # Populate the listbox with integers
        self._readingList.delete(0, 'end')
            self._readingList.insert('end', ('  %s' % (i+1)))
        self._readingList.config(height=min(len(self._readings), 25), width=5)
        self._readingList.bind('<<ListboxSelect>>', self._readingList_select)
        selection = self._readingList.curselection()
        readingListSize = self._readingList.size()
        if readingListSize > 0:
                    self._readingList_store_selection(index-1)
                self._readingList_store_selection(readingListSize-1)
            self._exampleList_store_selection(self._curExample-1)
            self._exampleList_store_selection(len(self._examples)-1)
        selection = self._readingList.curselection()
        readingListSize = self._readingList.size()
        if readingListSize > 0:
                if index >= (readingListSize-1):
                    self._readingList_store_selection(index+1)
                self._readingList_store_selection(0)
            self._exampleList_store_selection(self._curExample+1)
            self._exampleList_store_selection(0)
        self._exampleList.selection_clear(0, 'end')
        self._populate_readingListbox()
    def _exampleList_select(self, event):
        selection = self._exampleList.curselection()
        self._exampleList_store_selection(int(selection[0]))
    def _exampleList_store_selection(self, index):
        self._exampleList.selection_clear(0, 'end')
                if isinstance(cache, list):
                    self._exampleList.delete(index)
                    self._exampleList.insert(index, ('  %s *' % example))
                    self._exampleList.config(height=min(len(self._examples), 25), width=40)
            self._populate_readingListbox()
            self._exampleList.selection_set(index)
    def _readingList_select(self, event):
        selection = self._readingList.curselection()
        self._readingList_store_selection(int(selection[0]))
    def _readingList_store_selection(self, index):
        self._readingList.selection_clear(0, 'end')
            self._readingList.selection_set(index)
        :rtype: list
        :rtype: list of ``ProbDistI``
        y_proba_list = self._clf.predict_proba(X)
        return [self._make_probdist(y_proba) for y_proba in y_proba_list]
        :rtype: list
        return list(self._encoder.classes_)
        :param labeled_featuresets: A list of ``(featureset, label)``
        X, y = list(compat.izip(*labeled_featuresets))
(stored as a list of words) to a featureset describing the set of
Most classifiers are built by training them on a list of hand-labeled
as lists of ``(featuredict, label)`` tuples.
    >>> positive_featuresets = list(map(features, sports_sentences))
    >>> unlabeled_featuresets = list(map(features, various_sentences))
        :param positive_featuresets: A list of featuresets that are known as positive
        :param unlabeled_featuresets: A list of featuresets whose label is unknown.
        :type weights: list of float
        :type new_weights: list of float
        :rtype: list of float
        fids = sorted(list(range(len(self._weights))),
    #: A list of the algorithm names that are accepted for the
        :type train_toks: list
        :param train_toks: Training data, represented as a list of
        :type labels: list(str)
    represented as a list of ``(index, value)`` tuples, specifying the
        a list of ``(index, value)`` tuples, specifying the value of
        :rtype: list(tuple(int, int))
        :return: A list of the \"known labels\" -- i.e., all labels
        :rtype: list
        :type train_toks: list(tuple(dict, str))
        :param train_toks: Training data, represented as a list of
             list of ``(index,value)`` tuples.
        :type labels: list
        :param labels: A list of the \"known labels\" for this
        :param labels: A list of the \"known labels\" for this encoding.
        self._labels = list(labels)
        """A list of attested labels."""
        :type train_toks: list(tuple(dict, str))
        :param train_toks: Training data, represented as a list of
        :type labels: list
        :param labels: A list of labels that should be used by the
        train_toks = list(train_toks)
        :param labels: A list of the \"known labels\" for this encoding.
        self._labels = list(labels)
        """A list of attested labels."""
        :type train_toks: list(tuple(dict, str))
        :param train_toks: Training data, represented as a list of
        :type labels: list
        :param labels: A list of labels that should be used by the
    # nfmap compresses this sparse set of values to a dense list.
    that are attested for at least one token in the given list of
    :type train_toks: list(tuple(dict, str))
    :type train_toks: list(tuple(dict, str))
    :param train_toks: Training data, represented as a list of
        all joint features have binary values, and are listed iff they
        are true.  Otherwise, list feature values explicitly.  If
        list the features that would fire for any of the possible
        # For implicit file formats, just list the features that fire
        # For explicit formats, list the features that would fire for
        raise TypeError('args should be a list of strings')
    :type train_toks: list(tuple(dict, str))
    :param train_toks: Training data, represented as a list of
        raise TypeError('args should be a list of strings')
        self._labels = list(label_probdist.samples())
        for fname in list(featureset.keys()):
        Return a list of the 'most informative' features used by this
        # Convert features to a list, & sort it by how informative
        :param labeled_featuresets: A list of classified featuresets,
            i.e., a list of tuples ``(featureset, label)``.
                # Keep a list of all feature names.
    Use the ``LazyMap`` class to construct a lazy list-like
    particular, if ``labeled=False``, then the returned list-like
    If ``labeled=True``, then the returned list-like object's values
    significant when the underlying list of tokens is itself lazy (as
    :param toks: The list of tokens to which ``feature_func`` should be
        applied.  If ``labeled=True``, then the list elements will be
        then the list elements should be tuples ``(tok,label)``, and
        labeled = toks and isinstance(toks[0], (tuple, list))
    :return: A list of all labels that are attested in the given list
    :rtype: list of (immutable)
    :param tokens: The list of classified tokens from which to extract
    :type tokens: list
    # Construct a list of classified names, using the names corpus.
    namelist = ([(name, 'male') for name in names.words('male.txt')] +
    random.shuffle(namelist)
    train = namelist[:5000]
    test = namelist[5000:5500]
        for ((name, gender), pdist) in list(zip(test, pdists))[:5]:
    # Create a list of male names to be used as positive-labeled examples for training
    # Create a list of male and female names to be used as unlabeled examples
    senses = list(set(l for (i,l) in instances))
        return list(set(labels))
                                         list(self._decisions.keys())[0])
        # Collect a list of all feature names.
        # Collect a list of the values each feature can take.
                               list(best_stump._decisions.keys())[0])
            for f in os.listdir(temp_dir):
    # [xx] full list of classifiers (some may be abstract?):
            cmd += list(options)
            for f in os.listdir(temp_dir):
        :param labels: A list of all class labels that can be generated.
        :param features: A list of feature specifications, where
        """Returns the list of classes."""
        return list(self._labels)
        :param tokens: a list of featuresets (dicts) or labelled featuresets
            labeled if the first token's value is a tuple or list.
            labeled = tokens and isinstance(tokens[0], (tuple, list))
                                  for filename in sorted(os.listdir(lib_dir))
        :return: the list of category labels used by this classifier.
        :rtype: list of (immutable)
        :rtype: list(label)
        :rtype: list(ProbDistI)
        :return: the list of category labels used by this classifier.
        :rtype: list of (immutable)
        :rtype: list(set(label))
        :rtype: list(ProbDistI)
#         :return: the list of category labels used by this classifier.
#         :rtype: list of (immutable)
#         If ``featureset`` is a list of featuresets, then return a
#         corresponding list containing the probability distribution
#         *i*\ th element of this list is the most appropriate label for
#         If ``featureset`` is a list of featuresets, then return a
#         corresponding list containing the most appropriate label for
#         this list is the most appropriate label for the *i*\ th element
Tkinter widgets for displaying multi-column listboxes and tables.
from tkinter import (Frame, Label, Listbox, Scrollbar, Tk)
# Multi-Column Listbox
class MultiListbox(Frame):
    A multi-column listbox, where the current selection applies to an
    entire row.  Based on the MultiListbox Tkinter widget
    For the most part, ``MultiListbox`` methods delegate to its
    contained listboxes.  For any methods that do not have docstrings,
    see ``Tkinter.Listbox`` for a description of what that method does.
    #: Default configuration for the column listboxes.
    LISTBOX_CONFIG = dict(borderwidth=1,
        Construct a new multi-column listbox widget.
            multi-column listbox.
            the new multi-column listbox.  If ``columns`` is an integer,
            a list, then its length indicates the number of columns
            to include; and each element of the list will be used as
            Use ``label_*`` to configure all labels; and ``listbox_*``
            to configure all listboxes.  E.g.:
                >>> mlb = MultiListbox(master, 5, label_foreground='red')
        # If columns was specified as an int, convert it to a list.
            columns = list(range(columns))
        self._listboxes = []
            # Create a listbox for the column
            lb = Listbox(self, **self.LISTBOX_CONFIG)
            self._listboxes.append(lb)
            # the default listbox behavior, which scrolls):
            for i, lb in enumerate(self._listboxes):
        lb = self._listboxes[self._resize_column_index]
        multi-column listbox.
        listbox was created without labels, then this will be an empty
    def listboxes(self):
        A tuple containing the ``Tkinter.Listbox`` widgets used to
        return tuple(self._listboxes)
        for lb in self._listboxes:
        past the top or the bottom of the list.
        labels; and ``listbox_*`` to configure all listboxes.  E.g.:
                >>> mlb = MultiListbox(master, 5)
                >>> mlb.configure(listbox_foreground='red')
        cnf = dict(list(cnf.items()) + list(kw.items()))
        for (key, val) in list(cnf.items()):
            elif key.startswith('listbox_') or key.startswith('listbox-'):
                for listbox in self._listboxes:
                    listbox.configure({key[8:]: val})
        for lb in self._listboxes: lb.itemconfigure(row_index, cnf, **kw)
        lb = self._listboxes[col_index]
        cnf = dict(list(cnf.items()) + list(kw.items()))
        for (key, val) in list(cnf.items()):
        lb = self._listboxes[col_index]
        ``Tkinter.Listbox``.
        for (lb,elts) in zip(self._listboxes, list(zip(*rows))):
        return a list of row values.  Each row value is a tuple of
        values = [lb.get(first, last) for lb in self._listboxes]
        x, y, w, h = self._listboxes[col].bbox(row)
        self.listboxes[col_index].grid_forget()
        self._listboxes[col_index].grid(column=col_index, row=1,
        mult-column listbox that will call ``func`` in response to the
        :return: A list of the identifiers of replaced binding
    def bind_to_listboxes(self, sequence=None, func=None, add=None):
        Add a binding to each ``Tkinter.Listbox`` widget in this
        mult-column listbox that will call ``func`` in response to the
        :return: A list of the identifiers of replaced binding
        for listbox in self.listboxes:
            listbox.bind(sequence, func, add)
        Add a binding to each ``Tkinter.Label`` and ``Tkinter.Listbox``
        widget in this mult-column listbox that will call ``func`` in
        :return: A list of the identifiers of replaced binding
                self.bind_to_listboxes(sequence, func, add))
    # These methods delegate to the first listbox:
        return self._listboxes[0].curselection(*args, **kwargs)
        return self._listboxes[0].selection_includes(*args, **kwargs)
        return self._listboxes[0].itemcget(*args, **kwargs)
        return self._listboxes[0].size(*args, **kwargs)
        return self._listboxes[0].index(*args, **kwargs)
        return self._listboxes[0].nearest(*args, **kwargs)
    # These methods delegate to each listbox (and return None):
        for lb in self._listboxes: lb.activate(*args, **kwargs)
        for lb in self._listboxes: lb.delete(*args, **kwargs)
        for lb in self._listboxes: lb.scan_mark(*args, **kwargs)
        for lb in self._listboxes: lb.scan_dragto(*args, **kwargs)
        for lb in self._listboxes: lb.see(*args, **kwargs)
        for lb in self._listboxes: lb.selection_anchor(*args, **kwargs)
        for lb in self._listboxes: lb.selection_clear(*args, **kwargs)
        for lb in self._listboxes: lb.selection_set(*args, **kwargs)
        for lb in self._listboxes: v = lb.yview(*args, **kwargs)
        for lb in self._listboxes: lb.yview_moveto(*args, **kwargs)
        for lb in self._listboxes: lb.yview_scroll(*args, **kwargs)
    # These listbox methods are not defined for multi-listbox
    A display widget for a table of values, based on a ``MultiListbox``
    list-of-lists.  E.g., table[i] is a list of the values for row i;
    list-of-lists.
    :ivar _mlb: The multi-column listbox used to display this table's data.
    :ivar _rows: A list-of-lists used to hold the cell values of this
        table.  Each element of _rows is a row value, i.e., a list of
        :type column_names: list(str)
        :param column_names: A list of names for the columns; these
        :type rows: list(list)
        :param rows: A list of row values used to initialze the table.
            contained ``MultiListbox``.  See ``MultiListbox.__init__()``
        # Create our multi-list box.
        self._mlb = MultiListbox(self._frame, column_names,
            self._mlb.listboxes[0]['yscrollcommand'] = sb.set
            #for listbox in self._mlb.listboxes:
            #    listbox['yscrollcommand'] = sb.set
        # Fill in our multi-list box.
        """:see: ``MultiListbox.rowconfigure()``"""
        """:see: ``MultiListbox.columnconfigure()``"""
        """:see: ``MultiListbox.itemconfigure()``"""
        """:see: ``MultiListbox.bind_to_labels()``"""
    def bind_to_listboxes(self, sequence=None, func=None, add=None):
        """:see: ``MultiListbox.bind_to_listboxes()``"""
        return self._mlb.bind_to_listboxes(sequence, func, add)
        """:see: ``MultiListbox.bind_to_columns()``"""
    #{ Table as list-of-lists
        :param rowvalues: A list of row values used to initialze the
            self._mlb.listboxes[j].insert(i, val)
            self._mlb.listboxes[j].delete(i+1)
            self._rows[index] = list(val)
        """A list of the names of the columns in this table."""
        """:see: ``MultiListbox.hide_column()``"""
        """:see: ``MultiListbox.show_column()``"""
        """:see: ``MultiListbox.select()``"""
        multi-column listbox; and then filling it in with values from
            row_indices = list(range(len(self._rows)))
        the contents of its multi-listbox (``_mlb``).  This is just
        list-modifying operations are working correctly.
        for col in self._mlb.listboxes:
    from .cfg import ProductionList, CFGEditor, CFGDemo
    :type text: list(str) or enum(str)
    :type words: list of str
    text = list(text)
        words_to_comp = list(map(str.lower, words))
        text_to_comp = list(map(str.lower, text))
        x, y = list(zip(*points))
    pylab.yticks(list(range(len(words))), words, color="b")
    For a list of the attributes supported by a type of canvas widget,
      - ``_tags``: Returns a list of the canvas tags for all graphical
    :type __children: list(CanvasWidget)
        for (attr, value) in list(attribs.items()): self[attr] = value
        :return: A list of the hierarchical children of this canvas
        :rtype: list of CanvasWidget
        :return: a list of the canvas tags for all graphical
        :rtype: list of int
        class documentation for a list of attributes supported by this
            documentation for a list of attributes supported by this
          - It adds ``child`` to the list of canvas widgets returned by
          - It removes ``child`` from the list of canvas widgets
        :return: a list of canvas tags for all graphical elements
        :rtype: list of int
            for k,v in list(SymbolWidget.SYMBOLS.items()):
    A canvas widget that keeps a list of canvas widgets in a
        :type children: list(CanvasWidget)
        self._children = list(children)
    A canvas widget that keeps a list of canvas widgets in a vertical
        :type children: list(CanvasWidget)
        self._children = list(children)
        :type children: list(CanvasWidget)
##  Colorized List
class ColorizedList(object):
    An abstract base class for displaying a colorized list of items.
        will be used by the list.
      - ``_item_repr``, which returns a list of (text,colortag)
        Construct a new list.
        :param parent: The Tk widget that contains the colorized list
        :param items: The initial contents of the colorized list.
        Set up any colortags that will be used by this colorized list.
        Return a list of (text, colortag) tuples that make up the
        :return: A list of the items contained by this list.
        Modify the list of items contained by this list.
        items = list(items)
        self._items = list(items)
        :raise ValueError: If ``item`` is not contained in the list.
        :raise ValueError: If ``item`` is not contained in the list.
        :raise ValueError: If ``item`` is not contained in the list.
        Register a callback function with the list.  This function
        if event is None: events = list(self._callbacks.keys())
        for cb_func in list(self._callbacks[event].keys()):
        :type subtrees: list(CanvasWidgetI)
        # Return a list of the nodes we moved
        # Return a list of the nodes we moved
    for (key, value) in list(attribs.items()):
        :param path_to_tree: A list of indices i1, i2, ..., in, where
        :param path_to_tree: A list of indices i1, i2, ..., in, where
        for tseg in list(self._expanded_trees.values()):
        for tseg in list(self._collapsed_trees.values()):
        for tseg in list(self._expanded_trees.values()):
        for tseg in list(self._collapsed_trees.values()):
            for tseg in list(self._expanded_trees.values()): tseg['color'] = value
            for tseg in list(self._expanded_trees.values()): tseg['width'] = value
            for tseg in list(self._collapsed_trees.values()): tseg['width'] = value
            for tseg in list(self._collapsed_trees.values()): tseg['color'] = value
            for tseg in list(self._collapsed_trees.values()): tseg['fill'] = value
            for tseg in list(self._expanded_trees.values()):
            for tseg in list(self._collapsed_trees.values()):
            for tseg in list(self._expanded_trees.values()):
            for tseg in list(self._collapsed_trees.values()):
            for tseg in list(self._expanded_trees.values()):
            for tseg in list(self._collapsed_trees.values()):
            for tseg in list(self._expanded_trees.values()):
            for tseg in list(self._collapsed_trees.values()):
            for tseg in list(self._expanded_trees.values()):
            for tseg in list(self._collapsed_trees.values()):
        segs = list(self._expanded_trees.values()) + list(self._collapsed_trees.values())
#     - grammar is a list of productions
from nltk.draw.util import (CanvasFrame, ColorizedList, ShowText,
# Production List
class ProductionList(ColorizedList):
A context free grammar consists of a start symbol and a list of
the upper right hand corner of the editor; and the list of productions
is a list of nonterminals and terminals.
        # own tag, so they aren't listed here.
        a list of productions.
        self._prodlist = ProductionList(parent, self._grammar, width=20)
        self._prodlist.pack(side='top', fill='both', expand=1)
        self._prodlist.focus()
        self._prodlist.add_callback('select', self._selectprod_cb)
        self._prodlist.add_callback('move', self._selectprod_cb)
        self._prodlist.highlight(production)
    p = ProductionList(t, productions)
This package defines several taggers, which take a token list (typically a
sentence), assign a tag to each token, and return the resulting list of
    tag the given list of tokens.
    :type tokens: list(str)
    :rtype: list(tuple(str, str))
    given list of sentences, each consisting of a list of tokens.
a probabilistic function of the state. HMMs share the Markov chain's
        self._symbols = list(set(symbols))
        self._states = list(set(states))
        symbols = list(set(word for sent in labeled_sequence
        tag_set = list(set(tag for sent in labeled_sequence
            i.e. a list of sentences represented as tuples
        :type labeled_sequence: list(list)
        :type test_sequence: list(list)
            i.e. a list of sentences represented as words
        :type unlabeled_sequence: list(list)
        :rtype: list
        :type unlabeled_sequence: list
        return list(izip(unlabeled_sequence, path))
        :type unlabeled_sequence: list
        return list(map(self._states.__getitem__, sequence))
        :type unlabeled_sequence: list
        :rtype:         list
        :type unlabeled_sequence: list
        :type unlabeled_sequence: list
        :type test_sequence: list(list)
            return list(itertools.chain(*seq))
        predicted_sequence = list(imap(self._tag, imap(words, test_sequence)))
        :type labelled_sequences: list
        :type unlabeled_sequences: list
        :type unlabeled_sequences: list
                sequence = list(sequence)
        :type labelled_sequences: list
                # update the state and symbol lists
    return cleaned_sentences, list(tag_set), list(symbols)
    trainer = HiddenMarkovModelTrainer(tag_set, list(symbols))
    - List of the operations needed to be performed.
        Applies the specified operation(s) on a list of tokens.
        Applies the tag method over a list of sentences. This method will return a
        list of dictionaries. Every dictionary will contain a word with its
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
        :type corpus: list(tuple(str, str))
        :param corpus: Training data, represented as a list of
            sentences, where each sentence is a list of (token, tag) tuples.
        :type weight_groups: list(CRFInfo.WeightGroup)
        state_info_list = []
        # Construct a list of state descriptions.  Currently, we make
            state_info_list.append(state_info)
        return CRFInfo(state_info_list, gaussian_variance,
            :param weightgroups: A list of WeightGroup names, indicating
                this weight group, specified as either a list of state
                for this weight group, specified as either a list of state
                for this weight group, specified as either a list of
      - Input for training is expected to be a list of sentences
        where each sentence is a list of (word, tag) tuples
        Input for tagdata function is a list of sentences
        :param data: List of lists of (word, tag) tuples
        Tags each sentence in a list of sentences
        :param data:list of list of words
        :return: list of list of (word, tag) tuples
        compiles the results into a list of tagged sentences
        each tagged sentence is a list of (word, tag) tuples
        :param data: list of words
        to produce a list of tags
        returns a list of (word, tag) tuples
        sent = list(data)
        :param sent : List of words remaining in the sentence
        :param current_states : List of possible tag combinations for
                [(_w, t)] = list(self._unk.tag([word]))
        # return the best list of tags for the sentence
    :param data: list of tokens (words or (word, tag) tuples)
                as a list of words or a list of tagged words
    :return: list of sentences
             sentences are a list of tokens
    Function takes a list of tokens and separates the tokens into lists
    where each list represents a sentence fragment
    sents = list(brown.tagged_sents())
    test = list(brown.sents())
    d = list(treebank.tagged_sents())
    d = list(treebank.tagged_sents())
    e = list(brown.tagged_sents())
    sentence.  I.e., return a list containing the first element
improves the tagging by applying a list of transformation rules.
    >>> brown_train = list(brown.tagged_sents(categories='news')[:500])
    >>> brown_test = list(brown.tagged_sents(categories='news')[500:600])
    tag sequence to a text; and then apply an ordered list of
    a list of transformational rules; but more often, Brill taggers
        :param rules: An ordered list of transformation rules that
        :type rules: list(BrillRule)
        # Create a dictionary that maps each tag to a list of the
        :type tokens: list(tuple(str, str))
        :type positions: list(int)
            positions = list(range(len(tokens)))
        :type tokens: list(str)
    :param conditions: A list of 3-tuples (start, end, value),
            conditions=list(list(x) for x in data._conditions),
    An interface for generating lists of transformational rules that
        Return a list of the transformational rules that would correct
        return a list of zero or more rules that would change
        ``applicable_rules()`` should return the empty list.
        :type tokens: list(tuple)
        :rtype: list(BrillRule)
        :type token: list(tuple)
    A brill template that generates a list of
    parameterized by a proximate token brill rule class and a list of
      - use the given list of boundaries as the start and end
    :param boundaries: A list of (start, end) tuples each of
        (start, end).  I.e., return a list of all tuples
    :param boundaries: A list of tuples (start, end), each of
    # Generates lists of a subtype of ProximateTokensRule.
        :rtype: list of ProximateTokensRule
        :type train_sents: list(list(tuple))
                    # Add the rule to our list of rules.
        # Create a dictionary mapping from each tag to a list of the
        correct_indices = defaultdict(list)
        :return: A list of tuples ``(rule, fixscore)``, where rule
        # Create a list of all indices that are incorrectly tagged.
        # Convert the dictionary into a list of (rule, score) tuples,
        """Mapping from tags to lists of positions that use that tag."""
           extend this list to also include positions where each rule
        test_sents = [list(self._initial_tagger.tag(untag(sent)))
                # Find the best rule, and add it to our rule list.
        self._tag_positions = defaultdict(list)
            best_rules = list(self._rules_by_score[max_score])
        # Collect a list of all positions that might be affected.
# returns a list of errors in string format
def error_list (train_sents, test_sents, radius=2):
    Returns a list of human-readable strings indicating the errors in the
    :type train_sents: list(tuple)
    :type test_sents: list(tuple)
    for e in error_list(gold_data, testing_data):
    :ivar _taggers: A list of all the taggers that should be tried to
        return list(zip(tokens, tags))
        :type tokens: list
        :param tokens: The list of words that are being tagged.
        :type history: list(str)
        :param history: A list of the tags for all words before *index*.
        :type tokens: list
        :param tokens: The list of words that are being tagged.
        :type history: list(str)
        :param history: A list of the tags for all words before *index*.
            a list of (word, tag tuples.
        >>> list(default_tagger.tag('This is a test'.split()))
    :param train: A tagged corpus consisting of a list of tagged
        sentences, where each sentence is a list of (word, tag) tuples.
    :param train: The corpus of training data, a list of tagged sentences
    :type train: list(list(tuple(str, str)))
    :param train: The corpus of training data, a list of tagged sentences
    :type train: list(list(tuple(str, str)))
    :param train: The corpus of training data, a list of tagged sentences
    :type train: list(list(tuple(str, str)))
    :type regexps: list(tuple(str, str))
    :param regexps: A list of ``(regexp, tag)`` pairs, each of
    Where tokens is the list of unlabeled tokens in the sentence;
    should be performed; and history is list of the tags for all
    :param train: A tagged corpus consisting of a list of tagged
        sentences, where each sentence is a list of (word, tag) tuples.
        one argument, a list of labeled featuresets (i.e.,
        hunpos_paths = list(map(os.path.expanduser, hunpos_paths))
        """Tags a single sentence: a list of words.
    A processing interface for assigning a tag to each token in a list.
        token sequence, and return a corresponding list of tagged
        :rtype: list(tuple(str, str))
        :type gold: list(list(tuple(str, str)))
        :param gold: The list of tagged sentences to score the tagger on.
for a complete list.  Install corpora using nltk.download().
- If ``item`` is one of the unique identifiers listed in the corpus
Additionally, corpus reader functions can be given lists of item
- words(): list of str
- sents(): list of (list of str)
- paras(): list of (list of (list of str))
- tagged_words(): list of (str,str) tuple
- tagged_sents(): list of (list of (str,str))
- tagged_paras(): list of (list of (list of (str,str)))
- chunked_sents(): list of (Tree w/ (str,str) leaves)
- parsed_sents(): list of (Tree with str leaves)
- parsed_paras(): list of (list of (Tree with str leaves))
For example, to read a list of the words in the Brown Corpus, use
    'gazetteers', WordListCorpusReader, r'(?!LICENSE|\.).*\.txt',
    'names', WordListCorpusReader, r'(?!\.).*\.txt', encoding='ascii')
    'stopwords', WordListCorpusReader, r'(?!README|\.).*', encoding='utf8')
    'words', WordListCorpusReader, r'(?!README|\.).*', encoding='ascii')
					sent = list(map(self._parse_tag, TAGGEDWORD.findall(sent_str)))
		self._f2t = defaultdict(list)
		self._t2f = defaultdict(list)
		implemented. All the main functions can be supplied with a list
#: A list of all documents in this corpus.
                 list of tuples of fileids and scores.
        Returns a list of scored synonyms (tuples of synonyms and scores) for the current ngram
        :return: If fileid is specified, list of tuples of scores and synonyms; otherwise,
                 list of tuples of fileids and lists, where inner lists consist of tuples of
        Returns a list of synonyms for the current ngram.
        :return: If fileid is specified, list of synonyms; otherwise, list of tuples of fileids and
                 lists, where inner lists contain synonyms.
        :param fileids: A list or regexp specifying the fileids in this corpus.
        :return: the given file(s) as a list of words
        :rtype: list(str)
        :return: the given file(s) as a list of
            sentences or utterances, each encoded as a list of word
        :rtype: list(list(str))
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
            in turn encoded as lists of word strings.
        :rtype: list(list(list(str)))
        :return: the given file(s) as a list of
            chapters, each encoded as a list of sentences, which are
            in turn encoded as lists of word strings.
        :rtype: list(list(list(str)))
- If ``item`` is one of the unique identifiers listed in the corpus
Additionally, corpus reader functions can be given lists of item
- words(): list of str
- sents(): list of (list of str)
- paras(): list of (list of (list of str))
- tagged_words(): list of (str,str) tuple
- tagged_sents(): list of (list of (str,str))
- tagged_paras(): list of (list of (list of (str,str)))
- chunked_sents(): list of (Tree w/ (str,str) leaves)
- parsed_sents(): list of (Tree with str leaves)
- parsed_paras(): list of (list of (Tree with str leaves))
For example, to read a list of the words in the Brown Corpus, use
from nltk.corpus.reader.wordlist import *
    'ConllChunkCorpusReader', 'WordListCorpusReader',
    on blank lines; sentences are listed one per line; and sentences
        :param fileids: A list or regexp specifying the fileids in this corpus.
        :return: the given file(s) as a list of words
        :rtype: list(str)
        :return: the given file(s) as a list of
            sentences or utterances, each encoded as a list of word
        :rtype: list(list(str))
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
            in turn encoded as lists of word strings.
        :rtype: list(list(list(str)))
        :return: the given file(s) as a list of tagged
        :rtype: list(tuple(str,str))
        :return: the given file(s) as a list of
            sentences, each encoded as a list of ``(word,tag)`` tuples.
        :rtype: list(list(tuple(str,str)))
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
            in turn encoded as lists of ``(word,tag)`` tuples.
        :rtype: list(list(list(tuple(str,str))))
        :return: the given file(s) as a list of tagged
        :rtype: list(tuple(str,str) and Tree)
        :return: the given file(s) as a list of
        :rtype: list(Tree)
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
        :rtype: list(list(Tree))
        Return a list of document identifiers for all documents in
        Return a list of file identifiers for the files that make up
                            'a list of document identifiers.')
#: A list of all documents and their titles in ycoe.
    method.  For access to simple word lists and tagged word lists, use
        :return: the given file(s) as a list of words
        :rtype: list(str)
        :return: the given file(s) as a list of tagged
        :rtype: list(tuple(str,str))
        :return: the given file(s) as a list of
            sentences or utterances, each encoded as a list of word
        :rtype: list(list(str))
        :return: the given file(s) as a list of
            sentences, each encoded as a list of ``(word,tag)`` tuples.
        :rtype: list(list(tuple(str,str)))
        Helper used to implement the view methods -- returns a list of
        words or a list of sentences, optionally tagged.
class BNCSentence(list):
    A list of words, augmented by an attribute ``num`` used to record
        list.__init__(self, items)
        :param fileids: A list or regexp specifying the fileids in this corpus.
        :param framefiles: A list or regexp specifying the frameset
        framefiles = list(framefiles)
        :return: a corpus view that acts as a list of
        :return: a corpus view that acts as a list of strings, one for
        :return: list of xml descriptions for rolesets.
        :return: a corpus view that acts as a list of all noun lemmas
        """A list of tuples (argloc, argid), specifying the location
        ``'ARG0'`` or ``'ARGM-TMP'``.  This list does *not* contain
        """A list of the pieces that make up this chain.  Elements may
        """A list of the pieces that make up this chain.  Elements are
                    # End of node's child list: pop up a level.
# Natural Language Toolkit: Word List Corpus Reader
class WordListCorpusReader(CorpusReader):
    List of words, one per line.  Blank lines are ignored.
class SwadeshCorpusReader(WordListCorpusReader):
        wordlists = [self.words(f) for f in fileids]
        return zip(*wordlists)
        return concat([list(ToolboxData(fileid,enc).fields(
# default function to convert morphlist to str for tree representation
      - ``_word``, which takes a block and returns a list of list of words.
      - ``_tag``, which takes a block and returns a list of list of tagged
      - ``_parse``, which takes a block and returns a list of parsed
        morphs2str is a function to convert morphlist to str for tree representation
                node = dg.nodelist[i]
                while len(dg.nodelist) < i+1 or len(dg.nodelist) < dep_parent+1:
                    dg.nodelist.append({'word':[], 'deps':[]})
                    dg.nodelist[dep_parent]['deps'].append(i)
                dg.nodelist[i-1]['word'].append(morph)
            for node in dg.nodelist:
        self._lexelt_starts = [0] # list of streampos
        self._lexelts = [None] # list of lexelt names
    zero or more tokens from a stream, and returns them as a list.  A
    :ivar _toknum: A list containing the token index of each block
    :ivar _filepos: A list containing the file position of each block
       block; and tokens is a list of the tokens in the block.
        :rtype: list(any)
            assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (
                'block reader %s() should return list or tuple.' %
            self._cache = (toknum, toknum+num_toks, list(tokens))
        """A list of the corpus subviews that make up this
        """A list of offsets, indicating the index at which each
        typ = list(types)[0]
        if issubclass(typ, list):
            # Return the list of tokens we processed
    # Find fileids in a zipfile: scan the zipfile's namelist.  Filter
        fileids = [name[len(root.entry):] for name in root.zipfile.namelist()
        :return: the given file's text nodes as a list of words and punctuation symbols
        :rtype: list(str)
    file, and provides a flat list-like interface for accessing them.
    paths, where a tag path is a list of element tag names, separated
        context = list(self._tag_context.get(stream.tell()))
            for fileid in self._list_morph_files(fileids)])
            return self._list_morph_files_by('channel', channels)
            return self._list_morph_files_by('domain', domains)
            return self._list_morph_files_by('keyTerm', categories,
            for fileid in self._list_morph_files(fileids)])
            for fileid in self._list_morph_files(fileids)])
            for fileid in self._list_morph_files(fileids)])
            for fileid in self._list_morph_files(fileids)])
            for fileid in self._list_morph_files(fileids)])
            for fileid in self._list_morph_files(fileids)])
    def _list_morph_files(self, fileids):
    def _list_header_files(self, fileids):
                for f in self._list_morph_files(fileids)]
        for f in self._list_header_files(fileids):
            values_list = self._get_tag(f, tag)
            for v in values_list:
        return list(values)
    def _list_morph_files_by(self, tag, values, map=None):
            values_list = self._get_tag(fp, tag)
            for value in values_list:
        return list(ret_fileids)
   List of utterances in the corpus.  There are total 160 utterances,
   Here's an example of an utterance identifier in the list::
   List of speaker IDs.  An example of speaker ID::
   dictionary from words to phoneme lists.
   Given a list of items, returns an iterator of a list of word lists,
   each element of the word list is a tuple of word(string), start offset and
   Given a list of items, returns an iterator of a list of phoneme lists,
   each element of the phoneme list is a tuple of word(string), start offset
        """A list of the utterance identifiers for all utterances in
        Return a list of file identifiers for the files that make up
        :return: A list of the utterance identifiers for all
        :return: A list of all utterances associated with a given
class SwitchboardTurn(list):
    A specialized list object used to encode switchboard utterances.
    The elements of the list are the words in the utterance; and two
        list.__init__(self, words)
POS_LIST = [NOUN, VERB, ADJ, ADV]
    These methods all return lists of Lemmas:
    - lemmas: A list of the Lemma objects for this synset.
    - examples: A list of example strings for this synset.
    These methods all return lists of Synsets.
#            return list(set(root for h in self.hypernyms()
            >>> list(dog.closure(hyp))
        list of the synset nodes traversed on the way to the root.
        :return: A list of lists, where each list gives the node sequence
            for ancestor_list in hypernym.hypernym_paths():
                ancestor_list.append(self)
                paths.append(ancestor_list)
                           for self_synsets in self._iter_hypernym_lists()
                           for other_synsets in other._iter_hypernym_lists()
        return list(self_synsets.intersection(other_synsets))
        Get a list of lowest synset(s) that both synsets have as a hypernym.
            self_hypernyms = chain(self._iter_hypernym_lists(), [[fake_synset]])
            other_hypernyms = chain(other._iter_hypernym_lists(), [[fake_synset]])
            self_hypernyms = self._iter_hypernym_lists()
            other_hypernyms = other._iter_hypernym_lists()
        dist_list1 = self.hypernym_distances(simulate_root=simulate_root)
        dist_list2 = other.hypernym_distances(simulate_root=simulate_root)
        # Transform each distance list into a dictionary. In cases where
        # there are duplicate nodes in the list (due to there being multiple
        for (l, d) in [(dist_list1, dist_dict1), (dist_list2, dist_dict2)]:
    def _iter_hypernym_lists(self):
    #: A list of file identifiers for all the fileids used by this
    _FILES = ('cntlist.rev', 'lexnames', 'index.sense',
            pos = POS_LIST
            self._key_count_file = self.open('cntlist.rev')
        part of speech, by checking WordNet's list of exceptional
            analyses = chain(a for p in POS_LIST for a in morphy(form, p))
        first = list(islice(analyses, 1))
        # 0. Check the exception lists
        # Return an empty list if we can't find anything
        for pp in POS_LIST:
                for level in ss._iter_hypernym_lists():
        :return: the cmudict lexicon as a list of entries
        :return: a list of all words defined in the cmudict lexicon.
        lowercase words and whose values are lists of pronunciations.
    #: A list of all column types supported by the conll corpus reader.
        :return: a list of word/tag/IOB tuples
        :rtype: list(tuple)
        :param fileids: the list of fileids that make up this corpus
        :type fileids: None or str or list
        :return: a list of lists of word/tag/IOB tuples
        :rtype: list(list)
        :param fileids: the list of fileids that make up this corpus
        :type fileids: None or str or list
    # a list of words or a parse tree).
        return list(zip(self._get_column(grid, self._colmap['words']), pos_tags))
        return list(zip(self._get_column(grid, self._colmap['words']), pos_tags,
        list of list of (start, end), tag) tuples
        spanlists = []
            spanlist = []
                    spanlist.append( ((start, wordnum+1), tag) )
            spanlists.append(spanlist)
        return spanlists
        spanlists = self._get_srl_spans(grid)
        instances = ConllSRLInstanceList(tree)
            # Decide which spanlist to use.  Don't assume that they're
            for spanlist in spanlists:
                for (start, end), tag in spanlist:
                                              rolesets[wordnum], spanlist))
        """A list of the word indices of the words that compose the
        """A list of ``(argspan, argid)`` tuples, specifying the location
        """A list of ``(span, id)`` tuples, specifying the location and
        """A list of the words in the sentence containing this
                self.verb += list(range(start, end))
class ConllSRLInstanceList(list):
        list.__init__(self, instances)
    List of words, one per line.  Blank lines are ignored.
Each entailment corpus is a list of 'text'/'hypothesis' pairs. The following
        :rtype: list(RTEPair)
        Build a list of RTEPairs from a RTE corpus.
        :param fileids: a list of RTE corpus fileids
        :type: list
        :rtype: list(RTEPair)
        :param fileids: A list or regexp specifying the fileids in this corpus.
        :return: the given file(s) as a list of words
        :rtype: list(str)
        :return: the given file(s) as a list of tagged
        :rtype: list(tuple(str,str))
        :return: the given file(s) as a list of sentences or utterances, each
            encoded as a list of word strings.
        :rtype: list(list(str))
        :param relation: If true, then return tuples of ``(str,pos,relation_list)``.
            tuples of ``(str,pos,test_relation_list,str,pos,gold_relation_list)``
        :return: the given file(s) as a list of
            sentences, each encoded as a list of ``(word,tag)`` tuples.
        :rtype: list(list(tuple(str,str)))
        :param relation: If true, then return tuples of ``(str,pos,relation_list)``.
            tuples of ``(str,pos,test_relation_list,str,pos,gold_relation_list)``
        :rtype: list(dict)
        :rtype: list(dict)
        :rtype: list or int
        :rtype: list(float)
            posList = [pos for (word,pos) in sent]
            if any(pos == 'unk' for pos in posList):
                if len(set(['co',None]).intersection(posList)) > 0:
                    numFillers += posList.count('co')
                    numFillers += posList.count(None)
            thisWordList = flatten(results)
                                          for word in thisWordList]))) - numFillers
        if isinstance(speaker, string_types) and speaker != 'ALL':  # ensure we have a list of speakers
        #childes = CHILDESCorpusReader(corpus_root_http_bates,corpus_root_http_bates.namelist())
        self._lemma_to_class = defaultdict(list)
        """A dictionary mapping from verb lemma strings to lists of
        self._wordnet_to_class = defaultdict(list)
        lists of verbnet class identifiers."""
        provide a complete list of all classes and subclasses."""
        Return a list of all verb lemmas that appear in any class, or
        Return a list of all wordnet identifiers that appear in any
        Return a list of the verbnet class identifiers.  If a file
        Return a list of fileids that make up this corpus.  If
    method.  For access to simple word lists and tagged word lists, use
        :return: the given file(s) as a list of words and punctuation symbols.
        :rtype: list(str)
        :return: the given file(s) as a list of chunks,
            each of which is a list of words and punctuation symbols
        :rtype: list(list(str))
        :return: the given file(s) as a list of tagged chunks, represented
        :rtype: list(Tree)
        :return: the given file(s) as a list of sentences, each encoded
            as a list of word strings.
        :rtype: list(list(str))
        :return: the given file(s) as a list of sentences, each encoded
            as a list of chunks.
        :rtype: list(list(list(str)))
        :return: the given file(s) as a list of sentences. Each sentence
            is represented as a list of tagged chunks (in tree form).
        :rtype: list(list(Tree))
        Helper used to implement the view methods -- returns a list of
                    return bottom # chunk as a list
class SemcorSentence(list):
    A list of words, augmented by an attribute ``num`` used to record
        list.__init__(self, items)
            return list(sent)
        :param framefiles: A list or regexp specifying the frameset
        framefiles = list(framefiles)
        :return: a corpus view that acts as a list of
        :return: a corpus view that acts as a list of strings, one for
        :return: list of xml descriptions for rolesets.
        :return: a corpus view that acts as a list of all verb lemmas
        """A list of tuples (argloc, argid), specifying the location
        ``'ARG0'`` or ``'ARGM-TMP'``.  This list does *not* contain
        """A list of the pieces that make up this chain.  Elements may
        """A list of the pieces that make up this chain.  Elements are
                    # End of node's child list: pop up a level.
class PrettyList(list):
    Displays an abbreviated repr of only the first several elements, not the whole list.
        super(PrettyList, self).__init__(*args, **kwargs)
        similar to a list's representation; but if it would be more
    Displays an abbreviated repr of only the first several elements, not the whole list.
        similar to a list's representation; but if it would be more
    When loading LUs for a frame, those whose status is in this list will be ignored. 
    'Problem' should always be listed for FrameNet 1.5, as these LUs are not included 
        for doclist in XMLCorpusView(self.abspath("fulltextIndex.xml"),
            for doc in doclist:
        freltypes = PrettyList(x for x in XMLCorpusView(self.abspath("frRelation.xml"),
        - 'sentence'   : a list of sentences in the document
           - Each item in the list is a dict containing the following keys:
              - 'annotationSet' : a list of annotation layers for the sentence
                 - Each item in the list is a dict containing the following keys:
                    - 'layer' : a list of labels for the layer
                          - 'label' : a list of labels in the layer
        :type ignorekeys: list(str)
        :type ignorekeys: list(str)
        - 'semTypes'   : a list of semantic types for this frame
           - Each item in the list is a dict containing the following keys:
        - 'frameRelation'      : a list of objects describing frame relations
        - 'FEcoreSets'  : a list of Frame Element core sets for this frame
           - Each item in the list is a list of FE objects
        :type ignorekeys: list(str)
        Returns a list of all frames that contain LUs in which the
        :return: A list of frame objects.
        :rtype: list(AttrDict)
        return PrettyList(f for f in self.frames() if any(re.search(pat, luName) for luName in f.lexUnit))
        >>> pprint(list(map(PrettyDict, fn.lu(256).lexemes)))
        - 'lexemes'  : a list of dicts describing the lemma of this LU. 
           Each dict in the list contains these keys:
        - 'semTypes'   : a list of semantic type objects for this LU
        - 'subCorpus'  : a list of subcorpora
           - Each item in the list is a dict containing the following keys:
              - 'sentence' : a list of sentences in the subcorpus
                 - each item in the list is a dict with the following keys:
                    - 'annotationSet': a list of annotation sets
                       - each item in the list is a dict with the following keys:
                          - 'layer': a list of layers
                                - 'label': a list of labels for the layer
        :type ignorekeys: list(str)
        queue = list(roots)
        >>> PrettyList(fn.frames(r'(?i)medical'), maxReprSize=0, breakLines=True)
            Frame names. If 'name' is None, then a list of all
        :return: A list of matching Frames (or all Frames).
        :rtype: list(AttrDict)
            fIDs = list(self._frame_idx.keys())
            fIDs = list(self._frame_idx.keys())
            return PrettyList(self.frame(fID) for fID,finfo in self.frame_ids_and_names(name).items())
        >>> PrettyList(fn.lus(r'(?i)a little'), maxReprSize=0, breakLines=True)
        :return: A list of selected (or all) lexical units
        :rtype: list of LU objects (dicts). See the lu() function for info
            luIDs = list(self._lu_idx.keys())
            luIDs = list(self._lu_idx.keys())
            return PrettyList(self.lu(luID) for luID,luName in self.lu_ids_and_names(name).items())
        Return a list of the annotated documents in Framenet.
        :return: A list of selected (or all) annotated documents
        :rtype: list of dicts, where each dict object contains the following
            ftlist = PrettyList(self._fulltext_idx.values())
            ftlist = PrettyList(self._fulltext_idx.values())
            return ftlist
            return PrettyList(x for x in ftlist if re.search(name, x['filename']) is not None)
        Obtain a list of frame relation types.
        >>> frts = list(fn.frame_relation_types())
        >>> isinstance(frts, list)
        :return: A list of all of the frame relation types in framenet
        :rtype: list(dict)
        :return: A list of all of the frame relations in framenet
        :rtype: list(dict)
        >>> isinstance(frels, list)
        >>> PrettyList(fn.frame_relations('Cooking_creation'), maxReprSize=0, breakLines=True)
        >>> PrettyList(fn.frame_relations(373), breakLines=True)
        >>> PrettyList(fn.frame_relations(fn.frame('Cooking_creation')), breakLines=True)
        >>> PrettyList(fn.frame_relations('Cooking_creation', type='Inheritance'))
        >>> PrettyList(fn.frame_relations('Cooking_creation', 'Apply_heat'), breakLines=True)
                rels = PrettyList(frame.frameRelations)
        return PrettyList(sorted(rels, 
        Obtain a list of frame element relations.
        >>> isinstance(ferels, list)
        :return: A list of all of the frame element relations in framenet
        :rtype: list(dict)
        return PrettyList(sorted(self._ferel_idx.values(), 
        Obtain a list of semantic types.
        :return: A list of all of the semantic types in framenet
        :rtype: list(dict)
        return PrettyList(self._semtypes[i] for i in self._semtypes if isinstance(i, int))
        retlist = []
                retlist.append(doc)
        return retlist
                frinfo['FEcoreSets'].append(PrettyList(frinfo['FE'][fe.name] for fe in coreset))
        info['frameRelations'] = PrettyList()
        info['feRelations'] = PrettyList()
        luinfo['lexemes'] = PrettyList()   # multiword LUs have multiple lexemes
        luinfo['semTypes'] = PrettyList()  # an LU can have multiple semtypes
        luinfo['subCorpus'] = PrettyList()
        luinfo['lexemes'] = PrettyList()   # multiword LUs have multiple lexemes
        luinfo['semTypes'] = PrettyList()  # an LU can have multiple semtypes
        semt['subTypes'] = PrettyList()
    # store the first frame in the list of frames
    # Get a list of all of the corpora used for fulltext annotation
    pprint(list(allcorpora))
    firstcorp = list(allcorpora)[0]
        :param fileids: A list or regexp specifying the fileids in this corpus.
        :return: the given file(s) as a list of words
        :rtype: list(str)
        :return: the given file(s) as a list of
            sentences or utterances, each encoded as a list of word
        :rtype: list(list(str))
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
            in turn encoded as lists of word strings.
        :rtype: list(list(list(str)))
        :return: the given file(s) as a list of tagged
        :rtype: list(tuple(str,str))
        :return: the given file(s) as a list of
            sentences, each encoded as a list of ``(word,tag)`` tuples.
        :rtype: list(list(tuple(str,str)))
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
            in turn encoded as lists of ``(word,tag)`` tuples.
        :rtype: list(list(list(tuple(str,str))))
    corpus contents, such as ``words()`` (for a list of words) and
    ``parsed_sents()`` (for a list of parsed sentences).  Called with
        :param fileids: A list of the files that make up this corpus.
            This list can either be specified explicitly, as a list of
            - A list: ``encoding`` should be a list of ``(regexp, encoding)``
        """A list of the relative paths for the fileids that make up
        # If encoding was specified as a list of regexps, then convert
        if isinstance(encoding, list):
        Return a list of file identifiers for the fileids that make up
        Return a list of the absolute paths for all fileids in this corpus;
        or for the given list of fileids, if specified.
        :type fileids: None or str or list
            be returned.  Can be None, for all fileids; a list of
            value is always a list of paths, even if ``fileids`` is a
        :param include_encoding: If true, then return a list of
        :rtype: list(PathPointer)
    ``categories()``, which returns a list of the categories for the
        Return a list of the categories that are defined for this corpus,
        Return a list of file identifiers for the files that make up
      - ``_word``, which takes a block and returns a list of list of words.
      - ``_tag``, which takes a block and returns a list of list of tagged
      - ``_parse``, which takes a block and returns a list of parsed
        return list(filter(None, [self._word(t) for t in self._read_block(stream)]))
        return list(filter(None, [self._tag(t, tagset)
        return list(filter(None, [self._parse(t) for t in self._read_block(stream)]))
        :param fileids: A list or regexp specifying the fileids in this corpus.
        :return: the given file(s) as a list of words
        :rtype: list(str)
        :return: the given file(s) as a list of
            sentences or utterances, each encoded as a list of word
        :rtype: list(list(str))
        :return: the given file(s) as a list of AlignedSent objects.
        :rtype: list(AlignedSent)
        :type train: list(str) or list(list(str))
        # If given a list of strings instead of a list of lists, create enclosing list
        :type context: list(str)
        :type context: list(str)
        :type context: list(str)
        :type context: list(str)
        text = list(context)
        :type text: list(str)
        text = list(self._lpad) + text + list(self._rpad)
        :type text: list(str)
        :rtype: list(str)
        :param tokens: The document (list of tokens) that this
            concordance index was created from.  This list can be used
        """The document (list of tokens) that this concordance index
        self._offsets = defaultdict(list)
        """Dictionary mapping words (or keys) to lists of offset
        :rtype: list(str)
        :rtype: list(int)
        :return: A list of the offset positions at which the given
        The text is a list of tokens, and a regexp pattern to match
            tokens = list(tokens)
            print("Building collocations list")
        same contexts as the specified word; list most similar words first.
        Find contexts where the specified words appear; list
        The text is a list of tokens, and a regexp pattern to match
    """A collection of texts, which can be loaded with list of texts, or
print("Type: 'texts()' or 'sents()' to list the materials.")
    l = list()
    out = list()
Classes for representing and processing probabilistic information.
        list ``samples``.
        Return a list (or an iterator under Python 3.x) of all samples
        :rtype: list
        Return a list of all samples that occur once (hapax legomena)
        :rtype: list
        :rtype: list(float)
        samples = list(islice(self, *args))
            freqs = list(self._cumulative_frequencies(samples))
        :type samples: list
        samples = list(islice(self, *args))
            freqs = list(self._cumulative_frequencies(samples))
        Returns a list in Python 2.x, and an iterator in Python 3.x.
        Returns a list in Python 2.x, and an iterator in Python 3.x.
        :rtype: list(tuple)
        Update the frequency distribution with the provided list of samples.
        :type samples: list
        Return a list of all samples that have nonzero probabilities.
        :rtype: list
        return random.choice(list(self.samples()))
        :type samples: list
        self._samples = list(self._sampleset)
        self._samples = list(self._probs.keys())
                value_sum = sum_logs(list(self._prob_dict.values()))
    :type _estimate: list(float)
    :ivar _estimate: A list mapping from *r*, the number of
        Return the list *Tr*, where *Tr[r]* is the total count in
        :rtype: list(float)
        Return the list *estimate*, where *estimate[r]* is the probability
        :rtype: list(float)
        :type Tr: list(float)
        :param Tr: the list *Tr*, where *Tr[r]* is the total count in
        :type Nr: list(float)
        :param Nr: The list *Nr*, where *Nr[r]* is the number of
        :type freqdists: list(FreqDist)
        :param freqdists: A list of the frequency distributions
        Return the list of frequency distributions that this ``ProbDist`` is based on.
        :rtype: list(FreqDist)
        Split the frequency distribution in two list (r, Nr), where Nr(r) > 0
        the list of samples given. These values are stored as log
        Return a list of the conditions that have been accessed for
        :rtype: list
        :type samples: list
        :type conditions: list
                freqs = list(self[condition]._cumulative_frequencies(samples))
        :type samples: list
        :type conditions: list
                freqs = list(self[c]._cumulative_frequencies(samples))
        Return a list of the conditions that are represented by
        :rtype: list
##  Probabilistic Mix-in
class ProbabilisticMixIn(object):
    (trees, rules, etc.).  To use the ``ProbabilisticMixIn`` class,
    ProbabilisticMixIn.  You will need to define a new constructor for
        >>> from nltk.probability import ProbabilisticMixIn
        >>> class ProbabilisticA(A, ProbabilisticMixIn):
        ...         ProbabilisticMixIn.__init__(self, **prob_kwarg)
    See the documentation for the ProbabilisticMixIn
                ProbabilisticMixIn.set_prob(self, kwargs['prob'])
            ProbabilisticMixIn.set_logprob(self, kwargs['logprob'])
class ImmutableProbabilisticMixIn(ProbabilisticMixIn):
    zvals = list(zip(*vals))
           'ImmutableProbabilisticMixIn', 'LaplaceProbDist', 'LidstoneProbDist',
           'MLEProbDist', 'MutableProbDist', 'KneserNeyProbDist', 'ProbDistI', 'ProbabilisticMixIn',
        each of which is a list (or iterable) of tokens.
    a print_assumptions() method that is used to print the list
        :type assumptions: list(sem.Expression)
        :param values: a list of 1's and 0's that represent whether a relation holds in a Mace4 model.
        :type values: list of int
            sublist_size = len(values) / num_entities
            sublist_start = position / sublist_size
            sublist_position = position % sublist_size
            sublist = values[sublist_start*sublist_size:(sublist_start+1)*sublist_size]
            return [MaceCommand._make_model_var(sublist_start)] + \
                   MaceCommand._make_relation_tuple(sublist_position,
                                                    sublist,
        :param value: where to index into the list of characters
        :param args: A list of command-line arguments.
        :param args: A list of command-line arguments.
        alist = [lp.parse(a) for a in assumptions]
        m = MaceCommand(g, assumptions=alist, end_size=50)
        for a in alist:
    alist = [lp.parse(a) for a in ['man(John)',
    m = MaceCommand(g, assumptions=alist)
    for a in alist:
    alist = [lp.parse(a) for a in argument_pair[1]]
    m = MaceCommand(g, assumptions=alist)
    for a in alist:
                variable_to_use = list(bv_available)[0]
        :type assumptions: list(sem.Expression)
        set_list = [s.copy() for s in self.sets]
        for allEx,_ in set_list[Categories.ALL]:
        set_list[Categories.ALL] = new_allExs
        set_list[Categories.N_EQ] = set((NegatedExpression(n_eq.term),ctx)
                                        for (n_eq,ctx) in set_list[Categories.N_EQ])
        new_agenda.sets = tuple(set_list)
        :type assumptions: list(sem.Expression)
        #map indices to lists of indices, to store attempted unifications
        tried = defaultdict(list)
        :type assumptions: list(sem.Expression)
class Clause(list):
        list.__init__(self, data)
        Attempt to unify this Clause with the other, returning a list of
        :param used: tuple of two lists of atoms.  The first lists the
        'other'.  The second lists the atoms from 'other' that were successfully
        :param skipped: tuple of two ``Clause`` objects.  The first is a list of all
        :return: list containing all the resulting ``Clause`` objects that could be
        #remove subsumed clauses.  make a list of all indices of subsumed
        #clauses, and then remove them from the list
        return Clause(list.__getslice__(self, start, end))
        return Clause(list.__add__(self, other))
        :param bindings: A list of tuples mapping Variable Expressions to the
    clause_list = []
        clause_list.append(clause)
    return clause_list
    def __init__(self, binding_list=None):
        :param binding_list: list of (``AbstractVariableExpression``, ``AtomicExpression``) to initialize the dictionary
        if binding_list:
            for (v, b) in binding_list:
    :return: a list of bindings
        domain = list(get_domain(self._command.goal(), assumptions))
class SetHolder(list):
    A list of sets of Variables.
        and 'signature' as the list of arguments.
        :param assumptions: a list of ``Expression``s
    The 'signatures' property is a list of tuples defining signatures for
    The second element of the pair is a list of pairs such that the first
                        #   (sos list exhausted).
        Print the list of the current assumptions.
    the a print_assumptions() method that is used to print the list
        :type assumptions: list(sem.Expression)
            s += 'end_of_list.\n\n'
            s += 'end_of_list.\n\n'
        A list of directories that should be searched for the prover9
        executables.  This list is used by ``config_prover9`` when searching
        :param args: A list of command-line arguments.
    if isinstance(input, list):
        :param args: A list of command-line arguments.
        :param args: A list of command-line arguments.
        alist = [LogicParser().parse(a) for a in assumptions]
        p = Prover9Command(g, assumptions=alist).prove()
        for a in alist:
The basic data structure is a list of sentences, stored as ``self._sentences``. Each sentence in the list
A "thread" is a list of readings, represented as a list of ``rid``\ s.
        :type sentence_readings: list(Expression)
        :return: the list of readings after processing
        :rtype: list(Expression)
        :type readings: list(Expression)
        :type input: list of str
        :type background: list(Expression)
        Display the list of sentences in the current discourse.
        :param verbose: If ``True``,  report on the updated list of sentences.
        Build a list of semantic readings for a sentence.
        :rtype: list(Expression)
        thread_list = [[]]
            thread_list = self.multiply(thread_list, sorted(self._readings[sid]))
        self._threads = dict([("d%s" % tid, thread) for tid, thread in enumerate(thread_list)])
        Given a thread ID, find the list of ``logic.Expression`` objects corresponding to the reading IDs in that thread.
        :param threads: a mapping from thread IDs to lists of reading IDs
        :return: A list of pairs ``(rid, reading)`` where reading is the ``logic.Expression`` associated with a reading ID
        :rtype: list of tuple
            assumptions = list(map(self._reading_command.to_fol, self._reading_command.process_thread(assumptions)))
            idlist = [rid for rid in threads[tid]]
                print("Inconsistent discourse: %s %s:" % (tid, idlist))
                print("Consistent discourse: %s %s:" % (tid, idlist))
        Add a list of background assumptions for reasoning about the discourse.
        :type background: list(Expression)
        :param discourse: the current list of readings
        :type discourse: list of lists
        :param readings: an additional list of readings
        :type readings: list(Expression)
        :rtype: A list of lists
        for sublist in discourse:
                new += sublist
    Convert a  file of first order formulas into a list of ``Expression`` objects.
    :return: a list of parsed formulas.
    :rtype: list(Expression)
list of assumptions.
    This class holds a goal and a list of assumptions to be used in proving
        Add new assumptions to the assumption list.
        :type new_assumptions: list(sem.Expression)
        Retract assumptions from the assumption list.
        assumptions list.
        :type retracted: list(sem.Expression)
        List the current assumptions.
        :return: list of ``Expression``
        Print the list of the current assumptions.
    This class holds a ``Prover``, a goal, and a list of assumptions.  When
    This class holds a ``ModelBuilder``, a goal, and a list of assumptions.
    This class holds a goal and a list of assumptions to be used in proving
        :type assumptions: list(sem.Expression)
            self._assumptions = list(assumptions)
        Add new assumptions to the assumption list.
        :type new_assumptions: list(sem.Expression)
        Retract assumptions from the assumption list.
        assumptions list.
        :type retracted: list(sem.Expression)
        result_list = list(filter(lambda a: a not in retracted, self._assumptions))
        if debug and result_list == self._assumptions:
            print(Warning("Assumptions list has not been changed:"))
        self._assumptions = result_list
        List the current assumptions.
        :return: list of ``Expression``
        Print the list of the current assumptions.
    This class holds a ``Prover``, a goal, and a list of assumptions.  When
    This class holds a ``ModelBuilder``, a goal, and a list of assumptions.  When
        #: A list of directories that should be searched for the malt
        #: executables.  This list is used by ``config_malt`` when searching
        Use MaltParser to parse a sentence. Takes a sentence as a list of
        :type sentence: list(str)
        list where each sentence is a list of words.
        :type sentence: list(list(str))
        :return: list(``DependencyGraph``) the dependency graph representation
        Use MaltParser to parse a sentence. Takes a sentence as a list of
        :type sentence: list(tuple(str, str))
        where each sentence is a list of (word, tag) tuples.
        :type sentence: list(list(tuple(str, str)))
        :return: list(``DependencyGraph``) the dependency graph representation
        Train MaltParser from a list of ``DependencyGraph`` objects
        :param depgraphs: list of ``DependencyGraph`` objects for training input data
  - ``nltk.parser.probabilistic`` defines probabilistic parsing, which
from nltk.parse.pchart import (BottomUpProbabilisticChartParser, InsideChartParser,
                                                   ProbabilisticProjectiveDependencyParser)
                                                      ProbabilisticNonprojectiveParser)
    be discarded), and a list of spans serving as the cell's entries.
        Appends the given span to the list of spans
        Performs a projective dependency parse on the list of tokens using
        :param tokens: The list of input tokens.
        :type tokens: list(str)
        :return: A list of parse trees.
        :rtype: list(Tree)
        self._tokens = list(tokens)
        :return: A list of new spans formed through concatenation.
        :rtype: list(DependencySpan)
# Parsing  with Probabilistic Dependency Grammars
class ProbabilisticProjectiveDependencyParser(object):
    A probabilistic, projective dependency parser.  This parser returns
    the most probable projective parse derived from the probabilistic
    dependency grammar derived from the train() method.  The probabilistic
        Create a new probabilistic dependency parser.  No additional
        Parses the list of tokens subject to the projectivity constraint
        probabilistic dependency grammar.
        self._tokens = list(tokens)
        :return: A list of new spans formed through concatenation.
        :rtype: list(DependencySpan)
        Trains a StatisticalDependencyGrammar based on the list of input
        :param graphs: A list of dependency graphs to train from.
        :type: list(DependencyGraph)
            for node_index in range(1,len(dg.nodelist)):
                children = dg.nodelist[node_index]['deps']
                    head_word = dg.nodelist[node_index]['word']
                    head_tag = dg.nodelist[node_index]['tag']
                            child = dg.nodelist[children[array_index]]['word']
                            child_tag = dg.nodelist[children[array_index]]['tag']
                            prev_word = dg.nodelist[children[array_index + 1]]['word']
                            prev_tag =  dg.nodelist[children[array_index + 1]]['tag']
                            child = dg.nodelist[children[array_index]]['word']
                            child_tag = dg.nodelist[children[array_index]]['tag']
                            prev_word = dg.nodelist[children[array_index - 1]]['word']
                            prev_tag =  dg.nodelist[children[array_index - 1]]['tag']
        for node_index in range(1,len(dg.nodelist)):
            children = dg.nodelist[node_index]['deps']
                head_word = dg.nodelist[node_index]['word']
                head_tag = dg.nodelist[node_index]['tag']
                        child = dg.nodelist[children[array_index]]['word']
                        child_tag = dg.nodelist[children[array_index]]['tag']
                        prev_word = dg.nodelist[children[array_index + 1]]['word']
                        prev_tag =  dg.nodelist[children[array_index + 1]]['tag']
                        child = dg.nodelist[children[array_index]]['word']
                        child_tag = dg.nodelist[children[array_index]]['tag']
                        prev_word = dg.nodelist[children[array_index - 1]]['word']
                        prev_tag =  dg.nodelist[children[array_index - 1]]['tag']
    in which a specific number of modifiers is listed for a given
    ppdp = ProbabilisticProjectiveDependencyParser()
    print('Training Probabilistic Projective Dependency Parser...')
    structure of a portion of the text.  This stack is a list of
        whose CFG production is listed earliest in the grammar.
        tokens = list(tokens)
        :type stack: list(str and Tree)
        :param stack: A list of strings and Trees, encoding
        :type remaining_text: list(str)
        :type rhs: list(terminal and Nonterminal)
        :type rightmost_stack: list(string and Tree)
        matches the stack, then use the production that is listed
        :type stack: list(string and Tree)
        :param stack: A list of strings and Trees, encoding
        :type remaining_text: list(str)
    :ivar _history: A list of ``(stack, remaining_text)`` pairs,
        tokens = list(tokens)
        :rtype: list(str and Tree)
        :rtype: list(str)
        :return: A list of the productions for which reductions are
        :rtype: list(Production)
        :return: A list of the parses that have been found by this
        :rtype: list of Tree
    Generates a list of sentences from a CFG.
    :return: A list of lists of terminal tokens.
    return list(itertools.islice(sentences, n))
    :return: An iterator of lists of terminal tokens.
        in the nodelist, since the root node is often assigned '0'
        as its head. This also means that the indexing of the nodelist
        self.nodelist = [top]
        node_index = len(self.nodelist) - 1
            node = self.nodelist[node_index]
                self.nodelist.pop(node_index)
        Redirects arcs to any of the nodes in the originals list
        for node in self.nodelist:
        for node in self.nodelist:
        for node1 in self.nodelist:
            for node2 in self.nodelist:
        for node in self.nodelist:
        for node in self.nodelist:
        return pformat(self.nodelist)
        return "<DependencyGraph with %d nodes>" % len(self.nodelist)
        :return: a list of DependencyGraphs
        children = self.nodelist[node_index]['deps']
        index = self.nodelist[node_index]['address']
        children = self.nodelist[node_index]['deps']
        index = self.nodelist[node_index]['address']
            self.nodelist.append(node)
                self.nodelist.append({'address': index+1, 'word': word, 'tag': tag,
                    self.nodelist[head]['deps'].append(index+1)
        root_address = self.nodelist[0]['deps'][0]
        self.root = self.nodelist[root_address]
        :param i: index of a node in ``nodelist``
            return self.nodelist[i]['head']
            return self.nodelist[i]['rel']
    # what's the return type?  Boolean or list?
        for node in self.nodelist:
        for n in range(len(self.nodelist)):
                    path = self.get_cycle_path(self.get_by_address(pair[0]), pair[0]) #self.nodelist[pair[0]], pair[0])
            path = self.get_cycle_path(self.get_by_address(dep), goal_node_index)#self.nodelist[dep], goal_node_index)
        for i, node in enumerate(self.nodelist[1:]):
    Convert the data in a ``nodelist`` into a networkx
    nx_nodelist = list(range(1, len(self.nodelist)))
    nx_edgelist = [(n, self._hd(n), self._rel(n))
                        for n in nx_nodelist if self._hd(n)]
    for n in nx_nodelist:
        self.nx_labels[n] = self.nodelist[n]['word']
    g.add_nodes_from(nx_nodelist)
    g.add_edges_from(nx_edgelist)
    cyclic_dg.nodelist = [top, child1, child2, child3, child4]
        return find_variables([self._lhs] + list(self._rhs) +
                              list(self._bindings.keys()) +
                              list(self._bindings.values()),
    def insert(self, edge, child_pointer_list):
        return FeatureChart.insert(self, edge, child_pointer_list)
        # Get a list of variables that need to be instantiated.
    ``RecursiveDescentParser`` uses a list of tree locations called a
    tree location consists of a list of child indices specifying the
        tokens = list(tokens)
        a list of all parses found.
        :return: A list of all parses that can be generated by
        :rtype: list of Tree
        :type remaining_text: list(str)
        :type frontier: list(tuple(int))
        :param frontier: A list of the locations within ``tree`` of
            leaves that have not yet been matched.  This list sorted
        :rtype: list of Tree
        :return: a list of all parses that can be generated by
            return empty list.
        :type rtext: list(str)
        :type frontier: list of tuple of int
        :param frontier: A list of the locations within ``tree`` of
        :rtype: list of Tree
        :return: A list of all parses that can be generated by
            hand side, then return an empty list.  If ``production`` is
            not specified, then return a list of all parses that can
        :type remaining_text: list(str)
        :type frontier: list(tuple(int))
        :param frontier: A list of the locations within ``tree`` of
    :ivar _history: A list of ``(rtext, tree, frontier)`` tripples,
        tokens = list(tokens)
        :rtype: list(str)
        :return: A list of the tree locations of all subtrees that
        :rtype: list(tuple(int))
        :return: A list of all the productions for which expansions
        :rtype: list(Production)
        :return: A list of all the untried productions for which
        :rtype: list(Production)
        :rtype: list of int
        :return: A list of the parses that have been found by this
        :rtype: list of Tree
      - ``'pcfg'`` (probabilistic CFGs: ``WeightedGrammar``)
        Only used for probabilistic CFGs.
        ``trees`` will be a non-empty list. If a sentence should be rejected
    :return: a list of tuple of sentences and expected results,
        where a sentence is a list of str,
# Natural Language Toolkit: Viterbi Probabilistic Parser
from nltk.tree import Tree, ProbabilisticTree
        tokens = list(tokens)
        :type constituents: dict(tuple(int,int,Nonterminal) -> ProbabilisticToken or ProbabilisticTree)
            ``ProbabilisticTree`` that covers ``text[s:e]``
        :type tokens: list of tokens
            # ProbabilisticTree whose probability is the product
                tree = ProbabilisticTree(node, children, prob=p)
        :return: a list of the production instantiations that cover a
            a tuple containing a production and a list of children,
            where the production's right hand side matches the list of
            children; and the children cover ``span``.  :rtype: list
            of ``pair`` of ``Production``, (list of
            (``ProbabilisticTree`` or token.
        :type constituents: dict(tuple(int,int,Nonterminal) -> ProbabilisticToken or ProbabilisticTree)
            childlists = self._match_rhs(production.rhs(), span, constituents)
            for childlist in childlists:
                rv.append( (production, childlist) )
        :return: a set of all the lists of children that cover ``span``
        :rtype: list(list(ProbabilisticTree or token)
        :type rhs: list(Nonterminal or any)
        :param rhs: The list specifying what kinds of children need to
            trying to find child lists.  The span is specified as a
            the first token that should be covered by the child list;
            that should not be covered by the child list.
        :type constituents: dict(tuple(int,int,Nonterminal) -> ProbabilisticToken or ProbabilisticTree)
        childlists = []
                childlists += [[l]+r for r in rights]
        return childlists
    A demonstration of the probabilistic parsers.  The user is
    ``ProbabilisticNonprojectiveParser`` to initialize the edge
    multidimensional list representation of the edge weights can
        :type graphs: list(DependencyGraph)
        :param graphs: A list of dependency graphs to train the scorer.
        :rtype: A three-dimensional list of numbers.
        :return: The score is returned in a multidimensional(3) list, such
        scores[0][1] would reference the list of scores corresponding to
        For further illustration, a score list corresponding to Fig.2 of
        graphs list as positive examples, the edges not present as
        :type graphs: list(DependencyGraph)
        :param graphs: A list of dependency graphs to train the scorer.
            for head_node in graph.nodelist:
                for child_index in range(len(graph.nodelist)):
        positive label.  Scores are returned in a multidimensional list.
        :rtype: 3 dimensional list
        for i in range(len(graph.nodelist)):
            for j in range(len(graph.nodelist)):
            if count == len(graph.nodelist):
# Non-Projective Probabilistic Parsing
class ProbabilisticNonprojectiveParser(object):
    A probabilistic non-projective dependency parser.  Nonprojective
        :type graphs: list(DependencyGraph)
        :param graphs: A list of dependency graphs to train the scorer.
        Takes a list of nodes that have been identified to belong to a cycle,
        :type cycle_path: A list of integers.
        :param cycle_path: A list of node addresses, each of which is in the cycle.
        g_graph.nodelist.append(new_node)
        :type cycle_path: A list of integers.
        :param cycle_path: A list of node addresses that belong to the cycle.
        takes a list of node addresses and replaces any collapsed
        :type new_address: A list of integers.
        :param new_addresses: A list of node addresses to check for
        :type cycle_indexes: A list of integers.
        is a list of such nodes addresses.
        Parses a list of tokens in accordance to the MST parsing algorithm
        :type tokens: list(str)
        :param tokens: A list of words or punctuation to be parsed.
        :type tags: list(str)
        :param tags: A list of tags corresponding by index to the words in the tokens list.
            g_graph.nodelist.append({'word':token, 'tag':tags[index], 'deps':[], 'rel':'NTOP', 'address':index+1})
            original_graph.nodelist.append({'word':token, 'tag':tags[index], 'deps':[], 'rel':'NTOP', 'address':index+1})
        b_graph.nodelist = []
        c_graph.nodelist = [{'word':token, 'tag':tags[index], 'deps':[],
        # Initialize a list of unvisited vertices (by node address)
        unvisited_vertices = [vertex['address'] for vertex in c_graph.nodelist]
            # Add v_n+1 to list of unvisited vertices
        for node in original_graph.nodelist:
        param tokens: A list of tokens to parse.
        type tokens: list(str)
        rtype: list(DependencyGraph)
        self._graph.nodelist = []  # Remove the default root
            self._graph.nodelist.append({'word':token, 'deps':[], 'rel':'NTOP', 'address':index})
        for head_node in self._graph.nodelist:
            for dep_node in self._graph.nodelist:
                graph.nodelist[0]['deps'] = root + 1
                    graph.nodelist.append(node)
    npp = ProbabilisticNonprojectiveParser()
    npp = ProbabilisticNonprojectiveParser()
# Natural Language Toolkit: Probabilistic Chart Parsers
probabilistic parser module defines ``BottomUpProbabilisticChartParser``.
``BottomUpProbabilisticChartParser`` is an abstract class that implements
The ``BottomUpProbabilisticChartParser`` constructor has an optional
# to associate probabilities with child pointer lists.
from nltk.tree import Tree, ProbabilisticTree
# Probabilistic edges
class ProbabilisticLeafEdge(LeafEdge):
class ProbabilisticTreeEdge(TreeEdge):
        return ProbabilisticTreeEdge(p, (index, index), production.lhs(),
# Rules using probabilistic edges
class ProbabilisticBottomUpInitRule(AbstractChartRule):
            new_edge = ProbabilisticLeafEdge(chart.leaf(index), index)
class ProbabilisticBottomUpPredictRule(AbstractChartRule):
                new_edge = ProbabilisticTreeEdge.from_production(prod, edge.start(), prod.prob())
class ProbabilisticFundamentalRule(AbstractChartRule):
        new_edge = ProbabilisticTreeEdge(p,
        for cpl1 in chart.child_pointer_lists(left_edge):
class SingleEdgeProbabilisticFundamentalRule(AbstractChartRule):
    _fundamental_rule = ProbabilisticFundamentalRule()
class BottomUpProbabilisticChartParser(ParserI):
    record partial results.  ``BottomUpProbabilisticChartParser`` maintains
    parsed.  ``BottomUpProbabilisticChartParser`` inserts these edges into
    ``BottomUpProbabilisticChartParser``.  Different sorting orders will
        Create a new ``BottomUpProbabilisticChartParser``, that uses
            raise ValueError("The grammar must be probabilistic WeightedGrammar")
        chart = Chart(list(tokens))
        bu_init = ProbabilisticBottomUpInitRule()
        bu = ProbabilisticBottomUpPredictRule()
        fr = SingleEdgeProbabilisticFundamentalRule()
        # Get a list of complete parses.
        parses = chart.parses(grammar.start(), ProbabilisticTree)
        :type queue: list(Edge)
class InsideChartParser(BottomUpProbabilisticChartParser):
        :type queue: list(Edge)
# class InsideOutsideParser(BottomUpProbabilisticChartParser):
#         BottomUpProbabilisticChartParser.__init__(self, grammar, trace)
class RandomChartParser(BottomUpProbabilisticChartParser):
class UnsortedChartParser(BottomUpProbabilisticChartParser):
class LongestChartParser(BottomUpProbabilisticChartParser):
    A demonstration of the probabilistic parsers.  The user is
    # Define a list of parsers.  We'll use all parsers.
        # A sequence of edge lists contained in this chart.
        self._edgelists = tuple([] for x in self._positions())
        # The set of child pointer lists associated with each edge.
        # Indexes mapping attribute values to lists of edges
        return list(self.iteredges())
        return (edge for edgelist in self._edgelists for edge in edgelist)
        edgelist = self._edgelists[end]
        if restrictions=={}: return iter(edgelist)
        for end, edgelist in enumerate(self._edgelists):
            for edge in edgelist:
        self._edgelists[edge.end()].append(edge)
        edgelist = self._edgelists[end]
        if restrictions=={}: return iter(edgelist)
        for end, edgelist in enumerate(self._edgelists):
            for edge in edgelist:
        tokens = list(tokens)
            agenda = list(chart.select(end=end))
                        new_edges = list(new_edges)
      is the span, and ``sentence`` is the list of tokens in the
        :type rhs: list(Nonterminal and str)
            ``sentence`` is the list of tokens in the sentence, then
    the chart associates each edge with a set of child pointer lists.
    A child pointer list is a list of the edges that license an
    :ivar _edges: A list of the edges in the chart
        of child pointer lists that are associated with that edge.
        attribute values to lists of edges.
        :type tokens: list
        # A list of edges contained in this chart.
        # The set of child pointer lists associated with each edge.
        # Indexes mapping attribute values to lists of edges
        Return a list of the leaf values of each word in the
        :rtype: list(str)
        Return a list of all edges in this chart.  New edges
        will *not* be contained in this list.
        :rtype: list(EdgeI)
        cpls = self.child_pointer_lists(previous_edge)
    def insert(self, edge, *child_pointer_lists):
        ``child_pointer_lists`` with ``edge``.
        :type child_pointer_lists: sequence of tuple(EdgeI)
        :param child_pointer_lists: A sequence of lists of the edges that
            were used to form this edge.  This list is used to reconstruct
            # Add it to the list of edges.
        # Get the set of child pointer lists for this edge.
        for child_pointer_list in child_pointer_lists:
            child_pointer_list = tuple(child_pointer_list)
            if child_pointer_list not in cpls:
                cpls[child_pointer_list] = True
    # Tree extraction & child pointer lists
        Return a list of the complete tree structures that span
        Return a list of the tree structures that are associated
        :rtype: list(Tree)
        # Each child pointer list can be used to form trees.
        for cpl in self.child_pointer_lists(edge):
        # Return the list of trees.
        :param child_choices: A list that specifies the options for
            each child.  In particular, ``child_choices[i]`` is a list of
        children_lists = [[]]
                children_lists = [child_list+[child]
                                  for child_list in children_lists]
                children_lists = [child_list+[child_choice]
                                  for child_list in children_lists]
        return children_lists
    def child_pointer_lists(self, edge):
        Return the set of child pointer lists for the given edge.
        Each child pointer list is a list of edges that have
        :rtype: list(list(EdgeI))
        chart.  Return a list of the edges that were added.
        :type edges: list(EdgeI)
        :rtype: list(EdgeI)
        :type edges: list(EdgeI)
        chart to the chart.  Return a list of the edges that were added.
        :rtype: list(EdgeI)
        return list(self.apply_iter(chart, grammar, *edges))
        return list(self.apply_everywhere_iter(chart, grammar))
    A generic chart parser.  A "strategy", or list of
        :type strategy: list(ChartRuleI)
        :param strategy: A list of rules that should be used to decide
        :type tokens: list(str)
        tokens = list(tokens)
                        new_edges = list(new_edges)
        # Return a list of complete parses.
                          "use BottomUpProbabilisticChartParser instead",
        self._chart = Chart(list(tokens))
        :type strategy: list(ChartRuleI)
        :param strategy: A list of rules that should be used to decide
        tokens = list(tokens)
        # Return a list of complete parses.
        :type sent: list(str)
        :return: A list of parse trees that represent possible
        structures for the given sentence.  When possible, this list is
        specified, then the returned list will contain at most ``n``
        :type sent: list(str)
        :rtype: list(Tree)
            return list(itertools.islice(self.iter_parse(sent), n))
        this list is sorted from most likely to least likely.
        :type sent: list(str)
        :type sent: list(str)
        :rtype: list(Tree)
        :rtype: list(list(Tree))
        :rtype: list(iter(Tree))
        :rtype: list(ProbDistI(Tree))
        """A list of the ``Collections`` or ``Packages`` directly
        """A list of ``Packages`` contained by this collection or any
    def list(self, download_dir=None, show_packages=True,
        # If they gave us a list of ids, then download each one.
        if isinstance(info_or_id, (list,tuple)):
            for msg in self._download_list(info_or_id, download_dir, force):
    def _download_list(self, items, download_dir, force):
                'd) Download', 'l) List', ' u) Update', 'c) Config', 'h) Help', 'q) Quit')
                    self._ds.list(self._ds.download_dir, header=False,
                print('Download which package (l=list; x=cancel)?')
                    self._ds.list(self._ds.download_dir, header=False,
        print('  l) List packages & collections          h) Help')
    """A list of the names of columns.  This controls the order in
       Default weight (for columns not explicitly listed) is 1."""
       listed) is specified by ``DEFAULT_COLUMN_WIDTH``."""
    """The default width for columns that are not explicitly listed
                            highlightthickness=0, listbox_height=16,
        self._table.bind_to_listboxes('<Double-Button-1>',
        Given a package, return a list of values describing that
    # Get lists of directories & files
    namelist = zf.namelist()
    dirlist = set()
    for x in namelist:
            dirlist.add(x)
            dirlist.add(x.rsplit('/',1)[0] + '/')
    filelist = [x for x in namelist if not x.endswith('/')]
    for dirname in sorted(dirlist):
    for i, filename in enumerate(filelist):
        if verbose and (i*10/len(filelist) > (i-1)*10/len(filelist)):
        unzipped_size = sum(zf_info.file_size for zf_info in zf.infolist())
    collections = list(_find_collections(os.path.join(root, 'collections')))
            for name in zf.namelist() ):
    Helper for ``build_index()``: Yield a list of ElementTree.Element
    Helper for ``build_index()``: Yield a list of tuples
                        for name in zf.namelist() ):
    pprint(list(islice(data, start, end)))
    Pretty print a list of text tokens, breaking lines on whitespace
    :type tokens: list
        defaultdict.__init__(self, list)
    inverted_dict = defaultdict(list)
# FLATTEN LISTS
    Flatten a list.
    :param args: items and lists to be combined into a single list
    :rtype: list
        if not isinstance(l, (list, tuple)): l = [l]
            if isinstance(item, (list, tuple)):
    :rtype: list(tuple)
    sequence = list(sequence)
    :rtype: list(tuple)
    :rtype: list(tuple)
        >>> list(ingrams([1,2,3,4,5], 3))
    Use ngrams for a list version of this function.  Set pad_left
        >>> list(ingrams([1,2,3,4,5], 2, pad_right=True))
        >>> list(ibigrams([1,2,3,4,5]))
    Use bigrams for a list version of this function.
        >>> list(itrigrams([1,2,3,4,5]))
    Use trigrams for a list version of this function.
        # returns iterator under python 3 and list under python 2
                assert isinstance(keys, list)
                       isinstance(data, list)
                elif isinstance(data, list):
        """Return the number of times this list contains ``value``."""
        list that is greater than or equal to ``start`` and less than
        slice bounds -- i.e., they count from the end of the list."""
        raise ValueError('index(x): x not in list')
        """Return true if this list contains ``value``."""
        """Return a list concatenating self with other."""
        """Return a list concatenating other with self."""
        """Return a list concatenating self with itself ``count`` times."""
        """Return a list concatenating self with itself ``count`` times."""
        similar to a list's representation; but if it would be more
        return (type(self) == type(other) and list(self) == list(other))
        return list(self) < list(other)
        of a list) or greater than the length of ``source``.
            return list(islice(source.iterate_from(start), stop-start))
    A lazy sequence formed by concatenating a list of lists.  This
    underlying list of lists may itself be lazy.  ``LazyConcatenation``
    between offsets in the concatenated lists and offsets in the
    sublists.
    def __init__(self, list_of_lists):
        self._list = list_of_lists
        if len(self._offsets) <= len(self._list):
            sublist_index = bisect.bisect_right(self._offsets, start_index)-1
            sublist_index = len(self._offsets)-1
        index = self._offsets[sublist_index]
        # Construct an iterator over the sublists.
        if isinstance(self._list, AbstractLazySequence):
            sublist_iter = self._list.iterate_from(sublist_index)
            sublist_iter = islice(self._list, sublist_index, None)
        for sublist in sublist_iter:
            if sublist_index == (len(self._offsets)-1):
                assert index+len(sublist) >= self._offsets[-1], (
                self._offsets.append(index+len(sublist))
                assert self._offsets[sublist_index+1] == index+len(sublist), (
                        'inconsistent list value (num elts)')
            for value in sublist[max(0, start_index-index):]:
            index += len(sublist)
            sublist_index += 1
    function to each element in one or more underlying lists.  The
    list, ``LazyMap`` will calculate that value by applying its
    function to the underlying lists' value(s).  ``LazyMap`` is
        >>> list(LazyMap(function, sequence))
    Like the Python ``map`` primitive, if the source lists do not have
    if the underlying list's values are constructed lazily, as is the
    def __init__(self, function, *lists, **config):
            elements of ``lists``.  It should take as many arguments
            as there are ``lists``.
        :param lists: The underlying lists.
        if not lists:
        self._lists = lists
        # in case n >= 1 list is an AbstractLazySequence.  Presumably this
                             for lst in lists) == len(lists)
        # Special case: one lazy sublist
        if len(self._lists) == 1 and self._all_lazy:
            for value in self._lists[0].iterate_from(index):
        # Special case: one non-lazy sublist
        elif len(self._lists) == 1:
                try: yield self._func(self._lists[0][index])
        # Special case: n lazy sublists
            iterators = [lst.iterate_from(index) for lst in self._lists]
                if elements == [None] * len(self._lists):
                try: elements = [lst[index] for lst in self._lists]
                    elements = [None] * len(self._lists)
                    for i, lst in enumerate(self._lists):
                    if elements == [None] * len(self._lists):
            sliced_lists = [lst[index] for lst in self._lists]
            return LazyMap(self._func, *sliced_lists)
        return max(len(lst) for lst in self._lists)
    element from each of the argument sequences.  The returned list is
    list, ``LazyZip`` will calculate that value by forming a tuple from
        >>> list(LazyZip(sequence1, sequence2))
        >>> list(zip(*sequences)) == list(LazyZip(*sequences))
    def __init__(self, *lists):
        :param lists: the underlying lists
        :type lists: list(list)
        LazyMap.__init__(self, lambda *elts: elts, *lists)
        return min(len(lst) for lst in self._lists)
    useful for obtaining an indexed list. The tuples are constructed lazily
    -- i.e., when you read a value from the list, ``LazyEnumerate`` will
        >>> list(enumerate(sequence))
        >>> list(LazyEnumerate(sequence))
    list for a long sequence of values.  By constructing tuples lazily and
        :param lst: the underlying list
        :type lst: list
# regular expression, and a list of possible responses,
        Initialize the chatbot.  Pairs is a list of patterns and responses.  Each
        e.g. r'I like (.*)'.  For each such pattern a list of possible responses
        :type pairs: list of tuple
# for each match, a list of possible responses is provided
# e.g. "are you listening?", "are you a duck"
    - feature lists, implemented by ``FeatList``, act like Python
      lists.  Feature identifiers are integers.
directly to simple Python dictionaries and lists, rather than to
full-fledged ``FeatDict`` and ``FeatList`` objects.  In other words,
Python ``dicts`` and ``lists`` can be used as "light-weight" feature
  - Python dictionaries & lists ignore reentrance when checking for
    keys in hash tables.  Python dictionaries and lists can not.
    Python dictionaries and lists do not.
  - FeatStructs may *not* be mixed with Python dictionaries and lists
    and ``cyclic()``, which are not available for Python dicts and lists.
      - feature lists, implemented by ``FeatList``, act like Python
        lists.  Feature identifiers are integers.
        or the ``FeatList`` class.
              - FeatStruct(sequence) -> FeatList(sequence)
        # whether to create a FeatDict or a FeatList, based on the
                    return FeatList.__new__(FeatList, features, **morefeatures)
                return FeatList.__new__(FeatList, features)
    # really lists.  (Lists are treated as mappings from ints to vals)
            parsed using ``FeatStructParser``.  If ``features`` is a list of
            dictionary.  If a feature is listed under both ``features`` and
            raise ValueError('Expected mapping or list of tuples')
        :return: A list of lines composing a string representation of
            elif isinstance(fval, FeatList):
# Feature List
class FeatList(FeatStruct, list):
    A list of feature values, where each feature value is either a
    Feature lists may contain reentrant feature values.  A "reentrant
    multiple feature paths.  Feature lists may also be cyclic.
    Two feature lists are considered equal if they assign the same
        Create a new feature list, with the specified features.
        :param features: The initial list of features for this feature
            list.  If ``features`` is a string, then it is paresd using
            list.__init__(self, features)
    #{ List methods
            return list.__getitem__(self, name_or_path)
            return list.__delitem__(self, name_or_path)
            return list.__setitem__(self, name_or_path, value)
#    __delslice__ = _check_frozen(list.__delslice__, '               ')
#    __setslice__ = _check_frozen(list.__setslice__, '               ')
    __iadd__ = _check_frozen(list.__iadd__)
    __imul__ = _check_frozen(list.__imul__)
    append = _check_frozen(list.append)
    extend = _check_frozen(list.extend)
    insert = _check_frozen(list.insert)
    pop = _check_frozen(list.pop)
    remove = _check_frozen(list.remove)
    reverse = _check_frozen(list.reverse)
    sort = _check_frozen(list.sort)
    def _keys(self): return list(range(len(self)))
        items = list(fstruct.items())
        items = list(enumerate(fstruct))
                             "dicts and lists is not supported.")
    Return a list of the feature paths of all features which are
    :rtype: list(tuple)
    conflict_list = []
        conflict_list.append(path)
    return conflict_list
    if isinstance(obj, (dict, list)): return (dict, list)
                sum([list(elt.variables()) for elt in self
        # If the resulting list contains no variables, then
            return list(values)[0]
        # If the resulting list contains no variables, then
            return list(values)[0]
    Helper function -- return a copy of list, with all elements of
                 flist_class=FeatList, logic_parser=None):
        self._flist_class = flist_class
    # This one is used to distinguish fdicts from flists:
                fstruct = self._flist_class()
            return self._partial_parse_featlist(s, position, match,
    def _partial_parse_featlist(self, s, position, match,
        # Build a list of the features defined by the structure.
        # Build a list of the features defined by the structure.
    l: List all feature structures
    This demo will repeatedly present you with a list of feature
    new feature structure is generated, it is added to the list of
    want to see the complete lists, type "l".  For a list of valid
    def list_fstructs(fstructs):
        # Pick 5 feature structures at random from the master list.
        list_fstructs(fstructs)
                        list_fstructs(all_fstructs); continue
__all__ = ['FeatStruct', 'FeatDict', 'FeatList', 'unify', 'subsumes', 'conflicts',
To convert a chunk structure back to a list of tokens, simply use the
    chunk the given list of tagged tokens.
    given list of tagged sentences, each consisting of a list of tagged tokens.
    def _english_wordlist(self):
            wl = self._en_wordlist
            self._en_wordlist = set(words.words('en-basic'))
            wl = self._en_wordlist
            'en-wordlist': (word in self._english_wordlist()),
    Expected input: list of pos-tagged words
        Convert a list of tagged tokens to a chunk-parse tree.
        Convert a chunk-parse tree to a list of tagged tokens.
    # Read the xml file, and get a list of entities
    :type _tp: list(Token)
    :ivar _tp: List of true positives
    :type _fp: list(Token)
    :ivar _fp: List of false positives
    :type _fn: list(Token)
    :ivar _fn: List of false negatives
        structures, listed in input order.
        :rtype: list of chunks
        chunks = list(self._fn)
        but not in the correct chunk structures, listed in input order.
        :rtype: list of chunks
        chunks = list(self._fp)
        chunk structures, listed in input order.
        :rtype: list of chunks
        chunks = list(self._correct)
        chunk structures, listed in input order.
        :rtype: list of chunks
        chunks = list(self._guessed)
    Return a list of 3-tuples containing ``(word, tag, IOB-tag)``.
    :rtype: list(tuple)
    # return the empty list in place of a Tree
    ``ChunkString`` are created from tagged texts (i.e., lists of
    :type _pieces: list(tagged tokens and chunks)
            list of tokens.  If this is true, then ``_verify`` will
        # Use this alternating list to create the chunkstruct.
            # Find the list of tokens contained in this piece.
            # Add this list of tokens to our pieces.
        lst = list(str)
    :type _rules: list(RegexpChunkRule)
    :ivar _rules: The list of rules that should be applied to a text.
        :type rules: list(RegexpChunkRule)
        :rtype: list(RegexpChunkRule)
    :ivar _stages: The list of parsing stages corresponding to the grammar
        :param grammar: The grammar, or a list of RegexpChunkParser objects
        :type grammar: str or list(RegexpChunkParser)
            type_err = ('Expected string or list of RegexpChunkParsers '
            try: grammar = list(grammar)
        :param tokens: The list of (word, tag) tokens to be chunked.
        :type tokens: list(tuple)
        :type gold: list(Tree)
        :param gold: The list of chunked sentences to score the chunker on.
from nltk.probability import ProbabilisticMixIn
class Tree(list):
    A tree's children are encoded as a list of leaves and subtrees,
        specified node value and list of children.
                raise TypeError("%s: Expected a node value and child list "
            list.__init__(self, tree)
            raise TypeError("%s() argument 2 should be a list, not a "
            list.__init__(self, children)
                (self.node, list(self)) == (other.node, list(other)))
            return (self.node, list(self)) < (other.node, list(other))
    # Disabled list operations
            return list.__getitem__(self, index)
        elif isinstance(index, (list, tuple)):
            return list.__setitem__(self, index, value)
        elif isinstance(index, (list, tuple)):
            return list.__delitem__(self, index)
        elif isinstance(index, (list, tuple)):
        :return: a list containing this tree's leaves.
        :rtype: list
        :rtype: list(Production)
        :return: a list of tuples containing leaves and pre-terminals (part-of-speech tags).
        :rtype: list(tuple)
        stack = [(None, [])] # list of (node, children) tuples
        child list of ``self``.
        child list of ``self``.
            # Delete the children from our child list.
            # Remove the child from our child list.
        elif isinstance(index, (list, tuple)):
            if not isinstance(value, (list, tuple)):
                value = list(value)
            # finally, update the content of the child list itself.
            # Update our child list.
        elif isinstance(index, (list, tuple)):
        # is done for consistency with list.__getitem__ and list.index.
        # Set the child's parent, and update our child list.
    # n.b.: like `list`, this is done by equality, not identity!
    # they're deprecated, because otherwise list.__getslice__ will get
    # called (since we're subclassing from list).  Just delegate to
    if hasattr(list, '__getslice__'):
        """A list of this tree's parents.  This list should not
        :type: list(MultiParentedTree)
        return list(self._parents)
        A list of all left siblings of this tree, in any of its parent
        appear multiple times in this list if it is the left sibling
        :type: list(MultiParentedTree)
        A list of all right siblings of this tree, in any of its parent
        appear multiple times in this list if it is the right sibling
        :type: list(MultiParentedTree)
        :type: list(MultiParentedTree)
        return list(self._get_roots_helper({}).values())
        Return a list of the indices where this tree occurs as a child
        ``parent``, then the empty list is returned.  The following is
        Return a list of all tree positions that can be used to reach
        # self from child's parent list.
        # Add self as a parent pointer if it's not already listed.
## Probabilistic trees
class ProbabilisticTree(Tree, ProbabilisticMixIn):
        ProbabilisticMixIn.__init__(self, **prob_kwargs)
    def _frozen_class(self): return ImmutableProbabilisticTree
            if isinstance(val, ProbabilisticMixIn):
                (self.node, list(self), self.prob()) ==
                (other.node, list(other), other.prob()))
            return ((self.node, list(self), self.prob()) <
                    (other.node, list(other), other.prob()))
class ImmutableProbabilisticTree(ImmutableTree, ProbabilisticMixIn):
        ProbabilisticMixIn.__init__(self, **prob_kwargs)
    def _frozen_class(self): return ImmutableProbabilisticTree
            if isinstance(val, ProbabilisticMixIn):
    # Demonstrate probabilistic trees.
    pt = tree.ProbabilisticTree('x', ['y', 'z'], prob=0.5)
    print("Probabilistic Tree:")
__all__ = ['ImmutableProbabilisticTree', 'ImmutableTree', 'ProbabilisticMixIn',
           'ProbabilisticTree', 'Tree', 'bracket_parse',
    :type words: list(str)
    :type mots: list(str)
            positions = list(range(len(self._index)))
        Build a list self._index such that self._index[i] is a list
    :type aligned_sents: list(AlignedSent)
        Return a list of AlignedSents with Alignments calculated using
            # we are only interested in tkinter modules listed
        :type parameters: list(tuple(str, str, str))
        :rtype list
stylistic patterns that Chomsky is noted for.
# List of LEADINs to buy time.
# List of SUBJECTs chosen for maximum professorial macho.
#List of VERBs chosen for autorecursive obfuscation.
# List of OBJECTs selected for profound sententiousness.
        phraselist = list(map(str.strip, part.splitlines()))
        random.shuffle(phraselist)
        parts.append(phraselist)
    :param words: the list of words to be put into the grid
    :type words: list
    :type alph: list
    wordlist = words.words()
    random.shuffle(wordlist)
    wordlist = wordlist[:200]
    wordlist = [w for w in wordlist if 3 <= len(w) <= 12]
    grid, used = wordfinder(wordlist)
# Natural Language Toolkit: List Sorting
This module provides a variety of list sorting algorithms, to
    Selection Sort: scan the list to find its smallest element, then
    swap it with the first element.  The remainder of the list is one
    element smaller; apply the same method to this list, and so on.
    Bubble Sort: compare adjacent elements of the list left-to-right,
    the list swapping adjacent items, the largest item will be in
    apply the same method to this list, and so on.
def _merge_lists(b, c):
    Merge Sort: split the list in half, and sort each half, then
        result, count_a = _merge_lists(b, c)
        a = list(range(size))
"""A list of directories where the NLTK data package might reside.
                # Sometimes directories aren't explicitly listed in
                    [n for n in zipfile.namelist() if n.startswith(entry)]):
    'pcfg': "A probabilistic CFG, parsed by nltk.parse_pcfg().",
    'fol': "A list of first order logic expressions, parsed by "
    'logic': "A list of first order logic expressions, parsed by "
      - ``pcfg`` (probabilistic CFGs)
           ``readline()``.  This buffer consists of a list of unicode
           The final element of the list may or may not be a complete
        encoding, and return it as a list of unicode lines.
        :rtype: list(unicode)
    :param options: A list of options that should be passed to the
        specified, then do not modify the options list.
    :type options: list(str)
        _java_options = list(options)
        a list of strings.  Typically, the first string will be the name
    :type cmd: list(str)
    :param classpath: A ``':'`` separated list of directories, JAR
        raise TypeError('cmd should be a list of strings')
    cmd = list(cmd)
    Return the method resolution order for ``cls`` -- i.e., a list
    :param env_vars: A list of environment variable names to check.
    :param file_names: A list of alternative file names to check.
    :param searchpath: List of directories to search.
    # Check the path list.
    :param env_vars: A list of environment variable names to check.
    :param file_names: A list of alternative file names to check.
    :param searchpath: List of directories to search.
    :param env_vars: A list of environment variable names to check
    :param searchpath: List of directories to search.
    env_vars = ['CLASSPATH'] + list(env_vars)
    # Check the path list.
    # Make sure stop doesn't go past the end of the list.  Note that
    whose CFG production is listed earliest in the grammar.
listed earliest in the grammar.  There are two ways to manually choose
  - Click on a CFG production from the list of available reductions,
      [g]\t Show/hide available production list
from tkinter import (IntVar, Listbox, Button, Frame, Label, Menu,
        self._prodframe = listframe = Frame(parent)
        self._prodlist_label = Label(self._prodframe,
        self._prodlist_label.pack()
        self._prodlist = Listbox(self._prodframe, selectmode='single',
        self._prodlist.pack(side='right', fill='both', expand=1)
        self._productions = list(self._parser.grammar().productions())
            self._prodlist.insert('end', (' %s' % production))
        self._prodlist.config(height=min(len(self._productions), 25))
            listscroll = Scrollbar(self._prodframe,
            self._prodlist.config(yscrollcommand = listscroll.set)
            listscroll.config(command=self._prodlist.yview)
            listscroll.pack(side='left', fill='y')
        self._prodlist.bind('<<ListboxSelect>>', self._prodlist_select)
        self._prodlist.bind('<Motion>', self._highlight_hover)
        self._prodlist.bind('<Leave>', self._clear_hover)
        self._prodlist.selection_clear(0, 'end')
            self._prodlist.selection_set(index)
        #self._prodlist['font'] = ('helvetica', -size)
        #self._prodlist_label['font'] = ('helvetica', -size-2, 'bold')
        self._productions = list(grammar.productions())
        self._prodlist.delete(0, 'end')
            self._prodlist.insert('end', (' %s' % production))
    def _prodlist_select(self, event):
        selection = self._prodlist.curselection()
            self._prodlist.selection_clear(0, 'end')
                self._prodlist.selection_set(index)
        index = self._prodlist.nearest(event.y)
        selection = [int(s) for s in self._prodlist.curselection()]
        top.title('NLTK Collocations List')
        other_corpora = list(self.model.CORPORA.keys()).remove(self.model.DEFAULT_CORPUS)
        copy.extend(list(self.CORPORA.keys()))
                self.model.collocations = list(map(itemgetter(0), scored))
list of available expansions.
The parser maintains a list of tree locations called a "frontier" to
is listed earliest in the grammar.  To manually choose which expansion
to apply, click on a CFG production from the list of available
      [g]\t Show/hide available expansions list
from tkinter import (Listbox, IntVar, Button,
        self._prodframe = listframe = Frame(parent)
        self._prodlist_label = Label(self._prodframe, font=self._boldfont,
        self._prodlist_label.pack()
        self._prodlist = Listbox(self._prodframe, selectmode='single',
        self._prodlist.pack(side='right', fill='both', expand=1)
        self._productions = list(self._parser.grammar().productions())
            self._prodlist.insert('end', ('  %s' % production))
        self._prodlist.config(height=min(len(self._productions), 25))
            listscroll = Scrollbar(self._prodframe,
            self._prodlist.config(yscrollcommand = listscroll.set)
            listscroll.config(command=self._prodlist.yview)
            listscroll.pack(side='left', fill='y')
        self._prodlist.bind('<<ListboxSelect>>', self._prodlist_select)
        self._highlight_prodlist()
        self._highlight_prodlist()
        # Highlight the list of nodes to be checked.
    def _highlight_prodlist(self):
        # Boy, too bad tkinter doesn't implement Listbox.itemconfig;
        self._prodlist.delete(0, 'end')
                    self._prodlist.insert(index, ' %s' % productions[index])
                    self._prodlist.insert(index, ' %s (TRIED)' %
                self._prodlist.selection_set(index)
                self._prodlist.insert(index, ' %s' % productions[index])
            self._prodlist.selection_clear(0, 'end')
            self._prodlist.selection_set(index)
    def _prodlist_select(self, event):
        selection = self._prodlist.curselection()
            self._prodlist.selection_clear(0, 'end')
            self._prodlist.selection_set(index)
            self._prodlist.selection_clear(0, 'end')
                self._prodlist.selection_set(index)
        self._productions = list(grammar.productions())
        self._prodlist.delete(0, 'end')
            self._prodlist.insert('end', (' %s' % production))
from nltk.draw.util import (CanvasFrame, ColorizedList,
# Edge List
class EdgeList(ColorizedList):
        self._init_list(self._root)
    def _init_list(self, root):
        self._list = EdgeList(root, [], width=20, height=5)
        self._list.pack(side='top', expand=1, fill='both', pady=3)
        self._list.add_callback('select', cb)
        self._list.focus()
        # Update the edge list.
        edges = list(self._chart.select(span=self._selected_cell))
        self._list.set(edges)
        for cb_func in list(self._callbacks[event].keys()): cb_func(*args)
        self._list.set([])
        self._list.view(edge)
        self._list.mark(edge)
        self._list.unmark(edge)
        self._list.markonly(edge)
            cv1_frame, list(self._charts.keys()), command=self._select_left)
            cv2_frame, list(self._charts.keys()), command=self._select_right)
    :ivar _sentence: The list of tokens that the chart spans.
    :ivar _treetags: A list of all the tags that make up the tree;
    :ivar _edgelevels: A list of edges at each level of the chart (the
        top level is the 0th element).  This list is used to remember
            old_marked_edges = list(self._marks.keys())
        # If it's a new edge, then get a new list of treetoks.
        for cb_func in list(self._callbacks[event].keys()): cb_func(*args)
        self._tokens = list(sentence.split())
    tokens = list(sent.split())
        'LS':   'List item marker',           'VBD':  'Verb, past tense',
    #: Contents for the help box.  This is a list of tuples, one for
    #:     for a list of tags you can use for colorizing.
        :param devset: A list of chunked sentences
        """The development set -- a list of chunked sentences."""
        """A list of (grammar, precision, recall, fscore) tuples for
                    ('\t%s\t%s' % item for item in sorted(list(self.tagset.items()),
        other_corpora = list(self.model.CORPORA.keys()).remove(self.model.DEFAULT_CORPUS)
        copy.extend(list(self.CORPORA.keys()))
    :param port: The port number for the server to listen on, defaults to
    Return a HTML unordered list of synsets for the given word and
            # It's probably a tuple containing a Synset and a list of
            raise TypeError("r must be a synset, lemma or list, it was: type(r) = %s, r = %s" % (type(r), r))
        synset relation identifaiers to unfold a list of synset
    pos_forms = defaultdict(list)
    # Traverse the tree depth-first keeping a list of ancestor nodes to the root.
    nodeList = [(tree, [tree.node])]
    while nodeList != []:
        node, parent = nodeList.pop()
                nodeList.append((child, parent))
    nodeList = [(tree,[])]
    while nodeList != []:
        node,parent = nodeList.pop()
                nodeList.append((child,node))
        nodeList = [tree[0]]
        nodeList = [tree]
    while nodeList != []:
        node = nodeList.pop()
                nodeList.append(node)
                    nodeList.append(child)
    - argnames (the names of the arguments : list)
    argnames = list(regargs)
##   notice, this list of conditions and the following disclaimer.
##   notice, this list of conditions and the following disclaimer in
print("Type: 'texts()' or 'sents()' to list the materials.")
Tokenizers divide strings into lists of substrings.  For example,
tokenizers can be used to find the list of sentences or words in a
    >>> list(WhitespaceTokenizer().span_tokenize(s))
    :param stopwords: A list of stopwords that are filtered out (defaults to NLTK's stopwords corpus)
    :type stopwords: list(str)
            ts.wrdindex_list = [wi for wi in ts.wrdindex_list
        return list(smooth(numpy.array(gap_scores[:]),
        wrdindex_list = []
            wrdindex_list.append((match.group(), match.start()))
        return [TokenSequence(i/w, wrdindex_list[i:i+w])
                for i in range(0, len(wrdindex_list), w)]
            for word, index in ts.wrdindex_list:
    "A token list with its original length and its index"
                 wrdindex_list,
        original_length=original_length or len(wrdindex_list)
        >>> list(string_span_tokenize(s, " "))
        >>> list(WhitespaceTokenizer().span_tokenize(s))
        >>> list(spans_to_relative(WhitespaceTokenizer().span_tokenize(s)))
        return list(s)
        - ``discard``: strip blank lines out of the token list before returning it.
        - ``keep``: leave all blank lines in the token list.
listed as their own s-expression; and the last partial sexpr with
unmatched open parentheses will be listed as its own sexpr:
        two-character string, or a list of two strings.
    :type parens: str or list
        Return a list of s-expressions extracted from *text*.
        unmatched close parentheses will be listed as their own
        parentheses will be listed as its own s-expression:
This tokenizer divides a text into a list of sentences,
However, Punkt is designed to learn parameters (a list of abbreviations, etc.)
:class:`.PunktTrainer` learns parameters such as a list of abbreviations
        with eval(), which lists all the token's non-default
        segmentation regular expression, and generate the resulting list
        called. If verbose is True, abbreviations found will be listed.
        Collects training data from a given list of tokens.
        # Ensure tokens are a list
        tokens = list(tokens)
        tokens = list(self._annotate_first_pass(tokens))
        tokens = list(tokens)
        Given a text, returns a list of the sentences in that text.
        return list(self.sentences_from_text(text, realign_boundaries))
            tokens = list(self._annotate_first_pass(tokens))
        Given a text, returns a list of the (start, end) spans of sentences
        return self._build_sentence_list(text, tokens)
        Given a sequence of tokens, generates lists of tokens, each list
        #tokens = list(tokens)
    def _build_sentence_list(self, text, tokens):
        Given the original text and the list of augmented word tokens,
        construct and return a tokenized list of sentence strings.
            # frequent-sentence-starters list, then label tok as a
    # List of contractions adapted from Robert MacIntyre's tokenizer.
        :rtype: list of str
        :rtype: list(list(str))
        :rtype: iter(list(tuple(int, int)))
            yield list(self.span_tokenize(s))
    Given a list of reference values and a corresponding list of test
    :type reference: list
    :param reference: An ordered list of reference values.
    :type test: list
    :param test: A list of values to compare against the corresponding
        raise ValueError("Lists must have the same length.")
    Given a list of reference values and a corresponding list of test
    :param reference: A list of reference values
    :type reference: list
    :param test: A list of probability distributions over values to
    :type test: list(ProbDistI)
        raise ValueError("Lists must have the same length.")
    Returns an approximate significance level between two lists of
    statistic of the permutated lists varies from the actual statistic of
    the unpermuted argument lists.
    :param a: a list of test values
    :type a: list
    :param b: another list of independently generated test values
    :type b: list
    indices = list(range(len(a) + len(b)))
The simplest way to initialize an AnnotationTask is with a list of triples,
Note that the data list needs to contain the same number of triples for each
        The argument is a list of 3-tuples, each representing a coder's labeling of an item:
    The confusion matrix between a list of reference values and a
    corresponding list of test values.  Entry *[r,t]* of this
        Construct a new confusion matrix from a list of reference
        values and a corresponding list of test values.
        :type reference: list
        :param reference: An ordered list of reference values.
        :type test: list
        :param test: A list of values to compare against the
            raise ValueError('Lists must have the same length.')
        # Get a list of all values.
        #: A list of all values in ``reference`` or ``test``.
        #: The confusion matrix itself (as a list of lists of counts).
    :type seg1: str or list
    :type seg2: str or list
    :type ref: str or list
    :type hyp: str or list
    :type ref: str or list
    :type hyp: str or list
Tools for comparing ranked lists.
    present in only one list before ranking)."""
#        return pow(list(label1)[0]-list(label2)[0],2)
    primitives - The list of primitive categories for the lexicon
    entries = defaultdict(list)
While this returns a list of trees, the default representation
``chart.printCCGDerivation(<parse tree extracted from list>)``
        tokens = list(tokens)
        chart = CCGChart(list(tokens))
        for cpl in self.child_pointer_lists(edge):
    #  - Returns a list of necessary substitutions if they can.'''
        """A list of restrictions on the combinators.
    list of strings specifying the morphological subcategories.
class lazylist(list):
        # Must load data with list.append(self, v) instead of lazylist.append(v).
        """ If the list is empty, calls lazylist.load().
            Replaces lazylist.method() with list.method() and calls it.
        if list.__len__(self) == 0:
            setattr(self, method, types.MethodType(getattr(list, method), self))
        return getattr(list, method)(self, *args)
    """ Returns a list of sentences. Each sentence is a space-separated string of tokens (words).
    for a, b in list(replace.items()):
        """ Applies the rule to the given token or list of tokens.
class Morphology(lazylist, Rules):
        """ A list of rules based on word morphology (prefix, suffix).
        cmd.update(("f" + k, v) for k, v in list(cmd.items()))
        list.extend(self, (x.split() for x in _read(self._path)))
        """ Applies lexical rules to the given token, which is a [word, tag] list.
        lazylist.insert(self, i, r)
class Context(lazylist, Rules):
        """ A list of rules based on context (preceding and following words).
        list.extend(self, (x.split() for x in _read(self._path)))
        """ Applies contextual rules to the given list of tokens,
            where each token is a [word, tag] list.
        lazylist.insert(self, i, [tag1, tag2, cmd, x or "", y or ""])
        """ Applies the named entity recognizer to the given list of tokens,
            where each token is a [word, tag] list.
# Sentiment().assessments(txt) returns a list of (chunk, polarity, subjectivity, label)-tuples.
def avg(list):
    return sum(list) / float(len(list) or 1)
        for w, pos in list(words.items()):
            as a function that takes a list of words and returns a weight.
        # A list of words.
        elif isinstance(s, list):
        """ Returns a list of (chunk, polarity, subjectivity, label)-tuples for the given list of words:
            where chunk is a list of successive words: a known word optionally
    """ Returns a list of [token, tag]-items for the given list of tokens:
        tagged = [list(map(token, tag)) or [token, default[0]] for token, tag in tagged]
    """ The input is a list of [token, tag]-items.
        The output is a list of [token, tag, chunk]-items:
    """ The input is a list of [token, tag, chunk]-items.
        The output is a list of [token, tag, chunk, preposition]-items.
        """ Returns a list of sentences from the given string.
        """ Annotates the given list of tokens with part-of-speech tags.
            Returns a list of tokens, where each token is now a [word, tag]-list.
        """ Annotates the given list of tokens with chunk tags.
        """ Annotates the given list of tokens with prepositional noun phrase tags.
        """ Annotates the given list of tokens with verb/predicate tags.
        """ Annotates the given list of tokens with word lemmata.
        if isinstance(s, (list, tuple)):
        # With collapse=False (or split=True), returns raw list
        # Collapse raw list.
        # From a TaggedString.split(TOKENS) list:
        if isinstance(string, list):
        s.tags = list(tags)
        """ Returns a list of sentences, where each sentence is a list of tokens,
            where each token is a list of word + tags.
        """ Returns the given list of words filtered by known words.
        """ Return a list of (word, confidence) spelling corrections for the given word,
    :param dataset: A list of tuples of the form ``(words, label)`` where
        ``words`` is either a string of a list of tokens.
    :param train_set: Training data set, a list of tuples of the form
    :param train_set: The training set, either a list of tuples of the form
        else:  # train_set is a list of tuples
        :param test_set: A list of tuples of the form ``(text, label)``, or a
        else:  # test_set is a list of tuples
        :param new_data: New data as a list of tuples of the form
    :param train_set: The training set, either a list of tuples of the form
        '''Return the most informative features as a list of tuples of the
        :rtype: list
        '''Displays a listing of the most informative features for this
    :param train_set: The training set, either a list of tuples of the form
        :param new_positive_data: List of new, labeled strings.
        :param new_unlabeled_data: List of new, unlabeled strings.
    'list_dialects',
def _stringify_list(l, encoding, errors='strict'):
        self.writer.writerow(_stringify_list(row, self.encoding, self.encoding_errors))
        fieldnames = _stringify_list(self.fieldnames, self.encoding, self.encoding_errors)
            fieldnames = _stringify_list(fieldnames, encoding)
            self.fieldnames = _stringify_list(reader.next(), reader.encoding)
and :class:`WordList <textblob.blob.WordList>` classes.
    WordList([u'simple'])
    WordList([u'Simple', u'is', u'better', u'than', u'complex'])
        '''Return a list of (word, confidence) tuples of spelling corrections.
        '''The list of Synset objects for this Word.
        :rtype: list of Synsets
        '''The list of definitions for this word. Each definition corresponds
        '''Return a list of Synset objects for this word.
        :rtype: list of Synsets
        '''Return a list of definitions for this word. Each definition
class WordList(list):
    '''A list-like collection of words.'''
        '''Initialize a WordList. Takes a collection of strings as
        super(WordList, self).__init__(self._collection)
        """Get the count of a word or phrase `s` within this WordList.
        '''Extend WordList by appending alements from ``iterable``. If an element
        '''Return a new WordList with each word upper-cased.'''
        '''Return a new WordList with each word lower-cased.'''
        '''Return the single version of each word in this WordList.'''
        '''Return the plural version of each word in this WordList.'''
        '''Return the lemma of each word in this WordList.'''
        '''Return a list of word tokens. This excludes punctuation characters.
        return WordList(WordTokenizer().itokenize(self.raw, include_punc=False))
        '''Return a list of tokens, using this blob's tokenizer object
        return WordList(self.tokenizer.tokenize(self.raw))
        '''Return a list of tokens, using ``tokenizer``.
        return WordList(t.tokenize(self.raw))
        '''Returns a list of noun phrases for this blob.'''
        return WordList([phrase.strip().lower()
        '''Returns an list of tuples of the form (word, POS tag).
        :rtype: list of tuples
        '''Return a list of n-grams (tuples of n successive words) for this
        grams = [WordList(self.words[i:i+n])
        WordList.
        return WordList(self._strkey().split(sep, maxsplit))
        '''Return list of :class:`Sentence <Sentence>` objects.'''
        '''Return a list of word tokens. This excludes punctuation characters.
        return WordList(words)
        '''List of strings, the raw sentences in the blob.'''
        '''Returns a list of each sentence's dict representation.'''
        '''Returns a list of Sentence objects given
        a list of sentence strings. Attempts to handle sentences that
        where each token is a [word, part-of-speech] list.
            for w, pos in list(dict.items(self)):
    """ Returns a list of sentences, where punctuation marks have been split from words.
    """ Returns a list of (token, tag)-tuples from the given string.
    """ Returns a list of (word, confidence)-tuples of spelling corrections.
        '''Return a list of noun phrases (strings) for body of text.'''
        '''Return a list of noun phrases (strings) for body of text.'''
    copy = list(tagged_phrase)  # A copy of the list
    n = list(range(len(plural_rules)))
    if word in list(custom.keys()):
    for w in list(singular_irregular.keys()):
"The most simple way to add onto the end of the list is to use my_list.append(list_item) or my_list.extend(some_list) and the fastest way to remove an item is my_list.pop() or del my_list[-1].
To use an index you can use my_list.insert(index, list_item) or list.pop(index) for list removal, but these are slower."
        # OR read all the lines into a list.
print os.listdir('.') # current level
print os.listdir('..') # one level up
print os.listdir('../..') # two levels up
retCode, choice = dialog.list_menu(choices)
    The analytics side isnâ€™t as interesting for me right now, but that could be due to the fact that Iâ€™ve barely been online since I came back from the US last weekend. The graphs also havenâ€™t given me any particularly special insights as I canâ€™t see which post got the actual feedback on the graph side (however there are numbers on the Timeline side.) This is a Beta though, and new features are being added and improved daily. Iâ€™m sure this is on the list. As they say, if you arenâ€™t launching with something youâ€™re embarrassed by, youâ€™ve waited too long to launch.
    How do you choose the articles listed on the site? Is there an algorithm involved? And is there any IP?
def regexp_answer(strQuestion, listQuestion, articleFile):
	"""Takes a question list tagged and tokenized and attempts to return a true/false value for easy questions."""
	for group in listQuestion[1:-1]:
			regex += group[0] + " " + listQuestion[0][0] + " "
       returned in the *dirs* list, but it is not recursed into.
       *dirs* list (as with `os.walk()`) but it *is conditionally*
    # get a list of the files the directory contains.  os.path.walk
        names = os.listdir(top)
    Generate a list of paths (files and/or dirs) represented by the given path
        "path_patterns" is a list of paths optionally using the '*', '?' and
        "includes" is a list of file patterns to include in recursive
        "excludes" is a list of file and dir patterns to exclude.
    Typically this is useful for a command-line tool that takes a list
                    return (list(sibling.subtrees(lambda x: x.node in verb_tags))[0], steps + 2)
    subtrees = list(tree.subtrees(lambda x: value in x))
        useful_roots = list(tree.subtrees(lambda x: (x.node in semi_tree_roots) and len(x) > 1))
    first_np = list(tree.subtrees(lambda x: x.node == "NP"))[0]
    has_pro = len(list(first_np.subtrees(lambda x: x.node in pers_pro_tags))) > 0
def dir_list(dir_name, subdir, *args):
    '''Return a list of file names in directory 'dir_name'
    Additional arguments, if any, are file extensions to add to the list.
    Example usage: fileList = dir_list(r'H:\TEMP', False, 'txt', 'py', 'dat', 'log', 'jpg')
    fileList = []
    for file in os.listdir(dir_name):
                fileList.append(dirfile)
                    fileList.append(dirfile)
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
def combine_files(fileList, fn):
    for file in fileList:
    fileList = dir_list(r'/home/ahmed/alltxt/', True, 'txt' )
    combine_files(fileList, fn)
retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])
    Return a 2-tuple: (settings object, args list).
    `argv` is a list of arguments, or `None` for ``sys.argv[1:]``.
