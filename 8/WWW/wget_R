--
WGET,qq/|xargs lynx -dump/);printf WGET qq{http
WGETRC
Wget
Wget Command to Download Full Recursive Version of Web Page
wget
wget  -H -r  -nv --level=1  -k -p -erobots=off -np -N  --exclude-domains=del.icio.us,doubleclick.net --exclude-directories=
wget  -q -O- --header\="Accept-Encoding
wget "$@"; foo=`echo "$@" | sed 's
wget "http
wget $(cat topic.php*|grep "Linux x86
wget $(wget -O- -U "" "http
wget $URL | htmldoc --webpage -f "$URL".pdf - ; xpdf "$URL".pdf &
wget $file -O -" > $PWD/${file##*/}
wget $i; then zsync $i; date; break; else sleep 30; fi; done
wget $line`; done
wget $mirror/$latest
wget $u
wget && [[ ! -f $HOME/.wgetrc ]] && (
wget &>/dev/null || {
wget 'link of a Picasa WebAlbum' -O - |perl -e'while(<>){while(s/"media"
wget - Part 1
wget - Part 2
wget --auth-no-challenge --server-response --http-user="user" --http-password="pw" --header="$(cat cookie.txt)" -O- $url
wget --auth-no-challenge --server-response -O- $url 2>&1 | grep "Cookie" | sed "s/^  Set-//g" > cookie.txt;  wget --auth-no-challenge --server-response --http-user="user" --http-password="pw" --header="$(cat cookie.txt)" -O- $url
wget --continue \ http
wget --help | grep file
wget --http-user='domain\account' --http-password='###' -p -r -l 8 --no-remove-listing -P . 'http
wget --http-user=YourUsername --http-password=YourPassword http
wget --input-file=~/donwloads.txt --user="$USER" --password="$(gpg2 --decrypt ~/.gnupg/passwd/http-auth.gpg 2>/dev/null)"
wget --load-cookies <cookie-file> -c -i <list-of-urls>
wget --mirror -p --convert-links -P ./<LOCAL-DIR> <WEBSITE-URL>
wget --no-check-certificate https
wget --no-use-server-timestamps  $(curl $(curl http
wget --output-document=/dev/null http
wget --post-data '{"jsonrpc"
wget --quiet -O - checkip.dyndns.org | sed -e 's/[^
wget --quiet URL -O - | grep util-vserver | tail -n 1 | sed 's|</a>.*||;s/.*>//'`; wget URL$UTILVSERVER;
wget --random-wait -r -p -e robots=off -U mozilla http
wget --recursive  --page-requisites --convert-links www.moyagraphix.co.za
wget --referer='http
wget --reject html,htm --accept pdf,zip -rl1 url
wget --save-cookies ~/.cookies/rapidshare --post-data "login=USERNAME&password=PASSWORD" -O - https
wget --server-response --spider http
wget --spider  -o wget.log  -e robots=off --wait 1 -r -p http
wget --spider $URL 2>&1 | awk '/Length/ {print $2}'
wget --spider -v http
wget -A mp3,mpg,mpeg,avi -r -l 3 http
wget -N http
wget -O $RANDOM.jpg --quiet "$line"; done
wget -O $d.flv "$y/get_video.php?video_id=$d&t=$(curl -s "$y/watch?v=$d"|sed -n 's/.* "t"
wget -O - "[PICASA ALBUM RSS LINK]" |sed 's/</\n</g' | grep media
wget -O - "https
wget -O - --save-cookies c --post-data "username=$1&password=$2&submit=Let+me+in" $URL/users/signin;for i in `seq 0 25 $3`;do wget -O - --load-cookies c $URL/commands/favourites/plaintext/$i >>$4;done;rm -f c;}
wget -O - -q http
wget -O - -q icanhazip.com
wget -O - `wget -O - -U "Mozilla/5.0" "http
wget -O - http
wget -O /dev/null http
wget -O LICENSE.txt http
wget -O OUPUT_FILE
wget -O alsa-info.sh http
wget -O chart.png 'http
wget -O gluon/contrib/feedparser.py http
wget -O gluon/contrib/simplejsonrpc.py http
wget -O url|grep '<a rel="nofollow"'|grep http|sed 's|.*<a rel="nofollow" class="[^"]\+" href="[^"]*https\?
wget -O xkcd_$(date +%y-%m-%d).png `lynx --dump http
wget -O ~/bin/dropbox.py "http
wget -O- "127.0.0.1
wget -O- -U "" "http
wget -O- http
wget -O/dev/null -q URLtoCheck && echo exists || echo not exist
wget -O/dev/sdb ftp
wget -S $1 2>&1 | grep ^Location; }
wget -S --spider http
wget -S -O/dev/null "INSERT_URL_HERE" 2>&1 | grep Server
wget -U "Mozilla/5.0" -qO - "http
wget -U "QuickTime/7.6.2 (qtver=7.6.2;os=Windows NT 5.1Service Pack 3)" `echo http
wget -U Mozilla -q -O - "$@" translate.google.com/translate_tts?tl=en\&q=$p|mpg123 -
wget -U Mozilla -qO - "http
wget -U Mozilla -qO - $(echo "http
wget -b http
wget -c
wget -c   or wget --continue
wget -c  {};
wget -c -t 1 --load-cookies ~/.cookies/rapidshare <URL>
wget -c \1/p" > dowload_deb_list.txt
wget -c http
wget -c https
wget -e robots=off -E -H -k -K -p http
wget -erobots=off --user-agent="Mozilla/5.0 (X11; U; Linux i686; en-US; rv
wget -i ls_bashrc    
wget -i lsf.txt  
wget -i url-list.txt
wget -i- -T 10 -t 3 --waitretry 1
wget -k $URL
wget -k -r -l 5 http
wget -m -k -K -E http
wget -mk -w 20 http
wget -mnd -q http
wget -nd -r -l 2 -A jpg,jpeg,png,gif http
wget -nv http
wget -p --convert-links http
wget -q  -O - http
wget -q "http
wget -q $(lynx --dump 'http
wget -q $netcast -O - |grep enclosure | tr '\r' '\n' | tr \' \" | sed -n 's/.*url="\([^"]*\)".*/\1/p'|head -n1; done <netcast.txt)
wget -q ${url} -O - | grep rsync
wget -q --spider http
wget -q --user=<username> --password=<password> 'https
wget -q -O - "" https
wget -q -O - "$@" https
wget -q -O - "http
wget -q -O - &quot;$@&quot; https
wget -q -O - 'URL/full?orderby=starttime&singleevents=true&start-min=2009-06-01&start-max=2009-07-31' |  perl -lane '@m=$_=~m/<title type=.text.>(.+?)</g;@a=$_=~m/startTime=.(2009.+?)T/g;shift @m;for ($i=0;$i<@m;$i++){ print $m[$i].",".$a[$i];}';
wget -q -O - 'http
wget -q -O - </span><span class="Special">\&quot;</span><span class="PreProc">$@</span><span class="Special">\&quot;</span><span class="Constant"> <a href="https
wget -q -O - URL1) <(wget -q -O - URL2)
wget -q -O - \"$@\" https
wget -q -O - \&quot;$@\&quot; <a href="https
wget -q -O - `youtube-dl -b -g $url`| ffmpeg -i - -f mp3 -vn -acodec libmp3lame - > "$file.mp3"
wget -q -O - `youtube-dl -b -g $url`| ffmpeg -i - -f mp3 -vn -acodec libmp3lame -| mpg123  -
wget -q -O - `youtube-dl -g $url`| ffmpeg -i - -f mp3 -vn -acodec libmp3lame - > "$audio.mp3"
wget -q -O - http
wget -q -O - https
wget -q -O <span class="Special">-</span> <span class="Special">\&quot;</span><span class="PreProc">$@</span><span class="Special">\&quot;</span> <a href="https
wget -q -O xkcd-${nn}.json http
wget -q -O- PAGE_URL | grep -o 'WORD_OR_STRING' | wc -w
wget -q -U "Mozilla/5.0" --post-file message.flac --header="Content-Type
wget -q -U "Mozilla/5.0" --post-file speech.flac --header="Content-Type
wget -q -U Mozilla -O "${FILE}" "${URL}${TEXT}"
wget -q -U Mozilla -O output.mp3 "http
wget -q -U busybox -O- "http
wget -q http
wget -q ip.nu && cat index.html
wget -qO - "http
wget -qO - 'http
wget -qO - --post-data "data[Row][clear]=text" http
wget -qO - --post-data "data[Row][cripted]=1cb251ec0d568de6a929b520c4aed8d1" http
wget -qO - http
wget -qO - sometrusted.web.site/tmp/somecommand | sh
wget -qO - www.commandlinefu.com/commands/random | grep "<div class=\"command\">" | sed 's/<[^>]*>//g; s/^[ \t]*//; s/&quot;/"/g; s/&lt;/</g; s/&gt;/>/g; s/&amp;/\&/g'
wget -qO - www.ip2location.com/$1 | grep "<span id=\"dgLookup__ctl2_lblICountry\">" | sed 's/<[^>]*>//g; s/^[\t]*//; s/&quot;/"/g; s/</</g; s/>/>/g; s/&amp;/\&/g'; }
wget -qO- "$url" | grep "$txt"`" ]; do
wget -qO- "VURL" | grep -o "googleplayer.swf?videoUrl\\\x3d\(.\+\)\\\x26thumbnailUrl\\\x3dhttp" | grep -o "http.\+" | sed -e's/%\([0-9A-F][0-9A-F]\)/\\\\\x\1/g' | xargs echo -e | sed 's/.\{22\}$//g' | xargs wget -O OUPUT_FILE
wget -qO- "http
wget -qO- $(wget -qO- "http
wget -qO- -U '' 'google.com/search?q=weather' | grep -oP '(-)?\d{1,3}\xB0[FC]'
wget -qO- -U Mozilla http
wget -qO- http
wget -qO- https
wget -qO- icanhazip.com
wget -qO- ifconfig.me/ip
wget -qO- whatismyip.org
wget -qO- www.commandlinefu.com/commands/by/PhillipNordwall | awk -F\> '/num-votes/{S+=$2; I++}END{print S/I}'
wget -qT1 -i-
wget -qqO- http
wget -r --no-parent http
wget -r --wait=5 --quota=5000m --tries=3 --directory-prefix=/home/erin/Documents/erins_webpages  --limit-rate=20k  --level=1 -k -p -erobots=off -np -N  --exclude-domains=del.icio.us,doubleclick.net -F -i ./delicious-20090629.htm
wget -r -A .pdf -l 5 -nH --no-parent http
wget -r -l1 --no-parent -nH -nd -P/tmp -A".gif,.jpg" http
wget -r -l1 -H -t1 -nd -N -np -A.mp3 -erobots=off -i ~/sourceurls.txt
wget -r -l1 -np -nd http
wget -r -np -nd -A.pdf --user *** --password *** http
wget -r ftp
wget -rc -A.flac --tries=5 http
wget -rq --spider --force-html "http
wget -t 3 -q -O - "$@" https
wget -t inf -k -r -l 3 -p -m http
wget <URL> -O- | wget -i -
wget <url>
wget URL$UTILVSERVER;
wget \
wget `lynx --dump http
wget `lynx -dump http
wget `sed -e 's/^-o /-O /g' -e 's/ -o / -O /g' -e 's/^--output /--output-file /g' -e 's/ --output / --output-file /g' <<< "$@"`; )
wget `youtube-dl -g 'http
wget as a spider
wget download
wget execstack libelfg0 dh-modaliases
wget exit
wget http
wget https
wget id info a2ps ls recode
wget ifconfig.me/ip -q -O -
wget multiple files (or mirror) from NTLM-protected Sharepoint
wget not installed"
wget python
wget randomfunfacts.com -O - 2>/dev/null | grep \<strong\> | sed "s;^.*<i>\(.*\)</i>.*$;\1;"
wget randomfunfacts.com -O - 2>/dev/null | grep \<strong\> | sed "s;^.*<i>\(.*\)</i>.*$;\1;" | while read FUNFACT; do notify-send -t $((1000+300*`echo -n $FUNFACT | wc -w`)) -i gtk-dialog-info "RandomFunFact" "$FUNFACT"; done
wget randomfunfacts.com -O - 2>/dev/null|grep \<strong\>|sed "s;^.*<i>\(.*\)</i>.*$;\1;"|cowsay -f tux
wget tazjel.com 
wget to check if a remote file exists
wget to download one page and all it's requisites for offline viewing
wget url' | at 01
wget url' | at 12
wget with resume
wget www.google.com 
wget www.google.com;cat index.html | sed 's/<script>/\n\n\<script>\n\n/g' | sed 's/<\/script>/>\n\n/g'
wget www.tazjel.com 
wget || sudo shutdown -P now; do sleep 1m; done
wget }
wget أو تقوم بعمل compile لبرنامج ما ثم تريد أغلقت النافذة بطريق الخطأ لا مشكلة يمكنك استعادة الجلسة دون انقطاع، أو حتى يمكنك إعادة تشغيل X دون أن تفقد ما كنت تقوم به
wget!
wget' .
wget' . >> vim -
wget' . >> vim =
wget' . | vim -
wget)
wget) from a list of URLs using a stored cookie
wget) is done
wget, tar xzvf, cd, ls
wget.
wget.log' -o -name 'foobar*' -o -name '*~' -o -name '.netrwhist'  \) -delete'
wget.log</span><span class="Constant">'</span> -o -name <span class="Constant">'</span><span class="Constant">foobar*</span><span class="Constant">'</span> -o -name <span class="Constant">'</span><span class="Constant">*~</span><span class="Constant">'</span> -o -name <span class="Constant">'</span><span class="Constant">.netrwhist</span><span class="Constant">'</span>  <span class="Special">\)</span> -delete<span class="Constant">'</span>
wget/wget-1.12.tar.gz
wget</code> - non-interactive download.</li>
wget='wget --content-disposition'
wget='wget --content-disposition' +alias whead='curl --head $1' +alias which-command=whence +alias wotgobblemem='ps -o time,ppid,pid,nice,pcpu,pmem,user,comm -A | sort -n -k 6 | tail -15' +alias wsulast='sudo $(history -p !-1)' +alias xclip='xclip -selection c' +alias xs='cd `pwd -P`' + +############## testing functions ######### +# +alias 'n1'='dpkg --get-selections | grep linux-image' +alias 'kla'=&quot;ls -a | grep '^\.'&quot; +alias 'klj'='ls -a | grep &quot;^\.\(.*\)n$&quot;' + + + +alias 1='cd -' +alias 2='cd +2' +alias 2d='cd ../../' +alias 3='cd +3' +alias 3d='cd ../../../' +alias 4='cd +4' +alias 5='cd +5' +alias 6='cd +6' +alias 7='cd +7' +alias 8='cd +8' +alias 9='cd +9' +alias ANYNAMEHERE=' ssh YOURWEBSITE.com -l USERNAME -p PORTNUMBERHERE' +#alias W=cat +alias a2-restart='sudo service apache2 restart' +alias a2r='/etc/init.d/apache2 restart' +alias aa='git add -A .' +alias acs='apt-cache search' +alias acsh='apt-cache show' +alias active='grep -v -e &quot;^$&quot; -e&quot;^ *#&quot;' +alias agdu='sudo apt-get dist-upgrade' +alias aliasupdt='wget -q -O - &quot;$@&quot; https
wget`" ]; do sleep 2 ;done; [ -e "/tmp/nosleep"] || echo  mem >/sys/power/state
wgetall () { wget -r -l2 -nd -Nc -A.$@ $@ }
wgetlist
wgetrc <<_AAWGETRC
wget|$'
