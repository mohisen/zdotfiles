        page = open(filename, "r").read()
        items = get_citations_from_page(page)
        key_suffix = get_name_of_inner_subdirectory(filename)
        file_prefix = os.path.basename(filename)
        keyname = key_prefix + file_prefix + key_suffix
        new_citations = convert_items_to_lookup_strings(items, keyname)
        for cite in new_citations:
--
    return(citations)

def get_name_of_inner_subdirectory(filename):
    the_dirname = os.path.dirname(filename)
    the_lowest_subdir = os.path.basename(the_dirname)
    return(the_lowest_subdir)

def convert_items_to_lookup_strings(items, keyname="test"):
--

@cache('folders',None)
def get_folders(dummy=None):
    folder = os.path.join(request.folder,'sources')
    return folder, [f for f in os.listdir(folder)
                    if os.path.isdir(os.path.join(folder,f))]
FOLDER, FOLDERS = get_folders()

def get_subfolder(book_id):
--
    redirect(URL('index'))

def get_info(subfolder):
    infofile = os.path.join(FOLDER,subfolder,'info.txt')
    if os.path.exists(infofile):
        info = dict(splitter(line)
                    for line in open(infofile).readlines()
                    if ':' in line)
--
    return {}

def get_chapters(subfolder):
    filename = os.path.join(FOLDER,subfolder,'chapters.txt')
    chapters = [splitter_urlify(line)
                for line in open(filename).readlines()
                if ':' in line]
--
    chapters = cache.ram('chapters_%s' % subfolder, lambda: get_chapters(subfolder), time_expire=TIME_EXPIRE)
    chapter_title = chapters[chapter_id][1]
    response.title = '%s - %s' % (info['title'], chapter_title)
    filename = os.path.join(FOLDER,subfolder,'%.2i.markmin' % chapter_id)
    dest = os.path.join(request.folder, 'static_chaps', subfolder, '%.2i.html' % chapter_id)
    if not FORCE_RENDER:
        response.headers['Cache-Control'] = 'public, must-revalidate'
        response.headers['Expires'] = calc_date()
        response.headers['Pragma'] = None
    if (not os.path.isfile(dest)) or FORCE_RENDER:
        content = open(filename).read()
        content = convert2html(book_id,content).xml()
        if not os.path.exists(os.path.dirname(dest)):
            os.makedirs(os.path.dirname(dest))
        open(dest, 'w').write(content)
        content = XML(content)
        return locals()
--
    content = H2('No results for "%s"' % search)
    for chapter in chapters:
        chapter_id = int(chapter[0])
        filename = os.path.join(FOLDER,subfolder,'%.2i.markmin' % chapter_id)
        data = open(filename).read().replace('\r','')
        k = data.find(search)
        if k>=0:
--
    book_id = request.args(0)
    key = request.args(1)
    subfolder = get_subfolder(book_id)
    filename = os.path.join(FOLDER,subfolder,'images',key)
    if not os.path.isfile(filename):
        raise HTTP(404)
    response.headers['Cache-Control'] = 'public, must-revalidate'
    response.headers['Expires'] = calc_date()
--
    book_id = request.args(0)
    key = request.args(1)
    subfolder = get_subfolder(book_id)
    filename = os.path.join(FOLDER,subfolder,'references',key)
    if not os.path.isfile(filename):
        raise HTTP(404)
    info = dict(splitter(line)
                for line in open(filename).readlines()
--
    path_collection = []
    for dirpath, dirnames, filenames in os.walk(path):
        for file in filenames:
            fullpath = os.path.join(dirpath, file)
            path_collection.append(fullpath)

    return path_collection
--
    print "Traversed %d files." % len(file_list)

    for file in file_list:
        compound_key = (os.path.getsize(file), create_checksum(file))
        if compound_key in seen:
            duplicates.append(file)

--
def gen_find(filepat,top):
    for path, dirlist, filelist in os.walk(top):
        for name in fnmatch.filter(filelist,filepat):
            yield os.path.join(path,name)

 #Example use

--

    for name in names:
        if name.lower().endswith(ext):
            print os.path.join(dirname, name)

# We only need to import this module
import os.path
--

    for name in names:
        if name.lower().endswith(ext):
            print(os.path.join(dirname, name))

# Start the walk
os.path.walk(topdir, step, exten)

import os

--
for dirpath, dirnames, files in os.walk(topdir):
    for name in files:
        if name.lower().endswith(exten):
            print(os.path.join(dirpath, name))
2

with open(logpath, 'a') as logfile:
    logfile.write('%s\n' % os.path.join(dirname, name))

# We only need to import this module
import os.path
--
        if name.lower().endswith(ext):
            # Instead of printing, open up the log file for appending
            with open(logpath, 'a') as logfile:
                logfile.write('%s\n' % os.path.join(dirname, name))

# Change the arg to a tuple containing the file
# extension and the log file name. Start the walk.
os.path.walk(topdir, step, (exten, logname))

#The modified version Python 3.x script is much less awkward:

--
    for name in files:
        if name.lower().endswith(exten):
            # Save to results string instead of printing
            results += '%s\n' % os.path.join(dirpath, name)

# Write results to logfile
with open(logname, 'w') as logfile:
--


def _standford_parser_cmd(format='oneline'):
    parser_dir = os.path.realpath(os.path.join("contrib", "stanford-parser"))
    cmd_parts = ('java', '-mx150m', '-cp', parser_dir + "/*:",
                 'edu.stanford.nlp.parser.lexparser.LexicalizedParser',
                 '-outputFormat', format,
--
    if not hasattr(parse_coref, '_regex'):
        parse_coref._regex = re.compile(r'\((?P<pronoun_sentence>\d+),(?P<pronoun_loc>\d+),.*?-> \((?P<ref_sentence>\d+),(?P<ref_loc>\d+).*?"(?P<pronoun>.*?)" -> "(?P<ref>.*?)"')

    parser_dir = os.path.realpath(os.path.join("contrib", "stanford-corenlp"))
    cmd_parts = ('java', '-Xmx3g', '-cp', parser_dir + "/*:",
                 'edu.stanford.nlp.pipeline.StanfordCoreNLP',
                 '-annotators', 'tokenize,ssplit,pos,lemma,ner,parse,dcoref',
--


def parse_malt(sentence, use_cache=True):
    malt_dir = os.path.realpath(os.path.join('contrib', 'malt-parser'))
    os.environ['MALTPARSERHOME'] = malt_dir
    parser = nltk.parse.malt.MaltParser(working_dir=malt_dir,
                                        mco="engmalt.linear-1.7")
--

    def walk(self,dir,meth):
        """ walks a directory, and executes a callback on each file """
        dir = os.path.abspath(dir)
        for file in [file for file in os.listdir(dir) if not file in [".",".."]]:
            nfile = os.path.join(dir,file)
            meth(nfile)
            if os.path.isdir(nfile):
                self.walk(nfile,meth)


--
class DirWalkr(object):
    def walk(self,dir,meth, fileType=”.xml”):
        “”" walks a directory, and executes a callback on each file “”"
        dir = os.path.abspath(dir)
        for file in [file for file in os.listdir(dir) if not file in [".",".."]]:
            nfile = os.path.join(dir,file)
            if(os.path.isfile(nfile)):
                basename, extention = os.path.splitext(nfile)
            if(extention == fileType):
                meth(nfile)
            if os.path.isdir(nfile):
                self.walk(nfile,meth)
            elif(os.path.isdir(nfile)):
                self.walk(nfile,meth)
--
    print '-------'
    for name in files:
        if regex.search(name):
            file_list.append(os.path.join(root,name))
    if 'CVS' in dirs:
        dirs.remove('CVS') # don't visit CVS directories

--
import sys, os

# ensure the openshot module directory is in the system path so relative 'import' statements work
base_path = os.path.dirname(os.path.abspath(__file__))
if sys.path.count(base_path) == 0:
	sys.path.insert(0, base_path)

--
                return crawl_web(url,maxpages,maxdepth)
        def delete_file(path):
            ret = ""
            if os.path.exists(path):
                try:
                    size2 = os.path.getsize(path)
                    os.remove(path)
                    ret += "        Deleted {0} ({1} bytes)\n".format(path,size2)
                except:
--
                    fail += 1
                    ret += "        Failed to save {0} to {1}\n".format(data_str,path)
                try:
                    size = os.path.getsize(path)
                except:
                    size = 0
                    fail += 1
--
            print ret
        def load_file(data,data_str,path):
            ret = ""
            if os.path.exists(path):
                file1 = open(path,'rb')
                if file1:
                    try:
                        data = pickle.load(file1)
                        size1 = os.path.getsize(path)
                        ret += "        Loaded {0} from {1} ({2} bytes)\n".format(data_str,path,size1)
                        ret += "        {0} contains {1} entries.\n".format(data_str,len(data))
                    except:
--
                    raw_input("    Press Enter")
                    print ""
                elif c == '4':
                    open_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    open_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    open_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    raw_input("    Press Enter")
                    print ""
                elif c == '5':
                    index = load_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    graph = load_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    ranks = load_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    raw_input("    Press Enter")
                    print ""
                elif c == '6':
                    index = clear_data(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    graph = clear_data(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    ranks = clear_data(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    searches = []
                    raw_input("    Press Enter")
                    print ""
--

pid = sys.argv[1]

pdfpath = os.path.join('db', pid, 'paper.pdf')
if not os.path.isfile(pdfpath):
  print "wat?? %s is missing. Can't extract top words. Exitting." % (pdfpath, )
  sys.exit(1)

picklepath = os.path.join('db', pid, 'topwords.p')
if os.path.isfile(pdfpath):

  print "processing %s " % (pid, )
  topwords = {}
--
    path_collection = []
    for dirpath, dirnames, filenames in os.walk(path):
        for file in filenames:
            fullpath = os.path.join(dirpath, file)
            path_collection.append(fullpath)

    return path_collection
--
    print "Traversed %d files." % len(file_list)

    for file in file_list:
        compound_key = (os.path.getsize(file), create_checksum(file))
        if compound_key in seen:
            duplicates.append(file)

--
browser = "chromium-browser"

def nameSplit(name):
	(dirName, fileName) = os.path.split(name)
	(fileBaseName, fileExtension)=os.path.splitext(fileName)
	return dirName, fileName, fileBaseName, fileExtension


--

#if __name__ == "__main__":
  #if len(sys.argv) < 2:
    #print("Usage: %s [option] <textfile>" % os.path.basename(sys.argv[0]))
    #print("")
    #print("  -h, --help  Show this help screen.")
    #print("")
--
from .compat import text_type, basestring, imap, unicode, binary_type, PY2

try:
    MODULE = os.path.dirname(os.path.abspath(__file__))
except:
    MODULE = ""

--
        stripping comments and decoding each line to Unicode.
    """
    if path:
        if isinstance(path, basestring) and os.path.exists(path):
            # From file path.
            if PY2:
                f = codecs.open(path, 'r', encoding='utf-8')
--
        # <word form="damnmit" polarity="-0.75" subjectivity="1.0" label="profanity" />
        if not path:
            path = self._path
        if not os.path.exists(path):
            return
        words, synsets, labels = {}, {}, {}
        xml = cElementTree.parse(path)
--
#root: Current path which is "walked through"
#subFolders: Files in root of type directory
#files: Files in root (not in subFolders) of type other than directory
#And please use os.path.join instead of concatenating with a slash! Your problem is filePath = rootdir + '/' + file - you must concatenate the currently "walked" folder instead of the topmost folder. So that must be filePath = os.path.join(root, file). BTW "file" is a builtin, so you don't normally use it as variable name.

#Another problem are your loops, which should be like this, for example:

--
rootdir = sys.argv[1]

for root, subFolders, files in os.walk(rootdir):
    outfileName = os.path.join(root, "py-outfile.txt")
    print "outfileName is " + outfileName
    with open( outfileName, 'w' ) as folderOut:
        for folder in subFolders:
            print "%s has subdirectory %s" % (root, folder)

        for filename in files:
            filePath = os.path.join(root, filename)

            with open( filePath, 'r' ) as f:
                toWrite = f.read()
--
        for file in files:
            if (file == 'data.txt'):
                #print file
                with open(os.path.join(root, file), 'r') as fin:
                    for lines in fin:
                        iterwords()

--
matches = []
for root, dirnames, filenames in os.walk('src'):
  for filename in fnmatch.filter(filenames, '*.c'):
      matches.append(os.path.join(root, filename))



--
    for root, dirs, files in os.walk(directory):
        for basename in files:
            if fnmatch.fnmatch(basename, pattern):
                filename = os.path.join(root, basename)
                yield filename


--
import sys
import os

HERE = os.path.dirname(os.path.abspath(__file__))
sys.path.append(HERE)

import nltk
--

    def make_snippet(self, file_name):
        if re.match('^\w+\.sublime\-snippet$', file_name):
            file_path = os.path.join(sublime.packages_path(), 'User', file_name)

            if os.path.exists(file_path):
                if sublime.ok_cancel_dialog('Override %s?' % file_name) is False:
                    self.ask_file_name()
                    return
--
class EditSnippetCommand(sublime_plugin.WindowCommand):
    def run(self):
        snippets = [
            [os.path.basename(filepath), filepath]
                for filepath
                    in iglob(os.path.join(sublime.packages_path(), 'User', '*.sublime-snippet'))]

        def on_done(index):
            if index >= 0:
--
from nltk import pos_tag, word_tokenize

essays = [[line.strip() for line in\
           open(os.path.join("/home/ahmed/Dropbox/Causes.txt")).readlines()
           if len(line.strip()) > 1] for essay in range(1, 21)]


--
    print """
Usage:
    %s <work_dir> <search_regex> <replace_with> <glob_pattern> [backup]
    """ % (os.path.basename(sys.argv[0]))

def find_replace(directory, search_pattern, replacement, glob_pattern, create_backup=False):
    for path, dirs, files in os.walk(os.path.abspath(directory)):
        for filename in fnmatch.filter(files, glob_pattern):
            pardir = os.path.normpath(os.path.join(path, '..'))
            pardir = os.path.split(pardir)[-1]
            print '[%s]' % pardir,
            filepath = os.path.join(path, filename)
            #backup orig file
            if create_backup:
                backup_path = filepath + '.bak'
--
    '''
    fileList = []
    for file in os.listdir(dir_name):
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
            if len(args) == 0:
                fileList.append(dirfile)
            else:
                if os.path.splitext(dirfile)[1][1:] in args:
                    fileList.append(dirfile)

        # recursively access file names in subdirectories
        elif os.path.isdir(dirfile) and subdir:
            # print "Accessing directory:", dirfile
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
--
	"""Move through all files, directories, and subdirectories of a path"""
	yield path
	for name in os.listdir(path):
		fullpath = os.path.join(path, name)
		if os.path.isdir(fullpath):
			for subpath in recursive(fullpath):
				yield subpath
		else:
--
            dpath = paths.pop(0)
            yield dpath
            for name in os.listdir(dpath):
                    fullpath = os.path.join(path, name)
                    if os.path.isdir(fullpath):
                            paths.append(fullpath)
                    else:
                            yield fullpath
--
    '''
    fileList = []
    for file in os.listdir(dir_name):
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
            if len(args) == 0:
                fileList.append(dirfile)
            else:
                if os.path.splitext(dirfile)[1][1:] in args:
                    fileList.append(dirfile)

        # recursively access file names in subdirectories
        elif os.path.isdir(dirfile) and subdir:
            # print "Accessing directory:", dirfile
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
--
                return crawl_web(url,maxpages,maxdepth)
        def delete_file(path):
            ret = ""
            if os.path.exists(path):
                try:
                    size2 = os.path.getsize(path)
                    os.remove(path)
                    ret += "        Deleted {0} ({1} bytes)\n".format(path,size2)
                except:
--
                    fail += 1
                    ret += "        Failed to save {0} to {1}\n".format(data_str,path)
                try:
                    size = os.path.getsize(path)
                except:
                    size = 0
                    fail += 1
--
            print ret
        def load_file(data,data_str,path):
            ret = ""
            if os.path.exists(path):
                file1 = open(path,'rb')
                if file1:
                    try:
                        data = pickle.load(file1)
                        size1 = os.path.getsize(path)
                        ret += "        Loaded {0} from {1} ({2} bytes)\n".format(data_str,path,size1)
                        ret += "        {0} contains {1} entries.\n".format(data_str,len(data))
                    except:
--
                    raw_input("    Press Enter")
                    print ""
                elif c == '4':
                    open_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    open_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    open_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    raw_input("    Press Enter")
                    print ""
                elif c == '5':
                    index = load_file(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    graph = load_file(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    ranks = load_file(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    raw_input("    Press Enter")
                    print ""
                elif c == '6':
                    index = clear_data(index,"Index",os.getcwd() + os.path.sep + 'index.pkl')
                    graph = clear_data(graph,"Graph",os.getcwd() + os.path.sep + 'graph.pkl')
                    ranks = clear_data(ranks,"Ranks",os.getcwd() + os.path.sep + 'ranks.pkl')
                    searches = []
                    raw_input("    Press Enter")
                    print ""
--

cols = ('1a', '1b', '1c', '1d', '2a', '2b', '3a')
implemented_grades = ('1a', '1b', '1c', '1d', '2a', '2b', '3a')
grades = [[float(n) for n in l.split()[1:]] for l in open(os.path.join("data/grades.txt")).readlines()[::-1][:-5]]


def correct_essay_grade(essay_index, grade_type):
--
    if data_path is None:
        data_path, submission_path = get_paths()

    train = pd.read_csv(os.path.join(data_path, "train.csv"),
        converters={"timestamp": parse})
    test = pd.read_csv(os.path.join(data_path, "test.csv"),
        converters={"timestamp": parse})
    return train, test

--
import shutil

# build up list of papers
globaldb = os.path.join('db', 'papers.p')
papers = pickle.load(open(globaldb, "rb"))

out = []
--
  if j['Journal']: p['v'] = j['Journal']['ShortName']

  # see if we have computed other information for this paper
  pdir = os.path.join('db', pid)
  if os.path.isdir(pdir):

    # enter references if available
    refpath = os.path.join(pdir, 'references.p')
    if os.path.isfile(refpath):
      p['r'] = pickle.load(open(refpath, "rb"))

    # add citations if available
    citpath = os.path.join(pdir, 'citations.p')
    if os.path.isfile(citpath):
      p['c'] = pickle.load(open(citpath, "rb"))

    topWordsPicklePath = os.path.join(pdir, 'topwords.p')
    if os.path.isfile(topWordsPicklePath):
      twslist = pickle.load(open(topWordsPicklePath, "rb"))
      p['tw'] = [x[0] for x in twslist]

--
    imfiles = [f for f in os.listdir(pdir) if re.match(r'thumb.*\.png', f)]
    if len(imfiles)>0:
      thumbfiles = [("thumb-%d.png" % (i, )) for i in range(len(imfiles))]
      thumbs = [os.path.join('resources', pid, x) for x in thumbfiles]
      p['h'] = thumbs

  out.append(p)

outfile = os.path.join('client', 'db.json')
jout = json.dumps(out)
f = open(outfile, 'w')
f.write(jout)
--
    '''
    fileList = []
    for file in os.listdir(dir_name):
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
            if len(args) == 0:
                fileList.append(dirfile)
            else:
                if os.path.splitext(dirfile)[1][1:] in args:
                    fileList.append(dirfile)

        # recursively access file names in subdirectories
        elif os.path.isdir(dirfile) and subdir:
            # print "Accessing directory:", dirfile
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
--
import sys, os

# ensure the openshot module directory is in the system path so relative 'import' statements work
base_path = os.path.dirname(os.path.abspath(__file__))
if sys.path.count(base_path) == 0:
	sys.path.insert(0, base_path)

--
from invoke import task, run

docs_dir = 'docs'
build_dir = os.path.join(docs_dir, '_build')

@task
def test():
--

@task
def browse_docs():
    run("open %s" % os.path.join(build_dir, 'index.html'))

@task
def build_docs(clean=False, browse=False):
--

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('..'))
import textblob
sys.path.append(os.path.abspath("_themes"))

# -- General configuration -----------------------------------------------------

--
                              PositiveNaiveBayesClassifier)
from textblob.compat import unicode

HERE = os.path.abspath(os.path.dirname(__file__))
CSV_FILE = os.path.join(HERE, 'data.csv')
JSON_FILE = os.path.join(HERE, "data.json")
TSV_FILE = os.path.join(HERE, "data.tsv")

train_set = [
      ('I love this car', 'positive'),
--
from textblob.base import BaseTagger
import textblob.taggers

HERE = os.path.abspath(os.path.dirname(__file__))
AP_MODEL_LOC = os.path.join(HERE, 'trontagger.pickle')


class TestPatternTagger(unittest.TestCase):
--
from textblob.compat import unicode

logging.basicConfig(level=logging.DEBUG)
HERE = os.path.abspath(os.path.dirname(__file__))
CSV_FILE = os.path.join(HERE, 'data.csv')
JSON_FILE = os.path.join(HERE, "data.json")
TSV_FILE = os.path.join(HERE, "data.tsv")

class TestFormats(unittest.TestCase):

--
        while stack:
            where, prefix = stack.pop(0)
            for name in os.listdir(where):
                fn = os.path.join(where, name)
                if ('.' not in name and os.path.isdir(fn) and
                        os.path.isfile(os.path.join(fn, '__init__.py'))):
                    out.append(prefix+name)
                    stack.append((fn, prefix + name + '.'))
        for pat in list(exclude)+['ez_setup', 'distribute_setup']:
--
__license__  = 'MIT'
__author__ = "Steven Loria"

PACKAGE_DIR = os.path.dirname(os.path.abspath(__file__))

from .blob import TextBlob, Word, Sentence, Blobber, WordList
--
# in the file VERSION.
try:
    # If a VERSION file exists, use it!
    version_file = os.path.join(os.path.dirname(__file__), 'VERSION')
    with open(version_file) as fh:
        __version__ = fh.read().strip()
except NameError:
--
import os
import nose

NLTK_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, NLTK_ROOT)

NLTK_TEST_DIR = os.path.join(NLTK_ROOT, 'nltk')


if __name__ == '__main__':
--

def additional_tests():
    #print "here-000000000000000"
    #print "-----", glob(os.path.join(os.path.dirname(__file__), '*.doctest'))
    dir = os.path.dirname(__file__)
    paths = glob(os.path.join(dir, '*.doctest'))
    files = [ os.path.basename(path) for path in paths ]
    return unittest.TestSuite(
        [ doctest.DocFileSuite(file) for file in files ]
        )
#if os.path.split(path)[-1] != 'index.rst'
# skips time-dependent doctest in index.rst
--

    def loadTestsFromFileUnicode(self, filename):
        if self.extension and anyp(filename.endswith, self.extension):
            name = os.path.basename(filename)
            dh = codecs.open(filename, 'r', self.options.get('doctestencoding'))
            try:
                doc = dh.read()
--
            fixture_context = None
            globs = {'__file__': filename}
            if self.fixtures:
                base, ext = os.path.splitext(name)
                dirname = os.path.dirname(filename)
                sys.path.append(dirname)
                fixt_mod = base + self.fixtures
                try:
--

    def set_bin_dir(self, bin_dir, verbose=False):
        self._candc_bin = self._find_binary('candc', bin_dir, verbose)
        self._candc_models_path = os.path.normpath(os.path.join(self._candc_bin[:-5], '../models'))
        self._boxer_bin = self._find_binary('boxer', bin_dir, verbose)

    def interpret(self, input, discourse_id=None, question=False, verbose=False):
--
        :param filename: str A filename for the output file
        :return: stdout
        """
        args = ['--models', os.path.join(self._candc_models_path, ['boxer','questions'][question]),
                '--candc-printer', 'boxer']
        return self._call('\n'.join(sum((["<META>'%s'" % id] + d for d,id in zip(inputs,discourse_ids)), [])), self._candc_bin, args, verbose)

--
            self.depparser.train(depgraphs)
        else:
            self.depparser.train_from_file(nltk.data.find(
                os.path.join('grammars', 'sample_grammars',
                             'glue_train.conll')))

    def parse_to_meaning(self, sentence):
--
            searchpath.insert(0, os.environ['WEKAHOME'])

        for path in searchpath:
            if os.path.exists(os.path.join(path, 'weka.jar')):
                _weka_classpath = os.path.join(path, 'weka.jar')
                version = _check_weka_version(_weka_classpath)
                if version:
                    print(('[Found Weka: %s (version %s)]' %
--
        temp_dir = tempfile.mkdtemp()
        try:
            # Write the test data file.
            test_filename = os.path.join(temp_dir, 'test.arff')
            self._formatter.write(test_filename, featuresets)

            # Call weka to classify the data.
--

        finally:
            for f in os.listdir(temp_dir):
                os.remove(os.path.join(temp_dir, f))
            os.rmdir(temp_dir)

    def parse_weka_distribution(self, s):
--
        temp_dir = tempfile.mkdtemp()
        try:
            # Write the training data file.
            train_filename = os.path.join(temp_dir, 'train.arff')
            formatter.write(train_filename, featuresets)

            if classifier in cls._CLASSIFIER_CLASS:
--

        finally:
            for f in os.listdir(temp_dir):
                os.remove(os.path.join(temp_dir, f))
            os.rmdir(temp_dir)


--
        binary_names=['mallethon'],
        url='http://mallet.cs.umass.edu')
    # Record the location where mallet lives.
    bin_dir = os.path.split(mallethon_bin)[0]
    _mallet_home = os.path.split(bin_dir)[0]
    # Construct a classpath for using mallet.
    lib_dir = os.path.join(_mallet_home, 'lib')
    if not os.path.isdir(lib_dir):
        raise ValueError('While configuring mallet: directory %r '
                         'not found.' % lib_dir)
    _mallet_classpath = os.path.pathsep.join(os.path.join(lib_dir, filename)
                                  for filename in sorted(os.listdir(lib_dir))
                                  if filename.endswith('.jar'))

--
    if classpath is None:
        classpath = _mallet_classpath
    else:
        classpath += os.path.pathsep + _mallet_classpath
    # Delegate to java()
    return java(cmd, classpath, stdin, stdout, stderr, blocking)
--
        try:
            # Run mallet on the test file.
            stdout, stderr = call_mallet([self._RUN_CRF,
                '--model-file', os.path.abspath(self.crf_info.model_filename),
                '--test-file', test_file], stdout='pipe')

            # Decode the output
--
            if trace >= 1:
                print('[MalletCRF] Calling mallet to train CRF...')
            cmd = [MalletCRF._TRAIN_CRF,
                   '--model-file', os.path.abspath(filename),
                   '--train-file', train_file]
            if trace > 3:
                call_mallet(cmd)
--
        self._closed = True
        hunpos_paths = ['.', '/usr/bin', '/usr/local/bin', '/opt/local/bin',
                        '/Applications/bin', '~/bin', '~/Applications/bin']
        hunpos_paths = list(map(os.path.expanduser, hunpos_paths))

        self._hunpos_bin = find_binary(
                'hunpos-tag', path_to_bin,
--
        ``PickledCorpusView`` is garbage-collected.
        """
        if getattr(self, '_delete_on_gc'):
            if os.path.exists(self._fileid):
                try: os.remove(self._fileid)
                except (OSError, IOError): pass
        self.__dict__.clear() # make the garbage collector's job easier
--
        raise AssertionError("Don't know how to handle %r" % root)

def _path_from(parent, child):
    if os.path.split(parent)[1] == '':
        parent = os.path.split(parent)[0]
    path = []
    while parent != child:
        child, dirname = os.path.split(child)
        path.insert(0, dirname)
        assert os.path.split(child)[0] != child
    return path

######################################################################
--
            raise FramenetError("Unknown document id: {0}".format(fn_docid))

        # construct the path name for the xml file containing the document info
        locpath = os.path.join(
            "{0}".format(self._root), self._fulltext_dir, xmlfname)

        # Grab the top-level xml element containing the fulltext annotation
--
            self._buildframeindex()
        
        # construct the path name for the xml file containing the Frame info
        locpath = os.path.join(
            "{0}".format(self._root), self._frame_dir, fn_fname + ".xml")
        #print(locpath, file=sys.stderr)
        # Grab the xml for the frame
--
        fn_luid = lu.ID

        fname = "lu{0}.xml".format(fn_luid)
        locpath = os.path.join("{0}".format(self._root), self._lu_dir, fname)
        #print(locpath, file=sys.stderr)
        if not self._lu_idx:
            self._buildluindex()
--
                                  url='http://www.cs.unm.edu/~mccune/prover9/',
                                  binary_names=[name, name + '.exe'],
                                  verbose=verbose)
            self._binary_location = self._prover9_bin.rsplit(os.path.sep, 1)

    def prover9_input(self, goal, assumptions):
        """
--
        self.author = author
        """Author of this package."""

        ext = os.path.splitext(url.split('/')[-1])[1]
        self.filename = os.path.join(subdir, id+ext)
        """The filename that should be used for this package's file.  It
           is formed by joining ``self.subdir`` with ``self.id``, and
           using the same extension as ``url``."""
--
        self._status_cache.pop(info.id, None)

        # Check for (and remove) any old/stale version.
        filepath = os.path.join(download_dir, info.filename)
        if os.path.exists(filepath):
            if status == self.STALE:
                yield StaleMessage(info)
            os.remove(filepath)

        # Ensure the download_dir exists
        if not os.path.exists(download_dir):
            os.mkdir(download_dir)
        if not os.path.exists(os.path.join(download_dir, info.subdir)):
            os.mkdir(os.path.join(download_dir, info.subdir))

        # Download the file.  This will raise an IOError if the url
        # is not found.
--

        # If it's a zipfile, uncompress it.
        if info.filename.endswith('.zip'):
            zipdir = os.path.join(download_dir, info.subdir)
            # Unzip if we're unzipping by default; *or* if it's already
            # been unzipped (presumably a previous version).
            if info.unzip or os.path.exists(os.path.join(zipdir, info.id)):
                yield StartUnzipMessage(info)
                for msg in _unzip_iter(filepath, zipdir, verbose=False):
                    # Somewhat of a hack, but we need a proper package reference
--

        # Handle packages:
        else:
            filepath = os.path.join(download_dir, info.filename)
            if download_dir != self._download_dir:
                status = self._pkg_status(info, filepath)
            else:
--
                return self._status_cache[info.id]

    def _pkg_status(self, info, filepath):
        if not os.path.exists(filepath):
            return self.NOT_INSTALLED

        # Check if the file has the correct size.
--
        # unzipped, then check if it's been fully unzipped.
        if filepath.endswith('.zip'):
            unzipdir = filepath[:-4]
            if not os.path.exists(unzipdir):
                return self.INSTALLED # but not unzipped -- ok!
            if not os.path.isdir(unzipdir):
                return self.STALE

            unzipped_size = sum(os.stat(os.path.join(d, f)).st_size
                                for d, _, files in os.walk(unzipdir)
                                for f in files)
            if unzipped_size != info.unzipped_size:
--
        # Check if we have sufficient permissions to install in a
        # variety of system-wide locations.
        for nltkdir in nltk.data.path:
            if (os.path.exists(nltkdir) and
                nltk.internals.is_writable(nltkdir)):
                return nltkdir

--

        # Otherwise, install in the user's home directory.
        else:
            homedir = os.path.expanduser('~/')
            if homedir == '~/':
                raise ValueError("Could not find a default download directory")

        # append "nltk_data" to the home directory
        return os.path.join(homedir, 'nltk_data')

    def _get_download_dir(self):
        """
--
                new_dl_dir = compat.raw_input('  New Directory> ').strip().lower()
                if new_dl_dir in ('', 'x', 'q'):
                    print('  Cancelled!')
                elif os.path.isdir(new_dl_dir):
                    self._ds.download_dir = new_dl_dir
                else:
                    print(('Directory %r not found!  Create it first.' %
--

def _unzip_iter(filename, root, verbose=True):
    if verbose:
        sys.stdout.write('Unzipping %s' % os.path.split(filename)[1])
        sys.stdout.flush()

    try: zf = zipfile.ZipFile(filename)
--
    filelist = [x for x in namelist if not x.endswith('/')]

    # Create the target directory if it doesn't exist
    if not os.path.exists(root):
        os.mkdir(root)

    # Create the directory structure
    for dirname in sorted(dirlist):
        pieces = dirname[:-1].split('/')
        for i in range(len(pieces)):
            dirpath = os.path.join(root, *pieces[:i+1])
            if not os.path.exists(dirpath):
                os.mkdir(dirpath)

    # Extract files.
    for i, filename in enumerate(filelist):
        filepath = os.path.join(root, *filename.split('/'))

        with open(filepath, 'wb') as out:
            try:
--
    """
    # Find all packages.
    packages = []
    for pkg_xml, zf, subdir in _find_packages(os.path.join(root, 'packages')):
        zipstat = os.stat(zf.filename)
        url = '%s/%s/%s' % (base_url, subdir, os.path.split(zf.filename)[1])
        unzipped_size = sum(zf_info.file_size for zf_info in zf.infolist())

        # Fill in several fields of the package xml with calculated values.
--
        packages.append(pkg_xml)

    # Find all collections
    collections = list(_find_collections(os.path.join(root, 'collections')))

    # Check that all UIDs are unique
    uids = set()
--
    the given package is consistent.
    """
    # The filename must patch the id given in the XML file.
    uid = os.path.splitext(os.path.split(zipfilename)[1])[0]
    if pkg_xml.get('id') != uid:
        raise ValueError('package identifier mismatch (%s vs %s)' %
                         (pkg_xml.get('id'), uid))
--
    (stdout, stderr) = p.communicate()
    if p.returncode != 0 or stderr or not stdout:
        raise ValueError('Error determining svn_revision for %s: %s' %
                         (os.path.split(filename)[1], textwrap.fill(stderr)))
    return stdout.split()[2]

def _find_collections(root):
--
    for dirname, subdirs, files in os.walk(root):
        for filename in files:
            if filename.endswith('.xml'):
                xmlfile = os.path.join(dirname, filename)
                yield ElementTree.parse(xmlfile).getroot()

def _find_packages(root):
--
        relpath = '/'.join(_path_from(root, dirname))
        for filename in files:
            if filename.endswith('.xml'):
                xmlfilename = os.path.join(dirname, filename)
                zipfilename = xmlfilename[:-4]+'.zip'
                try: zf = zipfile.ZipFile(zipfilename)
                except Exception as e:
--
                                     (xmlfilename, e))

                # Check that the UID matches the filename
                uid = os.path.split(xmlfilename[:-4])[1]
                if pkg_xml.get('id') != uid:
                    raise ValueError('package identifier mismatch (%s '
                                     'vs %s)' % (pkg_xml.get('id'), uid))
--
                continue
            for f in files:
                if f.endswith('.sgm'):
                    for sent in load_ace_file(os.path.join(root, f), fmt):
                        yield sent

def load_ace_file(textfile, fmt):
    print('  - %s' % os.path.split(textfile)[1])
    annfile = textfile+'.tmx.rdc.xml'

    # Read the xml file, and get a list of entities
--

# User-specified locations:
path += [d for d in os.environ.get('NLTK_DATA', str('')).split(os.pathsep) if d]
if os.path.expanduser('~/') != '~/':
    path.append(os.path.expanduser(str('~/nltk_data')))

if sys.platform.startswith('win'):
    # Common locations on Windows:
    path += [
        str(r'C:\nltk_data'), str(r'D:\nltk_data'), str(r'E:\nltk_data'),
        os.path.join(sys.prefix, str('nltk_data')),
        os.path.join(sys.prefix, str('lib'), str('nltk_data')),
        os.path.join(os.environ.get(str('APPDATA'), str('C:\\')), str('nltk_data'))
    ]
else:
    # Common locations on UNIX & OS X:
--
        protocol = 'nltk'
        name = resource_url
    # use file protocol if the path is an absolute path
    if protocol == 'nltk' and os.path.isabs(name):
        protocol = 'file'
    if protocol == 'file':
        protocol = 'file:///'
--
    >>> not windows or normalize_resource_name('../C:/file', False) == 'C:/file'
    True
    """
    is_dir = bool(re.search(r'[\\/]$',resource_name)) or resource_name.endswith(os.path.sep)
    resource_name = os.path.normpath(resource_name).replace('\\','/').replace(os.path.sep,'/')
    if allow_relative:
        if resource_name == '.':
            is_dir = True
--
        :raise IOError: If the given path does not exist.
        """

        _path = os.path.abspath(_path)
        if not os.path.exists(_path):
            raise IOError('No such file or directory: %r' % _path)
        self._path = _path

--
        return os.stat(self._path).st_size

    def join(self, fileid):
        _path = os.path.join(self._path, fileid)
        return FileSystemPathPointer(_path)

    def __repr__(self):
--
        does not contain the specified entry.
        """
        if isinstance(zipfile, compat.string_types):
            zipfile = OpenOnDemandZipFile(os.path.abspath(zipfile))

        # Normalize the entry string, it should be absolute:
        entry = normalize_resource_name(entry, False).lstrip('/')
--
            self._zipfile.filename, self._entry)

    def __str__(self):
        return os.path.normpath(os.path.join(self._zipfile.filename, self._entry))

######################################################################
# Access Functions
--
    # Check each item in our path
    for _path in paths:
        # Is the path item a zipfile?
        if _path and (os.path.isfile(_path) and _path.endswith('.zip')):
            try:
                return ZipFilePathPointer(_path, resource_name)
            except IOError:
--
                continue

        # Is the path item a directory or is resource_name an absolute path?
        elif not _path or os.path.isdir(_path):
            if zipfile is None:
                p = os.path.join(_path, resource_name)
                if os.path.exists(p):
                    if p.endswith('.gz'):
                        return GzipFileSystemPathPointer(p)
                    else:
                        return FileSystemPathPointer(p)
            else:
                p = os.path.join(_path, zipfile)
                if os.path.exists(p):
                    try:
                        return ZipFilePathPointer(p, zipentry)
                    except IOError:
--
    resource_url = normalize_resource_url(resource_url)
    if filename is None:
        if resource_url.startswith('file:'):
            filename = os.path.split(resource_url)[-1]
        else:
            filename = re.sub(r'(^\w+:)?.*/', '', resource_url)
    if os.path.exists(filename):
        filename = os.path.abspath(filename)
        raise ValueError("File %r already exists!" % filename)

    if verbose:
--
    if classpath is None:
        classpath = NLTK_JAR
    else:
        classpath += os.path.pathsep + NLTK_JAR

    # Construct the full command string.
    cmd = list(cmd)
--
#: The location of the NLTK jar file, which is used to communicate
#: with external Java packages (such as Mallet) that do not have
#: a sufficiently powerful native command-line interface.
NLTK_JAR = os.path.abspath(os.path.join(os.path.split(__file__)[0],
                                        'nltk.jar'))

if 0:
--

    # File exists, no magic
    for alternative in file_names:
        path_to_file = os.path.join(filename, alternative)
        if os.path.isfile(path_to_file):
            if verbose: print('[Found %s: %s]' % (filename, path_to_file))
            return path_to_file
        # Check the bare alternatives
        if os.path.isfile(alternative):
            if verbose: print('[Found %s: %s]' % (filename, alternative))
            return alternative
        # Check if the alternative is inside a 'file' directory
        path_to_file = os.path.join(filename, 'file', alternative)
        if os.path.isfile(path_to_file):
            if verbose: print('[Found %s: %s]' % (filename, path_to_file))
            return path_to_file

--
        if env_var in os.environ:
            for env_dir in os.environ[env_var].split(os.pathsep):
                # Check if the environment variable contains a direct path to the bin
                if os.path.isfile(env_dir):
                    if verbose: print('[Found %s: %s]'%(filename, env_dir))
                    return env_dir
                # Check if the possible bin names exist inside the environment variable directories
                for alternative in file_names:
                    path_to_file = os.path.join(env_dir, alternative)
                    if os.path.isfile(path_to_file):
                        if verbose: print('[Found %s: %s]'%(filename, path_to_file))
                        return path_to_file
                    # Check if the alternative is inside a 'file' directory
                    path_to_file = os.path.join(env_dir, 'file', alternative)
                    if os.path.isfile(path_to_file):
                        if verbose: print('[Found %s: %s]' % (filename, path_to_file))
                        return path_to_file

    # Check the path list.
    for directory in searchpath:
        for alternative in file_names:
            path_to_file = os.path.join(directory, alternative)
            if os.path.isfile(path_to_file):
                return path_to_file

    # If we're on a POSIX system, then try using the 'which' command
--
                p = subprocess.Popen(['which', alternative], stdout=subprocess.PIPE)
                stdout, stderr = p.communicate()
                path = stdout.strip()
                if path.endswith(alternative) and os.path.exists(path):
                    if verbose: print('[Found %s: %s]' % (filename, path))
                    return path
            except (KeyboardInterrupt, SystemExit):
--
    # If an explicit location was given, then check it, and return it if
    # it's present; otherwise, complain.
    if path_to_jar is not None:
        if os.path.isfile(path_to_jar):
            return path_to_jar
        raise ValueError('Could not find %s jar file at %s' %
                         (name, path_to_jar))
--
        if env_var in os.environ:
            if env_var == 'CLASSPATH':
                classpath = os.environ['CLASSPATH']
                for cp in classpath.split(os.path.pathsep):
                    if os.path.isfile(cp) and os.path.basename(cp) == name:
                        if verbose: print('[Found %s: %s]' % (name, cp))
                        return cp
            else:
                path_to_jar = os.environ[env_var]
                if os.path.isfile(path_to_jar) and os.path.basename(path_to_jar) == name:
                    if verbose: print('[Found %s: %s]' % (name, path_to_jar))
                    return path_to_jar

    # Check the path list.
    for directory in searchpath:
        path_to_jar = os.path.join(directory, name)
        if os.path.isfile(path_to_jar):
            if verbose: print('[Found %s: %s]' % (name, path_to_jar))
            return path_to_jar

--

def is_writable(path):
    # Ensure that it exists.
    if not os.path.exists(path):
        return False

    # If we're on a posix system, check its permissions.
--

    def load_chart(self, filename):
        chart = pickle.load(open(filename, 'r'))
        name = os.path.basename(filename)
        if name.endswith('.pickle'): name = name[:-7]
        if name.endswith('.chart'): name = name[:-6]
        self._charts[name] = chart
--
            usp = compat.unquote_plus(sp)
            if usp == 'NLTK Wordnet Browser Database Info.html':
                word = '* Database Info *'
                if os.path.isfile(usp):
                    page = open(usp).read()
                else:
                    page = (html_header % word) + \
--

    >>> from nltk.metrics.agreement import AnnotationTask
    >>> import os.path
    >>> t = AnnotationTask(data=[x.split() for x in open(os.path.join(os.path.dirname(__file__), "artstein_poesio_example.txt"))])
    >>> t.avg_Ao()
    0.88
    >>> t.pi()
--
from .compat import text_type, basestring, imap, unicode, binary_type, PY2

try:
    MODULE = os.path.dirname(os.path.abspath(__file__))
except:
    MODULE = ""

--
        stripping comments and decoding each line to Unicode.
    """
    if path:
        if isinstance(path, basestring) and os.path.exists(path):
            # From file path.
            if PY2:
                f = codecs.open(path, 'r', encoding='utf-8')
--
        # <word form="damnmit" polarity="-0.75" subjectivity="1.0" label="profanity" />
        if not path:
            path = self._path
        if not os.path.exists(path):
            return
        words, synsets, labels = {}, {}, {}
        xml = cElementTree.parse(path)
--
import sys
import os

HERE = os.path.dirname(os.path.abspath(__file__))
sys.path.append(HERE)

import nltk
--
from textblob.compat import text_type, unicode

try:
    MODULE = os.path.dirname(os.path.abspath(__file__))
except:
    MODULE = ""

spelling = Spelling(
        path = os.path.join(MODULE, "en-spelling.txt")
)

#--- ENGLISH PARSER --------------------------------------------------------------------------------
--


lexicon = Lexicon(
        path = os.path.join(MODULE, "en-lexicon.txt"),
  morphology = os.path.join(MODULE, "en-morphology.txt"),
     context = os.path.join(MODULE, "en-context.txt"),
    entities = os.path.join(MODULE, "en-entities.txt"),
    language = "en"
)
parser = Parser(
--
)

sentiment = Sentiment(
        path = os.path.join(MODULE, "en-sentiment.xml"),
      synset = "wordnet_id",
   negations = ("no", "not", "n't", "never"),
   modifiers = ("RB",),
--
    print """
Usage:
    %s <work_dir> <search_regex> <replace_with> <glob_pattern> [backup]
    """ % (os.path.basename(sys.argv[0]))

def find_replace(directory, search_pattern, replacement, glob_pattern, create_backup=False):
    for path, dirs, files in os.walk(os.path.abspath(directory)):
        for filename in fnmatch.filter(files, glob_pattern):
            pardir = os.path.normpath(os.path.join(path, '..'))
            pardir = os.path.split(pardir)[-1]
            print '[%s]' % pardir,
            filepath = os.path.join(path, filename)
            #backup orig file
            if create_backup:
                backup_path = filepath + '.bak'
--
Created by <Your Name> on Nov 08, 2013
"""
import os
essays = [[line.strip() for line in open(os.path.join("/home/ahmed/alltxt/02whole.txt")).readlines() if len(line.strip()) > 1] for essay in range(1, 21)]

def main():
    print essays
--
for root, dirs, files in os.walk(os.getcwd()):
    for name in dirs:
        try:
            os.rmdir(os.path.join(root, name))
        except WindowsError:
            print 'Skipping', os.path.join(root, name)
#
#####################################################
##
--


f.close
if os.path.exists(outName) :
    print 'File saved to: %s' % (outName)
--
    from os.path import join, isdir, islink, abspath

    # We may not have read permission for top, in which case we can't
    # get a list of the files the directory contains.  os.path.walk
    # always suppressed the exception then, rather than blow up for a
    # minor reason when (say) a thousand readable directories are still
    # left to visit.  That logic is copied here.
--

if __name__ == "__main__":
  if len(sys.argv) < 2:
    print("Usage: %s [option] <textfile>" % os.path.basename(sys.argv[0]))
    print("")
    print("  -h, --help  Show this help screen.")
    print("")
--
  print "python addpaper.py id 31415926535"
  sys.exit(1)

if not os.path.isfile('appid.txt'):
  print "OOPS! You're missing Microsoft Academic Search APP ID key in file appid.txt!"
  print "See Readme.md for instructions on obtaining one."
  print "Exitting."
  sys.exit(1)

if not os.path.isdir('db'): os.mkdir('db')

appid= open('appid.txt', 'r').read().rstrip()
globaldb = os.path.join('db', 'papers.p')
if not os.path.isfile(globaldb): pickle.dump([], open(globaldb, "wb"))

# form the query URL to MAS
url = "http://academic.research.microsoft.com/json.svc/search?AppId=%s" % (appid, )
--
  pub = j['d']['Publication']['Result'][rix] # publication json

  idstr = str(pub['ID'])
  dirpath = os.path.join('db', idstr)
  title = pub['Title']

  # print some info and ask user if this is the right paper to make sure
  papers = pickle.load(open(globaldb, "rb"))
  seenthis = any([pub['ID']==x['ID'] for x in papers])
  havethis = os.path.isdir(dirpath)

  v=""
  if pub['Conference']: v=pub['Conference']
--
if not havethis:
  print "Creating folder %s..." % (dirpath, )
  os.mkdir(dirpath)
jsonpath = os.path.join(dirpath, 'json.p')
pickle.dump(pub, open(jsonpath, "wb"))
print "Writing ", jsonpath

--

  # save ids
  ids = [x['ID'] for x in pubs]
  refPicklePath = os.path.join('db', idstr, fname)
  print "writing ", refPicklePath
  pickle.dump(ids, open(refPicklePath, "wb"))

--
  opencommand = "open"

# download full PDF
pdfpath = os.path.join('db', idstr, 'paper.pdf')
urls = pub['FullVersionURL']
pdfurls = [u for u in urls if u.endswith('.pdf')]
gotit = False
--
# create thumbnails
try:
  print "creating paper thumbnails..."
  thumbpath = os.path.join('db', idstr, 'thumb.png')
  cmd = "convert %s -thumbnail 150 -trim %s" % (pdfpath, thumbpath)
  print "running: " + cmd
  os.system(cmd)
--
    '''
    fileList = []
    for file in os.listdir(dir_name):
        dirfile = os.path.join(dir_name, file)
        if os.path.isfile(dirfile):
            if len(args) == 0:
                fileList.append(dirfile)
            else:
                if os.path.splitext(dirfile)[1][1:] in args:
                    fileList.append(dirfile)

        # recursively access file names in subdirectories
        elif os.path.isdir(dirfile) and subdir:
            # print "Accessing directory:", dirfile
            fileList += dir_list(dirfile, subdir, *args)
    return fileList
--
first_time=True
configs={}

config_handle = file(os.path.expanduser('~/.smsconf'),"r+")
config_lines = config_handle.readlines()
if len(config_lines) > 2:
    first_time=False
